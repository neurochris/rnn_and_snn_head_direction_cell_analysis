{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "agvYit-KkDzk"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "from google.colab import auth\n",
        "from google.colab import drive\n",
        "from oauth2client.client import GoogleCredentials"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/gdrive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "caqdINi7kGLn",
        "outputId": "0c942c23-8b5d-4e36-bad7-bade2cfe4397"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch_optimizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05bw7Vu-leMc",
        "outputId": "9987ab17-c5c4-43a7-a7ca-2218c5be41da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch_optimizer\n",
            "  Downloading torch_optimizer-0.3.0-py3-none-any.whl (61 kB)\n",
            "\u001b[?25l\r\u001b[K     |█████▎                          | 10 kB 41.3 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 20 kB 22.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 30 kB 12.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 40 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 51 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 61 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61 kB 517 kB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from torch_optimizer) (1.11.0+cu113)\n",
            "Collecting pytorch-ranger>=0.1.1\n",
            "  Downloading pytorch_ranger-0.1.1-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.5.0->torch_optimizer) (4.1.1)\n",
            "Installing collected packages: pytorch-ranger, torch-optimizer\n",
            "Successfully installed pytorch-ranger-0.1.1 torch-optimizer-0.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyparsing import actions\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torch import optim\n",
        "from random import randrange, uniform\n",
        "import math\n",
        "import pandas as pd\n",
        "import torch.utils as utils\n",
        "import operator\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import init\n",
        "from torch.nn import functional as F\n",
        "import math\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class CTRNN(nn.Module):\n",
        "    \"\"\"Continuous-time RNN.\n",
        "\n",
        "    Args:\n",
        "        input_size: Number of input neurons\n",
        "        hidden_size: Number of hidden neurons\n",
        "\n",
        "    Inputs:\n",
        "        input: (seq_len, batch, input_size), network input\n",
        "        hidden: (batch, hidden_size), initial hidden activity\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, dt=None, **kwargs):\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.tau = 100\n",
        "        if dt is None:\n",
        "            alpha = 1\n",
        "        else:\n",
        "            alpha = dt / self.tau\n",
        "        self.alpha = alpha\n",
        "        self.oneminusalpha = 1 - alpha\n",
        "\n",
        "        self.input2h = nn.Linear(input_size, hidden_size)\n",
        "        self.h2h = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "    def init_hidden(self, input_shape):\n",
        "        batch_size = input_shape[1]\n",
        "        return torch.zeros(batch_size, self.hidden_size)\n",
        "\n",
        "    def recurrence(self, input, hidden):\n",
        "        \"\"\"Recurrence helper.\"\"\"\n",
        "        pre_activation = self.input2h(input) + self.h2h(hidden)\n",
        "        h_new = hidden * self.oneminusalpha + pre_activation * self.alpha\n",
        "        return torch.max(torch.tensor(0.0), torch.tanh(h_new))\n",
        "\n",
        "    def forward(self, input, hidden=None):\n",
        "        \"\"\"Propogate input through the network.\"\"\"\n",
        "        if hidden is None:\n",
        "            hidden = self.init_hidden(input.shape).to(input.device)\n",
        "\n",
        "        output = []\n",
        "        steps = range(input.size(0))\n",
        "        for i in steps:\n",
        "            random_noise = np.random.normal(0,1)*0.1\n",
        "            hidden = self.recurrence(input[i], hidden)\n",
        "            output.append(hidden)\n",
        "\n",
        "        output = torch.stack(output, dim=0)\n",
        "        return output, hidden\n",
        "\n",
        "\n",
        "'''\n",
        "\n",
        "class CTRNN(nn.Module):\n",
        "    \"\"\"Continuous-time RNN.\n",
        "\n",
        "    Args:\n",
        "        input_size: Number of input neurons\n",
        "        hidden_size: Number of hidden neurons\n",
        "\n",
        "    Inputs:\n",
        "        input: (seq_len, batch, input_size), network input\n",
        "        hidden: (batch, hidden_size), initial hidden activity\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, dt=None, **kwargs):\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.tau = 100\n",
        "        if dt is None:\n",
        "            alpha = 1\n",
        "        else:\n",
        "            alpha = dt / self.tau\n",
        "        self.alpha = alpha\n",
        "        self.oneminusalpha = 1 - alpha\n",
        "\n",
        "        self.input2h = nn.Linear(input_size, hidden_size)\n",
        "        self.h2h = nn.Linear(hidden_size, hidden_size)\n",
        "        self.lif1 = snn.Leaky(beta=0.9)\n",
        "\n",
        "    def init_hidden(self, input_shape):\n",
        "        batch_size = input_shape[1]\n",
        "        return torch.zeros(batch_size, self.hidden_size)\n",
        "\n",
        "    def recurrence(self, input, hidden):\n",
        "        \"\"\"Recurrence helper.\"\"\"\n",
        "        pre_activation = self.input2h(input) + self.h2h(hidden)\n",
        "        h_new = torch.tanh(hidden * self.oneminusalpha +\n",
        "                           pre_activation * self.alpha)\n",
        "        return torch.max(torch.tensor(0.0), h_new)\n",
        "\n",
        "    def forward(self, input, hidden=None):\n",
        "\n",
        "        mem2 = self.lif1.init_leaky()\n",
        "\n",
        "        # Record the final layer\n",
        "        spk2_rec = []\n",
        "        mem2_rec = []\n",
        "\n",
        "        \"\"\"Propogate input through the network.\"\"\"\n",
        "        if hidden is None:\n",
        "            hidden = self.init_hidden(input.shape).to(input.device)\n",
        "\n",
        "        output = []\n",
        "        steps = range(input.size(0))\n",
        "        for i in steps:\n",
        "            random_noise = np.random.normal(0,1)*0.1\n",
        "            pre_activation = self.input2h(input[i])\n",
        "            spk2, mem2 = self.lif1(pre_activation, mem2)\n",
        "            spk2_rec.append(spk2)\n",
        "            mem2_rec.append(mem2+random_noise)\n",
        "        return torch.stack(spk2_rec, dim=0), torch.stack(mem2_rec, dim=0)\n",
        "'''\n",
        "\n",
        "class RNNNet(nn.Module):\n",
        "    \"\"\"Recurrent network model.\n",
        "\n",
        "    Args:\n",
        "        input_size: int, input size\n",
        "        hidden_size: int, hidden size\n",
        "        output_size: int, output size\n",
        "        rnn: str, type of RNN, lstm, rnn, ctrnn, or eirnn\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, hidden_size, output_size, **kwargs):\n",
        "        super().__init__()\n",
        "\n",
        "        # Continuous time RNN\n",
        "        self.rnn = CTRNN(input_size, hidden_size, **kwargs)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        rnn_activity, acts = self.rnn(x)\n",
        "        out = self.fc(rnn_activity)\n",
        "        return out, rnn_activity, acts\n",
        "\n",
        "class ModelAnalyzer:\n",
        "    def __init__(self, model, loader):\n",
        "        self.model = model\n",
        "        self.loader = loader\n",
        "\n",
        "    def plot_pred_vs_ground_truth(self):\n",
        "\n",
        "        pred_array = []\n",
        "        gt_array = []\n",
        "        time_array = []\n",
        "\n",
        "        return_arr = []\n",
        "\n",
        "        model_input, labels = self.loader.generate_training_data()\n",
        "        outputs, rnn_output = self.model(model_input)\n",
        "\n",
        "\n",
        "        for i in range(60):\n",
        "            pred_array.append(outputs[i, 0, 0].item())\n",
        "            gt_array.append(labels[i, 0, 0].item())\n",
        "            time_array.append(i)\n",
        "\n",
        "        plt.plot(time_array, pred_array, label='pred')\n",
        "        plt.plot(time_array, gt_array, label='gt')\n",
        "        plt.show()\n",
        "\n",
        "        plt.plot(time_array, pred_array, label='pred')\n",
        "        plt.plot(time_array, gt_array, label='gt')\n",
        "        ax = plt.gca()\n",
        "        ax.set_ylim([-1.0, 1.0])\n",
        "        plt.show()\n",
        "\n",
        "    def plot_tuning_curve_hd(self, inputs, activations):\n",
        "        fig, a = plt.subplots(10, 10)\n",
        "\n",
        "        x_idx = 0\n",
        "        y_idx = 0\n",
        "\n",
        "        return_arr = []\n",
        "        max_array= []\n",
        "\n",
        "        start = True\n",
        "\n",
        "        for i in range(100):\n",
        "\n",
        "            hidden_activation = []\n",
        "            input_value = []\n",
        "\n",
        "\n",
        "            for k in range(500):\n",
        "                for j in range(60):\n",
        "                    hidden_activation.append(abs(activations[k, j, 0, i]))\n",
        "                    input_value.append(inputs[k, j, 0, 2])\n",
        "\n",
        "            dataset = pd.DataFrame()\n",
        "            dataset['HD'] = input_value\n",
        "            dataset['Activation'] = hidden_activation\n",
        "            dataset['bins'] = pd.cut(dataset['HD'], 20).astype(str)\n",
        "            dataset['bins_mean'] = dataset.groupby('bins')['Activation'].transform('mean')\n",
        "            dataset.drop_duplicates('bins', inplace=True)\n",
        "            dataset.reset_index(inplace=True)\n",
        "            col = 'bins'\n",
        "            df = dataset.join(\n",
        "                dataset[col]\n",
        "                    .str.replace(\"]\", \"\", regex=False)\n",
        "                    .str.replace(\"(\", \"\", regex=False)\n",
        "                    .str.replace(\" \", \"\", regex=False)\n",
        "                    .str.replace(\",\", \"-\", regex=False)\n",
        "                    .str.replace(\".0\", \"\", regex=False)\n",
        "                    .str.replace(\".1\", \"\", regex=False)\n",
        "                    .str.replace(\".2\", \"\", regex=False)\n",
        "                    .str.replace(\".3\", \"\", regex=False)\n",
        "                    .str.replace(\".4\", \"\", regex=False)\n",
        "                    .str.replace(\".5\", \"\", regex=False)\n",
        "                    .str.replace(\".6\", \"\", regex=False)\n",
        "                    .str.replace(\".7\", \"\", regex=False)\n",
        "                    .str.replace(\".8\", \"\", regex=False)\n",
        "                    .str.replace(\".9\", \"\", regex=False)\n",
        "                    .str.extract(pat=r\"^[$]*(\\d+)[-\\s$]*(\\d+)$\")\n",
        "                    .astype(\"float\")\n",
        "                    .rename({0: f\"{col}_lower\", 1: f\"{col}_upper\"}, axis=\"columns\")\n",
        "            )\n",
        "            dff = df.dropna()\n",
        "            dff = dff.sort_values(by=['bins_lower'])\n",
        "\n",
        "            return_arr.append(df['bins_mean'].to_numpy())\n",
        "            max_array.append([dff['HD'].to_numpy(), dff['bins_mean'].to_numpy()])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            a[x_idx][y_idx].plot(dff['bins_lower'].to_numpy(), dff['bins_mean'].to_numpy())\n",
        "            #a[x_idx][y_idx].set_xlim(0, 360)\n",
        "            #a[x_idx][y_idx].set_ylim(0, 1.0)\n",
        "            x_idx += 1\n",
        "            if x_idx == 10 and start == False:\n",
        "                y_idx += 1\n",
        "                x_idx = 0\n",
        "            start = False\n",
        "\n",
        "        plt.setp(plt.gcf().get_axes(), xticks=[], yticks=[])\n",
        "        plt.show()\n",
        "        return np.array(return_arr), np.array(max_array)\n",
        "\n",
        "\n",
        "    def plot_tuning_curve_angular_velocity(self, inputs, activations):\n",
        "            fig, a = plt.subplots(10, 10)\n",
        "\n",
        "            x_idx = 0\n",
        "            y_idx = 0\n",
        "            return_arr = []\n",
        "            max_array = []\n",
        "\n",
        "            start = True\n",
        "\n",
        "            for i in range(100):\n",
        "\n",
        "                hidden_activation = []\n",
        "                input_value = []\n",
        "\n",
        "                for k in range(100):\n",
        "                    for j in range(60):\n",
        "                        hidden_activation.append(activations[k, j, 0, i])\n",
        "                        input_value.append(inputs[k, j, 0, 3])\n",
        "\n",
        "                dataset = pd.DataFrame()\n",
        "                dataset['HD'] = np.round(input_value)\n",
        "                dataset['Activation'] = hidden_activation\n",
        "                dataset['bins'] = pd.cut(dataset['HD'], 20).astype(str)\n",
        "                dataset['bins_mean'] = dataset.groupby('bins')['Activation'].transform('mean')\n",
        "                dataset.drop_duplicates('bins', inplace=True)\n",
        "                dataset.reset_index(inplace=True)\n",
        "                col = 'bins'\n",
        "                df = dataset.join(\n",
        "                    dataset[col]\n",
        "                        .str.replace(\"]\", \"\", regex=False)\n",
        "                        .str.replace(\"(\", \"\", regex=False)\n",
        "                        .str.replace(\" \", \"\", regex=False)\n",
        "                        .str.replace(\",\", \"-\", regex=False)\n",
        "                        .str.replace(\".0\", \"\", regex=False)\n",
        "                        .str.replace(\".1\", \"\", regex=False)\n",
        "                        .str.replace(\".2\", \"\", regex=False)\n",
        "                        .str.replace(\".3\", \"\", regex=False)\n",
        "                        .str.replace(\".4\", \"\", regex=False)\n",
        "                        .str.replace(\".5\", \"\", regex=False)\n",
        "                        .str.replace(\".6\", \"\", regex=False)\n",
        "                        .str.replace(\".7\", \"\", regex=False)\n",
        "                        .str.replace(\".8\", \"\", regex=False)\n",
        "                        .str.replace(\".9\", \"\", regex=False)\n",
        "                        .str.extract(pat=r\"^[$]*(\\d+)[-\\s$]*(\\d+)$\")\n",
        "                        .astype(\"float\")\n",
        "                        .rename({0: f\"{col}_lower\", 1: f\"{col}_upper\"}, axis=\"columns\")\n",
        "                )\n",
        "                dff = df.dropna()\n",
        "                dff = dff.sort_values(by=['bins_lower'])\n",
        "\n",
        "                return_arr.append(df['bins_mean'].to_numpy())\n",
        "                max_array.append([dff['bins_lower'].to_numpy(), dff['bins_mean'].to_numpy()])\n",
        "\n",
        "\n",
        "                a[x_idx][y_idx].plot(dff['bins_lower'].to_numpy(), dff['bins_mean'].to_numpy())\n",
        "                #a[x_idx][y_idx].set_xlim(0, 360)\n",
        "                #a[x_idx][y_idx].set_ylim(0, 1.0)\n",
        "                x_idx += 1\n",
        "                if x_idx == 10 and start == False:\n",
        "                    y_idx += 1\n",
        "                    x_idx = 0\n",
        "                start = False\n",
        "\n",
        "            plt.setp(plt.gcf().get_axes(), xticks=[], yticks=[])\n",
        "            plt.show()\n",
        "            return np.array(return_arr), np.array(max_array)\n",
        "\n",
        "    def plot_joint(self, HD, activations, AV):\n",
        "      fig, a = plt.subplots(10, 10)\n",
        "\n",
        "      x_idx = 0\n",
        "      y_idx = 0\n",
        "\n",
        "      for i in range(100):\n",
        "\n",
        "                act = []\n",
        "                hd = []\n",
        "                av = []\n",
        "\n",
        "                for k in range(500):\n",
        "                    for j in range(60):\n",
        "                        act.append(activations[k, j, 0, i]*100.0)\n",
        "                        hd.append(HD[k, j, 0, 2])\n",
        "                        av.append(HD[k, j, 0, 3])\n",
        "\n",
        "                df = pd.DataFrame({'Weekday' : np.round(np.array(hd)), 'Hour':np.round(np.array(av)), 'Value':np.array(act)})\n",
        "                df['Weekday'] = pd.cut(df['Weekday'], 25)\n",
        "\n",
        "                piv = pd.pivot_table(df, index='Hour', columns='Weekday', values='Value')\n",
        "                piv = piv.fillna(0.0)\n",
        "\n",
        "\n",
        "                #fig, ax = plt.subplots()\n",
        "                #ax.set_aspect('equal')\n",
        "                #plt.axis([0, 12, 0, 7])\n",
        "                heatmap = a[x_idx][y_idx].pcolormesh(piv, cmap=\"seismic\", shading='gouraud')\n",
        "\n",
        "                #a[x_idx][y_idx].set_yticks(np.arange(piv.shape[0]) + 0.5, minor=False)\n",
        "                a[x_idx][y_idx].set_xticks(np.arange(piv.shape[1]) + 0.5, minor=False)\n",
        "                a[x_idx][y_idx].set_xticklabels(piv.columns, minor=False)\n",
        "                #a[x_idx][y_idx].set_yticklabels(piv.index, minor=False)\n",
        "                #a[x_idx][y_idx].plot(dff['bins_lower'].to_numpy(), dff['bins_mean'].to_numpy())\n",
        "                #a[x_idx][y_idx].set_xlim(0, 360)\n",
        "                #a[x_idx][y_idx].set_ylim(0, 1.0)\n",
        "                x_idx += 1\n",
        "                if x_idx == 10 and start == False:\n",
        "                    y_idx += 1\n",
        "                    x_idx = 0\n",
        "                start = False\n",
        "\n",
        "      plt.setp(plt.gcf().get_axes(), xticks=[], yticks=[])\n",
        "      plt.show()\n",
        "\n",
        "\n",
        "\n",
        "    def get_delta(self, idx, idy, maxes):\n",
        "      ##get 2d array for two idxs\n",
        "      ##find max and corresponding value\n",
        "      ##delta for corresponding values\n",
        "      pref1_x = maxes[idx, 0, :].reshape((-1, 1))\n",
        "      pref1_y = maxes[idx, 1, :].reshape((-1, 1))\n",
        "\n",
        "      pref2_x = maxes[idy, 0, :].reshape((-1, 1))\n",
        "      pref2_y = maxes[idy, 1, :].reshape((-1, 1))\n",
        "\n",
        "      indices1 = np.where(pref1_y == pref1_y.max())\n",
        "      indices2 = np.where(pref2_y == pref2_y.max())\n",
        "\n",
        "      if len(np.atleast_1d(pref1_x[indices1])) == 1 and len(np.atleast_1d(pref2_x[indices2])) == 1:\n",
        "        return (pref1_x[indices1][0] - pref2_x[indices2][0])\n",
        "      else:\n",
        "        return None\n",
        "\n",
        "\n",
        "\n",
        "    def plot_pref(self, model, maxes):\n",
        "      delta_po = None\n",
        "      weights = model.rnn.h2h.weight.detach().numpy()\n",
        "      weight_avg = []\n",
        "      delta_pref_or = []\n",
        "      for i in range(100):\n",
        "        for j in range(100):\n",
        "          weight = weights[i][j]\n",
        "          delta_po = self.get_delta(i, j, maxes)\n",
        "          if delta_po != None:\n",
        "            weight_avg.append(weight)\n",
        "            delta_pref_or.append(delta_po)\n",
        "\n",
        "      dataset = pd.DataFrame()\n",
        "      dataset['pref'] = delta_pref_or\n",
        "      dataset['weight'] = weight_avg\n",
        "      dataset['bins'] = pd.cut(dataset['pref'], 200).astype(str)\n",
        "      dataset['bins_mean'] = dataset.groupby('bins')['weight'].transform('mean')\n",
        "      #dataset = dataset.groupby('weight').mean()\n",
        "      #dataset.drop_duplicates('bins', inplace=True)\n",
        "      #dataset.reset_index(inplace=True)\n",
        "      y_mean = [np.mean(dataset['bins_mean'].to_numpy())]*len(dataset['pref'].to_numpy())\n",
        "\n",
        "      #plt.scatter(dataset['pref'].to_numpy(), dataset['bins_mean'].to_numpy())\n",
        "      #plt.plot(dataset['pref'].to_numpy(), y_mean)\n",
        "      #plt.show()\n",
        "      x = dataset['pref'].to_numpy()\n",
        "      y = dataset['bins_mean'].to_numpy()\n",
        "\n",
        "\n",
        "      fig, ax = plt.subplots()\n",
        "      sns.regplot(x=x, y=y, ax=ax, lowess=True)\n",
        "      ax.set_xlim(-180, 180)\n",
        "      plt.show()\n",
        "\n",
        "\n",
        "    def plot_pca_bumps(self, X):\n",
        "      for i in range(60):\n",
        "        print(X.shape)\n",
        "        X_new = X[0, i, :, :]\n",
        "        pca = PCA(n_components=2)\n",
        "        Xt = pca.fit_transform(X_new)\n",
        "        plot = plt.scatter(Xt[:,0], Xt[:,1])\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "    def plot_pca(self, X):\n",
        "        print(X.shape)\n",
        "        X_new = X[:, 0, 0, :]\n",
        "        pca = PCA(n_components=2)\n",
        "        Xt = pca.fit_transform(X_new)\n",
        "        plot = plt.scatter(Xt[:,0], Xt[:,1])\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "    def plot_conn(self, model):\n",
        "      weight_array = np.zeros((100, 100))\n",
        "\n",
        "      weights = model.rnn.h2h.weight.detach().numpy().T\n",
        "\n",
        "      for i in range(100):\n",
        "        for j in range(100):\n",
        "          weight_array[i, j] = weights[i][j]\n",
        "\n",
        "      weight_array = 2.*(weight_array - np.min(weight_array))/np.ptp(weight_array)-1\n",
        "\n",
        "      plt.pcolormesh(np.array(np.array(weight_array).reshape((100, 100))), cmap='bwr')\n",
        "      plt.show()\n",
        "\n",
        "\n",
        "class ModelTrainer:\n",
        "    import torch\n",
        "    import numpy as np\n",
        "\n",
        "    def __init__(self, tau: object, dt: object, x_0: object, W: object, seed: object) -> object:\n",
        "        self.seed = seed\n",
        "        self.torch.manual_seed(seed)\n",
        "        self.np.random.seed(seed)\n",
        "        self.x_0 = x_0\n",
        "        self.dt = dt\n",
        "        self.tau = tau\n",
        "        self.W = W\n",
        "        self.hidden_activations = []\n",
        "        self.input_activations = []\n",
        "        self.train_loader = None\n",
        "\n",
        "    def get_train_loader(self):\n",
        "\n",
        "        if self.train_loader == None:\n",
        "            print('WARNING: loader is None')\n",
        "\n",
        "        return self.train_loader\n",
        "\n",
        "    def compute_angular_velocity(self, x, prev_angular_velocity):\n",
        "        return (0.03 * x) + (0.8 * prev_angular_velocity)  ##convert 0.03 rad/s to deg/s using 180/pi\n",
        "\n",
        "    def load_training_data(self):\n",
        "        train_x = np.load('/home/chris/Desktop/data_x.npy').astype(float)\n",
        "        train_y = np.load('/home/chris/Desktop/data_x.npy').astype(float)\n",
        "        dataset = utils.data.TensorDataset(train_x, train_y)\n",
        "        dataloader = utils.data.DataLoader(dataset)\n",
        "        print(train_x)\n",
        "        print(train_y)\n",
        "\n",
        "        return dataloader\n",
        "\n",
        "    def generate_training_data(self):\n",
        "\n",
        "        final_array1 = np.zeros((60, 8, 3))\n",
        "        final_array2 = np.zeros((60, 8, 4))\n",
        "\n",
        "        for i in range(8):\n",
        "          time_steps = 60\n",
        "          prev_angular_velocity = np.random.normal(0,1) * 0.03\n",
        "          angular_velocity = np.random.normal(0,1) * 0.03\n",
        "          theta = np.radians(uniform(0, 360))\n",
        "          theta_initial = theta\n",
        "\n",
        "          theta_array = []\n",
        "          time_array = []\n",
        "\n",
        "          input_values = []\n",
        "          input_array = []\n",
        "\n",
        "          output_values = []\n",
        "          output_array = []\n",
        "\n",
        "\n",
        "          for step in range(0, time_steps):\n",
        "\n",
        "\n",
        "              x = np.random.normal(0,1)\n",
        "\n",
        "\n",
        "              if step < 30:\n",
        "                  prev_angular_velocity = angular_velocity = 0.0\n",
        "                  input_values.append(0.0)\n",
        "\n",
        "                  input_values.append(np.sin(theta_initial))\n",
        "                  input_values.append(np.cos(theta_initial))\n",
        "              else:\n",
        "                  angular_velocity = self.compute_angular_velocity(x, prev_angular_velocity)\n",
        "                  prev_angular_velocity = angular_velocity\n",
        "                  input_values.append(angular_velocity)\n",
        "\n",
        "                  input_values.append(0.0)\n",
        "                  input_values.append(0.0)\n",
        "\n",
        "                  theta = theta + angular_velocity\n",
        "\n",
        "\n",
        "              output_values.append(np.sin(theta))\n",
        "              output_values.append(np.cos(theta))\n",
        "              output_values.append(np.degrees(theta))\n",
        "              output_values.append(angular_velocity*(180.0/math.pi))\n",
        "\n",
        "              #theta_initial = theta\n",
        "\n",
        "              theta_array.append(np.degrees(theta))\n",
        "              time_array.append(step)\n",
        "\n",
        "              input_array.append(input_values)\n",
        "              output_array.append(output_values)\n",
        "\n",
        "              input_values = []\n",
        "              output_values = []\n",
        "\n",
        "          #print(input_array)\n",
        "          #print(output_array)\n",
        "          input_arrayy = np.array(input_array).reshape((60, 1, 3))\n",
        "          output_arrayy = np.array(output_array).reshape((60, 1, 4))\n",
        "\n",
        "\n",
        "          #plt.plot(time_array, theta_array)\n",
        "          plt.show()\n",
        "\n",
        "          #data_pd = pd.DataFrame(input_array)\n",
        "          #data_pd.columns = ['Sin Theta0', 'Cos Theta0', 'w', 'Sin Theta', 'Cos Theta']\n",
        "\n",
        "          #print(data_pd.head())\n",
        "\n",
        "          #plt.plot(time_array, data_pd[['Sin Theta']].to_numpy())\n",
        "          #plt.plot(time_array, data_pd[['Cos Theta']].to_numpy())\n",
        "          #plt.show()\n",
        "          final_array1[:, i, :] = input_arrayy.reshape((60, 3))\n",
        "          final_array2[:, i, :] = output_arrayy.reshape((60, 4))\n",
        "\n",
        "\n",
        "        train_x = torch.Tensor(np.array(final_array1).reshape((60, 8, 3)))\n",
        "        train_y = torch.Tensor(np.array(final_array2).reshape((60, 8, 4)))\n",
        "\n",
        "\n",
        "        #dataset = utils.data.TensorDataset(train_x, train_y)\n",
        "        #dataloader = utils.data.DataLoader(dataset)\n",
        "\n",
        "        return train_x, train_y\n",
        "\n",
        "    def generate_training_data_with_noise(self):\n",
        "\n",
        "        final_array1 = np.zeros((60, 8, 3))\n",
        "        final_array2 = np.zeros((60, 8, 4))\n",
        "\n",
        "        for i in range(8):\n",
        "          time_steps = 60\n",
        "          prev_angular_velocity = np.random.normal(0,1) * 0.03\n",
        "          angular_velocity = np.random.normal(0,1) * 0.03\n",
        "          theta = np.radians(uniform(0, 360))\n",
        "          theta_initial = theta\n",
        "\n",
        "          theta_array = []\n",
        "          time_array = []\n",
        "\n",
        "          input_values = []\n",
        "          input_array = []\n",
        "\n",
        "          output_values = []\n",
        "          output_array = []\n",
        "\n",
        "\n",
        "          for step in range(0, time_steps):\n",
        "\n",
        "\n",
        "              x = np.random.normal(0,1)\n",
        "              random_noise = np.random.normal(0,1)*0.1\n",
        "\n",
        "\n",
        "              if step < 30:\n",
        "                  prev_angular_velocity = angular_velocity = 0.0\n",
        "                  input_values.append(0.0)\n",
        "\n",
        "                  input_values.append(np.sin(theta_initial))\n",
        "                  input_values.append(np.cos(theta_initial))\n",
        "              else:\n",
        "                  angular_velocity = self.compute_angular_velocity(x, prev_angular_velocity)\n",
        "                  prev_angular_velocity = angular_velocity\n",
        "                  input_values.append(angular_velocity + random_noise)\n",
        "\n",
        "                  input_values.append(0.0)\n",
        "                  input_values.append(0.0)\n",
        "\n",
        "                  theta = theta + angular_velocity\n",
        "\n",
        "\n",
        "              output_values.append(np.sin(theta))\n",
        "              output_values.append(np.cos(theta))\n",
        "              output_values.append(np.degrees(theta))\n",
        "              output_values.append(angular_velocity*(180.0/math.pi))\n",
        "\n",
        "              #theta_initial = theta\n",
        "\n",
        "              theta_array.append(np.degrees(theta))\n",
        "              time_array.append(step)\n",
        "\n",
        "              input_array.append(input_values)\n",
        "              output_array.append(output_values)\n",
        "\n",
        "              input_values = []\n",
        "              output_values = []\n",
        "\n",
        "          #print(input_array)\n",
        "          #print(output_array)\n",
        "          input_arrayy = np.array(input_array).reshape((60, 1, 3))\n",
        "          output_arrayy = np.array(output_array).reshape((60, 1, 4))\n",
        "\n",
        "\n",
        "          #plt.plot(time_array, theta_array)\n",
        "          plt.show()\n",
        "\n",
        "          #data_pd = pd.DataFrame(input_array)\n",
        "          #data_pd.columns = ['Sin Theta0', 'Cos Theta0', 'w', 'Sin Theta', 'Cos Theta']\n",
        "\n",
        "          #print(data_pd.head())\n",
        "\n",
        "          #plt.plot(time_array, data_pd[['Sin Theta']].to_numpy())\n",
        "          #plt.plot(time_array, data_pd[['Cos Theta']].to_numpy())\n",
        "          #plt.show()\n",
        "          final_array1[:, i, :] = input_arrayy.reshape((60, 3))\n",
        "          final_array2[:, i, :] = output_arrayy.reshape((60, 4))\n",
        "\n",
        "\n",
        "        train_x = torch.Tensor(np.array(final_array1).reshape((60, 8, 3)))\n",
        "        train_y = torch.Tensor(np.array(final_array2).reshape((60, 8, 4)))\n",
        "\n",
        "\n",
        "        #dataset = utils.data.TensorDataset(train_x, train_y)\n",
        "        #dataloader = utils.data.DataLoader(dataset)\n",
        "\n",
        "        return train_x, train_y\n",
        "\n",
        "\n",
        "    def generate_training_data_one_location(self):\n",
        "\n",
        "        final_array1 = np.zeros((60, 8, 3))\n",
        "        final_array2 = np.zeros((60, 8, 4))\n",
        "\n",
        "        for i in range(8):\n",
        "          time_steps = 60\n",
        "          prev_angular_velocity = np.random.normal(0,1) * 0.03\n",
        "          angular_velocity = np.random.normal(0,1) * 0.03\n",
        "\n",
        "          theta = np.radians(uniform(0, 360))\n",
        "          theta_initial = theta\n",
        "\n",
        "          theta_array = []\n",
        "          time_array = []\n",
        "\n",
        "          input_values = []\n",
        "          input_array = []\n",
        "\n",
        "          output_values = []\n",
        "          output_array = []\n",
        "\n",
        "\n",
        "          for step in range(0, time_steps):\n",
        "\n",
        "\n",
        "              x = np.random.normal(0,1)\n",
        "\n",
        "\n",
        "              if step < 30:\n",
        "                  prev_angular_velocity = angular_velocity = 0.0\n",
        "                  input_values.append(0.0)\n",
        "\n",
        "                  input_values.append(np.sin(np.radians(theta)))\n",
        "                  input_values.append(np.cos(np.radians(theta)))\n",
        "                  theta = np.radians(0.0)\n",
        "              else:\n",
        "                  angular_velocity = 0.0\n",
        "                  prev_angular_velocity = 0.0\n",
        "                  input_values.append(0.0)\n",
        "\n",
        "                  input_values.append(np.sin(np.radians(theta)))\n",
        "                  input_values.append(np.cos(np.radians(theta)))\n",
        "\n",
        "\n",
        "\n",
        "              output_values.append(np.sin(theta))\n",
        "              output_values.append(np.cos(theta))\n",
        "              output_values.append(np.degrees(theta))\n",
        "              output_values.append(angular_velocity*(180.0/math.pi))\n",
        "\n",
        "              #theta_initial = theta\n",
        "\n",
        "              theta_array.append(np.degrees(theta))\n",
        "              time_array.append(step)\n",
        "\n",
        "              input_array.append(input_values)\n",
        "              output_array.append(output_values)\n",
        "\n",
        "              input_values = []\n",
        "              output_values = []\n",
        "\n",
        "          #print(input_array)\n",
        "          #print(output_array)\n",
        "          input_arrayy = np.array(input_array).reshape((60, 1, 3))\n",
        "          output_arrayy = np.array(output_array).reshape((60, 1, 4))\n",
        "\n",
        "\n",
        "          #plt.plot(time_array, theta_array)\n",
        "          #plt.show()\n",
        "\n",
        "          #data_pd = pd.DataFrame(input_array)\n",
        "          #data_pd.columns = ['Sin Theta0', 'Cos Theta0', 'w', 'Sin Theta', 'Cos Theta']\n",
        "\n",
        "          #print(data_pd.head())\n",
        "\n",
        "          #plt.plot(time_array, data_pd[['Sin Theta']].to_numpy())\n",
        "          #plt.plot(time_array, data_pd[['Cos Theta']].to_numpy())\n",
        "          #plt.show()\n",
        "          final_array1[:, i, :] = input_arrayy.reshape((60, 3))\n",
        "          final_array2[:, i, :] = output_arrayy.reshape((60, 4))\n",
        "\n",
        "\n",
        "        train_x = torch.Tensor(np.array(final_array1).reshape((60, 8, 3)))\n",
        "        train_y = torch.Tensor(np.array(final_array2).reshape((60, 8, 4)))\n",
        "\n",
        "\n",
        "        #dataset = utils.data.TensorDataset(train_x, train_y)\n",
        "        #dataloader = utils.data.DataLoader(dataset)\n",
        "\n",
        "        return train_x, train_y\n",
        "\n",
        "\n",
        "    def test_data_size(self, dataset):\n",
        "        batch_size = 10\n",
        "        seq_len = 20  # sequence length\n",
        "        input_size = 3  # input dimension\n",
        "\n",
        "        # Make network\n",
        "        rnn = RNN(input_size=input_size, hidden_size=100, output_size=2, tau=250, dt=25)\n",
        "\n",
        "        # Run the sequence through the network\n",
        "        out, rnn_output = rnn(dataset)\n",
        "\n",
        "        print('Input of shape (SeqLen, BatchSize, InputDim)=', dataset.shape)\n",
        "        print('Output of shape (SeqLen, BatchSize, Neuron)=', out.shape)\n",
        "\n",
        "    def get_model(self):\n",
        "        return RNNNet(input_size=3, hidden_size=100, output_size=2, tau=250, dt=25)\n",
        "\n",
        "    def get_inputs_and_activations(self, model):\n",
        "\n",
        "        av = []\n",
        "\n",
        "        for i in range(500): #5000 for snn joint tuning plots\n",
        "\n",
        "            model_input, labels = self.generate_training_data()\n",
        "\n",
        "            self.input_activations.append(labels.detach().numpy())\n",
        "\n",
        "            outputs, rnn_output, acts = model(model_input)\n",
        "\n",
        "            self.hidden_activations.append(acts.detach().numpy())\n",
        "            av.append(model_input.detach().numpy()[:,:,0])\n",
        "\n",
        "\n",
        "\n",
        "        return np.array(self.input_activations), np.array(self.hidden_activations), np.array(av)\n",
        "\n",
        "    def get_inputs_and_activations_one_location(self, model):\n",
        "\n",
        "        av = []\n",
        "\n",
        "        for i in range(5000): #5000 for snn joint tuning plots\n",
        "\n",
        "            model_input, labels = self.generate_training_data_one_location(i)\n",
        "\n",
        "            self.input_activations.append(labels.detach().numpy())\n",
        "\n",
        "            outputs, rnn_output, acts = model(model_input)\n",
        "\n",
        "            self.hidden_activations.append(rnn_output.detach().numpy())\n",
        "            av.append(model_input.detach().numpy()[:,:,0])\n",
        "\n",
        "\n",
        "\n",
        "        return np.array(self.input_activations), np.array(self.hidden_activations), np.array(av)\n",
        "\n",
        "    def plot_grad_flow(self, named_parameters):\n",
        "        ave_grads = []\n",
        "        layers = []\n",
        "        for n, p in named_parameters:\n",
        "            if(p.requires_grad) and (\"bias\" not in n):\n",
        "                layers.append(n)\n",
        "                ave_grads.append(p.grad.abs().mean())\n",
        "        plt.plot(ave_grads, alpha=0.3, color=\"b\")\n",
        "        plt.hlines(0, 0, len(ave_grads)+1, linewidth=1, color=\"k\" )\n",
        "        plt.xticks(range(0,len(ave_grads), 1), layers, rotation=\"vertical\")\n",
        "        plt.xlim(xmin=0, xmax=len(ave_grads))\n",
        "        plt.xlabel(\"Layers\")\n",
        "        plt.ylabel(\"average gradient\")\n",
        "        plt.title(\"Gradient flow\")\n",
        "        plt.grid(True)\n",
        "\n",
        "    def get_test_acc(self, model):\n",
        "      criterion = nn.MSELoss()\n",
        "      loss_array = []\n",
        "      model_inputt, labelss = self.generate_training_data_one_location()\n",
        "      model.eval()\n",
        "      labels = labelss[: , :, 0:2]\n",
        "      outputs, rnn_output, acts = model(model_inputt)\n",
        "      loss = criterion(outputs, labels)\n",
        "      print(str(loss))\n",
        "      return float(loss.detach().numpy())\n",
        "\n",
        "    def train_model(self, model):\n",
        "        optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "        criterion = nn.MSELoss()\n",
        "\n",
        "        loss_array = []\n",
        "        iterations_array = []\n",
        "        itr_counter = 0\n",
        "        norm_array = []\n",
        "        total_norm = 0\n",
        "        model_analyzer = ModelAnalyzer(model, model_trainer)\n",
        "        test_acc = []\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        for epoch in range(1000):\n",
        "            model_inputt, labelss = self.generate_training_data()\n",
        "\n",
        "            model.train()\n",
        "\n",
        "            labels = labelss[: , :, 0:2]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs, rnn_output, acts = model(model_inputt)\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            loss.backward()\n",
        "            #self.plot_grad_flow(model.named_parameters())\n",
        "\n",
        "\n",
        "\n",
        "            norm_array.append(total_norm)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            print('loss:' + str(loss))\n",
        "            acc = self.get_test_acc(model)\n",
        "            test_acc.append(acc)\n",
        "            print(acc)\n",
        "\n",
        "\n",
        "            loss_array.append(loss)\n",
        "            iterations_array.append(epoch)\n",
        "            itr_counter += 1\n",
        "\n",
        "        plt.plot(iterations_array, test_acc)\n",
        "        plt.show()\n",
        "\n",
        "        return model\n",
        "\n",
        "    def test_with_weight_noise(self, model):\n",
        "\n",
        "      with torch.no_grad():\n",
        "        for param in model.parameters():\n",
        "          param.add_(torch.randn(param.size()) * 0.1)\n",
        "\n",
        "      criterion = nn.MSELoss()\n",
        "      loss_array = []\n",
        "      model_inputt, labelss = self.generate_training_data_one_location()\n",
        "      model.eval()\n",
        "      labels = labelss[: , :, 0:2]\n",
        "      outputs, rnn_output, acts = model(model_inputt)\n",
        "      loss = criterion(outputs, labels)\n",
        "      print('Test acc with weight noise:' + str(loss))\n",
        "\n",
        "    def test_with_input_noise(self, model):\n",
        "      criterion = nn.MSELoss()\n",
        "      loss_array = []\n",
        "      model_inputt, labelss = self.generate_training_data_with_noise()\n",
        "      model.eval()\n",
        "      labels = labelss[: , :, 0:2]\n",
        "      outputs, rnn_output = model(model_inputt)\n",
        "      loss = criterion(outputs, labels)\n",
        "      print('Test acc with input noise:' + str(loss))\n",
        "\n",
        "\n",
        "    def run(self):\n",
        "        model = self.get_model()\n",
        "        model = self.train_model(model)\n",
        "        self.get_test_acc(model)\n",
        "        print('***********************')\n",
        "        self.test_with_weight_noise(model)\n",
        "        return model\n",
        "\n",
        "model_trainer = ModelTrainer(tau=250.0, dt=25.0, x_0=1.0, W=2.0, seed=3) #3\n",
        "model = model_trainer.run()\n",
        "#model = model_trainer.get_test_acc(model)\n",
        "#loader = model_trainer.get_train_loader()\n",
        "\n",
        "#model_analyzer = ModelAnalyzer(model, model_trainer)\n",
        "\n",
        "\n",
        "#inputs, activations, av = model_trainer.get_inputs_and_activations_one_location(model.eval())\n",
        "\n",
        "#model_analyzer.plot_pred_vs_ground_truth()\n",
        "#HD_activations, maxes = model_analyzer.plot_tuning_curve_hd(inputs, activations)\n",
        "#AV_activations, maxes = model_analyzer.plot_tuning_curve_angular_velocity(inputs, activations)\n",
        "#model_analyzer.plot_joint(inputs, av, activations)\n",
        "#model_analyzer.plot_conn(model)\n",
        "#model_analyzer.plot_pref(model, maxes)\n",
        "#model_analyzer.plot_pca(activations)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YwxTHucBkIlw",
        "outputId": "11b76ce4-3182-4207-807c-93123244d047"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss:tensor(0.4626, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.1698, grad_fn=<MseLossBackward0>)\n",
            "0.16978444159030914\n",
            "loss:tensor(0.3710, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.1038, grad_fn=<MseLossBackward0>)\n",
            "0.10384824872016907\n",
            "loss:tensor(0.3307, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0475, grad_fn=<MseLossBackward0>)\n",
            "0.04749336466193199\n",
            "loss:tensor(0.1906, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0272, grad_fn=<MseLossBackward0>)\n",
            "0.0272146575152874\n",
            "loss:tensor(0.4065, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0289, grad_fn=<MseLossBackward0>)\n",
            "0.028942065313458443\n",
            "loss:tensor(0.2067, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0811, grad_fn=<MseLossBackward0>)\n",
            "0.08114262670278549\n",
            "loss:tensor(0.2813, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0759, grad_fn=<MseLossBackward0>)\n",
            "0.07590949535369873\n",
            "loss:tensor(0.2992, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0513, grad_fn=<MseLossBackward0>)\n",
            "0.05128977447748184\n",
            "loss:tensor(0.2567, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0245, grad_fn=<MseLossBackward0>)\n",
            "0.024502327665686607\n",
            "loss:tensor(0.1508, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0212, grad_fn=<MseLossBackward0>)\n",
            "0.02121480368077755\n",
            "loss:tensor(0.1306, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0295, grad_fn=<MseLossBackward0>)\n",
            "0.02948780730366707\n",
            "loss:tensor(0.1183, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0437, grad_fn=<MseLossBackward0>)\n",
            "0.04370826110243797\n",
            "loss:tensor(0.2265, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0427, grad_fn=<MseLossBackward0>)\n",
            "0.042734917253255844\n",
            "loss:tensor(0.2410, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0351, grad_fn=<MseLossBackward0>)\n",
            "0.03506064787507057\n",
            "loss:tensor(0.2603, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0212, grad_fn=<MseLossBackward0>)\n",
            "0.021215716376900673\n",
            "loss:tensor(0.1906, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0122, grad_fn=<MseLossBackward0>)\n",
            "0.012243877165019512\n",
            "loss:tensor(0.2074, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0135, grad_fn=<MseLossBackward0>)\n",
            "0.01346843596547842\n",
            "loss:tensor(0.1369, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0222, grad_fn=<MseLossBackward0>)\n",
            "0.022162439301609993\n",
            "loss:tensor(0.1396, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0313, grad_fn=<MseLossBackward0>)\n",
            "0.031290773302316666\n",
            "loss:tensor(0.1895, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0322, grad_fn=<MseLossBackward0>)\n",
            "0.0322083905339241\n",
            "loss:tensor(0.1284, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0287, grad_fn=<MseLossBackward0>)\n",
            "0.02874060720205307\n",
            "loss:tensor(0.1741, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0228, grad_fn=<MseLossBackward0>)\n",
            "0.022817660123109818\n",
            "loss:tensor(0.1419, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0195, grad_fn=<MseLossBackward0>)\n",
            "0.01945420727133751\n",
            "loss:tensor(0.1058, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0216, grad_fn=<MseLossBackward0>)\n",
            "0.021586472168564796\n",
            "loss:tensor(0.1839, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0247, grad_fn=<MseLossBackward0>)\n",
            "0.024729371070861816\n",
            "loss:tensor(0.2082, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0234, grad_fn=<MseLossBackward0>)\n",
            "0.02339324913918972\n",
            "loss:tensor(0.2233, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0172, grad_fn=<MseLossBackward0>)\n",
            "0.017187032848596573\n",
            "loss:tensor(0.0874, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0212, grad_fn=<MseLossBackward0>)\n",
            "0.021185072138905525\n",
            "loss:tensor(0.0841, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0369, grad_fn=<MseLossBackward0>)\n",
            "0.036865536123514175\n",
            "loss:tensor(0.0884, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0527, grad_fn=<MseLossBackward0>)\n",
            "0.052663013339042664\n",
            "loss:tensor(0.1195, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0503, grad_fn=<MseLossBackward0>)\n",
            "0.05030304938554764\n",
            "loss:tensor(0.1580, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0389, grad_fn=<MseLossBackward0>)\n",
            "0.03888687863945961\n",
            "loss:tensor(0.1098, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0320, grad_fn=<MseLossBackward0>)\n",
            "0.032003115862607956\n",
            "loss:tensor(0.0811, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0269, grad_fn=<MseLossBackward0>)\n",
            "0.026941141113638878\n",
            "loss:tensor(0.0976, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0250, grad_fn=<MseLossBackward0>)\n",
            "0.025003202259540558\n",
            "loss:tensor(0.0847, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0301, grad_fn=<MseLossBackward0>)\n",
            "0.03014780953526497\n",
            "loss:tensor(0.0994, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0360, grad_fn=<MseLossBackward0>)\n",
            "0.03601841628551483\n",
            "loss:tensor(0.1150, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0351, grad_fn=<MseLossBackward0>)\n",
            "0.0351458378136158\n",
            "loss:tensor(0.1451, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0290, grad_fn=<MseLossBackward0>)\n",
            "0.02900202013552189\n",
            "loss:tensor(0.0910, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0243, grad_fn=<MseLossBackward0>)\n",
            "0.024259215220808983\n",
            "loss:tensor(0.1003, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0209, grad_fn=<MseLossBackward0>)\n",
            "0.02087632194161415\n",
            "loss:tensor(0.0829, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0196, grad_fn=<MseLossBackward0>)\n",
            "0.01959761045873165\n",
            "loss:tensor(0.0907, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0178, grad_fn=<MseLossBackward0>)\n",
            "0.017844578251242638\n",
            "loss:tensor(0.1167, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0241, grad_fn=<MseLossBackward0>)\n",
            "0.02409343607723713\n",
            "loss:tensor(0.0796, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0329, grad_fn=<MseLossBackward0>)\n",
            "0.03290676325559616\n",
            "loss:tensor(0.0718, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0395, grad_fn=<MseLossBackward0>)\n",
            "0.039526402950286865\n",
            "loss:tensor(0.0984, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0409, grad_fn=<MseLossBackward0>)\n",
            "0.04087938740849495\n",
            "loss:tensor(0.0535, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0422, grad_fn=<MseLossBackward0>)\n",
            "0.04220406711101532\n",
            "loss:tensor(0.0625, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0412, grad_fn=<MseLossBackward0>)\n",
            "0.041179168969392776\n",
            "loss:tensor(0.0969, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0371, grad_fn=<MseLossBackward0>)\n",
            "0.03712574392557144\n",
            "loss:tensor(0.0833, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0328, grad_fn=<MseLossBackward0>)\n",
            "0.03275984898209572\n",
            "loss:tensor(0.0845, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0286, grad_fn=<MseLossBackward0>)\n",
            "0.028620528057217598\n",
            "loss:tensor(0.0773, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0256, grad_fn=<MseLossBackward0>)\n",
            "0.02559083327651024\n",
            "loss:tensor(0.0642, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0244, grad_fn=<MseLossBackward0>)\n",
            "0.02442029118537903\n",
            "loss:tensor(0.1120, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0204, grad_fn=<MseLossBackward0>)\n",
            "0.02035769633948803\n",
            "loss:tensor(0.0529, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0129, grad_fn=<MseLossBackward0>)\n",
            "0.012943652458488941\n",
            "loss:tensor(0.0775, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0127, grad_fn=<MseLossBackward0>)\n",
            "0.012656118720769882\n",
            "loss:tensor(0.0915, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0158, grad_fn=<MseLossBackward0>)\n",
            "0.01583094336092472\n",
            "loss:tensor(0.0556, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0198, grad_fn=<MseLossBackward0>)\n",
            "0.01976335048675537\n",
            "loss:tensor(0.1129, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0252, grad_fn=<MseLossBackward0>)\n",
            "0.025157397612929344\n",
            "loss:tensor(0.1079, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0267, grad_fn=<MseLossBackward0>)\n",
            "0.026677323505282402\n",
            "loss:tensor(0.0398, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0283, grad_fn=<MseLossBackward0>)\n",
            "0.028348684310913086\n",
            "loss:tensor(0.0905, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0272, grad_fn=<MseLossBackward0>)\n",
            "0.027173001319169998\n",
            "loss:tensor(0.0572, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0263, grad_fn=<MseLossBackward0>)\n",
            "0.026302214711904526\n",
            "loss:tensor(0.1530, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0262, grad_fn=<MseLossBackward0>)\n",
            "0.02617720142006874\n",
            "loss:tensor(0.0743, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0239, grad_fn=<MseLossBackward0>)\n",
            "0.02386564575135708\n",
            "loss:tensor(0.0692, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0234, grad_fn=<MseLossBackward0>)\n",
            "0.023364245891571045\n",
            "loss:tensor(0.1044, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0247, grad_fn=<MseLossBackward0>)\n",
            "0.02469380758702755\n",
            "loss:tensor(0.0706, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0278, grad_fn=<MseLossBackward0>)\n",
            "0.027835942804813385\n",
            "loss:tensor(0.0659, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0312, grad_fn=<MseLossBackward0>)\n",
            "0.03124276176095009\n",
            "loss:tensor(0.1108, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0279, grad_fn=<MseLossBackward0>)\n",
            "0.027915947139263153\n",
            "loss:tensor(0.0777, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0234, grad_fn=<MseLossBackward0>)\n",
            "0.02336840145289898\n",
            "loss:tensor(0.0922, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0176, grad_fn=<MseLossBackward0>)\n",
            "0.017592478543519974\n",
            "loss:tensor(0.0886, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0119, grad_fn=<MseLossBackward0>)\n",
            "0.01193455420434475\n",
            "loss:tensor(0.1290, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0095, grad_fn=<MseLossBackward0>)\n",
            "0.009513463824987411\n",
            "loss:tensor(0.0829, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0090, grad_fn=<MseLossBackward0>)\n",
            "0.00900034885853529\n",
            "loss:tensor(0.0843, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0085, grad_fn=<MseLossBackward0>)\n",
            "0.008497252129018307\n",
            "loss:tensor(0.1046, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0091, grad_fn=<MseLossBackward0>)\n",
            "0.00914754904806614\n",
            "loss:tensor(0.0966, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0102, grad_fn=<MseLossBackward0>)\n",
            "0.010176499374210835\n",
            "loss:tensor(0.1174, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0088, grad_fn=<MseLossBackward0>)\n",
            "0.008843583054840565\n",
            "loss:tensor(0.1059, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0087, grad_fn=<MseLossBackward0>)\n",
            "0.00870540551841259\n",
            "loss:tensor(0.1749, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0185, grad_fn=<MseLossBackward0>)\n",
            "0.018548576161265373\n",
            "loss:tensor(0.0873, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0355, grad_fn=<MseLossBackward0>)\n",
            "0.03553328290581703\n",
            "loss:tensor(0.1568, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0556, grad_fn=<MseLossBackward0>)\n",
            "0.0556088425219059\n",
            "loss:tensor(0.0794, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0687, grad_fn=<MseLossBackward0>)\n",
            "0.06867945194244385\n",
            "loss:tensor(0.1348, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0650, grad_fn=<MseLossBackward0>)\n",
            "0.06496746838092804\n",
            "loss:tensor(0.0909, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0623, grad_fn=<MseLossBackward0>)\n",
            "0.0622820109128952\n",
            "loss:tensor(0.0617, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0582, grad_fn=<MseLossBackward0>)\n",
            "0.05817480757832527\n",
            "loss:tensor(0.0992, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0603, grad_fn=<MseLossBackward0>)\n",
            "0.06027933582663536\n",
            "loss:tensor(0.1027, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0644, grad_fn=<MseLossBackward0>)\n",
            "0.0643874928355217\n",
            "loss:tensor(0.0763, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0680, grad_fn=<MseLossBackward0>)\n",
            "0.06797109544277191\n",
            "loss:tensor(0.0763, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0735, grad_fn=<MseLossBackward0>)\n",
            "0.07345117628574371\n",
            "loss:tensor(0.1146, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0759, grad_fn=<MseLossBackward0>)\n",
            "0.0758822038769722\n",
            "loss:tensor(0.1160, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0660, grad_fn=<MseLossBackward0>)\n",
            "0.06596513837575912\n",
            "loss:tensor(0.0687, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0553, grad_fn=<MseLossBackward0>)\n",
            "0.055304743349552155\n",
            "loss:tensor(0.0954, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0418, grad_fn=<MseLossBackward0>)\n",
            "0.041795626282691956\n",
            "loss:tensor(0.1096, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0210, grad_fn=<MseLossBackward0>)\n",
            "0.020950142294168472\n",
            "loss:tensor(0.0604, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0172, grad_fn=<MseLossBackward0>)\n",
            "0.017244776710867882\n",
            "loss:tensor(0.0583, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0384, grad_fn=<MseLossBackward0>)\n",
            "0.03838372230529785\n",
            "loss:tensor(0.0508, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0425, grad_fn=<MseLossBackward0>)\n",
            "0.04252977669239044\n",
            "loss:tensor(0.0857, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0384, grad_fn=<MseLossBackward0>)\n",
            "0.038434576243162155\n",
            "loss:tensor(0.0357, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0351, grad_fn=<MseLossBackward0>)\n",
            "0.035051845014095306\n",
            "loss:tensor(0.1431, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0299, grad_fn=<MseLossBackward0>)\n",
            "0.029876401647925377\n",
            "loss:tensor(0.0671, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0201, grad_fn=<MseLossBackward0>)\n",
            "0.020116690546274185\n",
            "loss:tensor(0.0728, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0162, grad_fn=<MseLossBackward0>)\n",
            "0.01619120128452778\n",
            "loss:tensor(0.0976, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0083, grad_fn=<MseLossBackward0>)\n",
            "0.008328141644597054\n",
            "loss:tensor(0.0546, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0072, grad_fn=<MseLossBackward0>)\n",
            "0.007238604128360748\n",
            "loss:tensor(0.0664, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0295, grad_fn=<MseLossBackward0>)\n",
            "0.02950180694460869\n",
            "loss:tensor(0.0971, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0737, grad_fn=<MseLossBackward0>)\n",
            "0.07368086278438568\n",
            "loss:tensor(0.0630, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0897, grad_fn=<MseLossBackward0>)\n",
            "0.08973057568073273\n",
            "loss:tensor(0.0784, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0765, grad_fn=<MseLossBackward0>)\n",
            "0.07649806886911392\n",
            "loss:tensor(0.0868, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0269, grad_fn=<MseLossBackward0>)\n",
            "0.026924433186650276\n",
            "loss:tensor(0.1070, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0112, grad_fn=<MseLossBackward0>)\n",
            "0.011245313100516796\n",
            "loss:tensor(0.1650, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0080, grad_fn=<MseLossBackward0>)\n",
            "0.007977030239999294\n",
            "loss:tensor(0.0916, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0128, grad_fn=<MseLossBackward0>)\n",
            "0.012834868393838406\n",
            "loss:tensor(0.1175, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0162, grad_fn=<MseLossBackward0>)\n",
            "0.01623380556702614\n",
            "loss:tensor(0.1270, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0148, grad_fn=<MseLossBackward0>)\n",
            "0.014768113382160664\n",
            "loss:tensor(0.1404, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0089, grad_fn=<MseLossBackward0>)\n",
            "0.0088876374065876\n",
            "loss:tensor(0.1614, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0044, grad_fn=<MseLossBackward0>)\n",
            "0.004351408686488867\n",
            "loss:tensor(0.1490, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0112, grad_fn=<MseLossBackward0>)\n",
            "0.011222979985177517\n",
            "loss:tensor(0.1503, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0217, grad_fn=<MseLossBackward0>)\n",
            "0.0217118039727211\n",
            "loss:tensor(0.1744, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0268, grad_fn=<MseLossBackward0>)\n",
            "0.026771364733576775\n",
            "loss:tensor(0.0696, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0330, grad_fn=<MseLossBackward0>)\n",
            "0.03296414762735367\n",
            "loss:tensor(0.0946, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0453, grad_fn=<MseLossBackward0>)\n",
            "0.0452524833381176\n",
            "loss:tensor(0.1139, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0554, grad_fn=<MseLossBackward0>)\n",
            "0.055394742637872696\n",
            "loss:tensor(0.0884, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0657, grad_fn=<MseLossBackward0>)\n",
            "0.06574077159166336\n",
            "loss:tensor(0.1483, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0668, grad_fn=<MseLossBackward0>)\n",
            "0.0668439120054245\n",
            "loss:tensor(0.0965, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0766, grad_fn=<MseLossBackward0>)\n",
            "0.07662949711084366\n",
            "loss:tensor(0.1399, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0972, grad_fn=<MseLossBackward0>)\n",
            "0.09722883254289627\n",
            "loss:tensor(0.0811, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.1251, grad_fn=<MseLossBackward0>)\n",
            "0.1251346915960312\n",
            "loss:tensor(0.0994, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.1643, grad_fn=<MseLossBackward0>)\n",
            "0.1642812341451645\n",
            "loss:tensor(0.0977, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.1810, grad_fn=<MseLossBackward0>)\n",
            "0.18097572028636932\n",
            "loss:tensor(0.1016, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.1911, grad_fn=<MseLossBackward0>)\n",
            "0.19114865362644196\n",
            "loss:tensor(0.1707, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.1553, grad_fn=<MseLossBackward0>)\n",
            "0.15530700981616974\n",
            "loss:tensor(0.1313, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.1110, grad_fn=<MseLossBackward0>)\n",
            "0.11103738844394684\n",
            "loss:tensor(0.0683, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0840, grad_fn=<MseLossBackward0>)\n",
            "0.0839516818523407\n",
            "loss:tensor(0.1424, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0549, grad_fn=<MseLossBackward0>)\n",
            "0.054914962500333786\n",
            "loss:tensor(0.0693, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0297, grad_fn=<MseLossBackward0>)\n",
            "0.029675401747226715\n",
            "loss:tensor(0.1674, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0151, grad_fn=<MseLossBackward0>)\n",
            "0.015125459991395473\n",
            "loss:tensor(0.0523, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0096, grad_fn=<MseLossBackward0>)\n",
            "0.00961389858275652\n",
            "loss:tensor(0.0642, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0063, grad_fn=<MseLossBackward0>)\n",
            "0.006338427308946848\n",
            "loss:tensor(0.1130, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0052, grad_fn=<MseLossBackward0>)\n",
            "0.00519901979714632\n",
            "loss:tensor(0.0734, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0041, grad_fn=<MseLossBackward0>)\n",
            "0.0040584406815469265\n",
            "loss:tensor(0.0890, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0044, grad_fn=<MseLossBackward0>)\n",
            "0.004375437740236521\n",
            "loss:tensor(0.1319, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0068, grad_fn=<MseLossBackward0>)\n",
            "0.006810678634792566\n",
            "loss:tensor(0.0984, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0095, grad_fn=<MseLossBackward0>)\n",
            "0.009528548456728458\n",
            "loss:tensor(0.0561, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0122, grad_fn=<MseLossBackward0>)\n",
            "0.012151363305747509\n",
            "loss:tensor(0.0596, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0135, grad_fn=<MseLossBackward0>)\n",
            "0.013494963757693768\n",
            "loss:tensor(0.0952, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0114, grad_fn=<MseLossBackward0>)\n",
            "0.011415633372962475\n",
            "loss:tensor(0.0788, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0081, grad_fn=<MseLossBackward0>)\n",
            "0.008108759298920631\n",
            "loss:tensor(0.1317, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0069, grad_fn=<MseLossBackward0>)\n",
            "0.006884991656988859\n",
            "loss:tensor(0.1235, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0128, grad_fn=<MseLossBackward0>)\n",
            "0.012774699367582798\n",
            "loss:tensor(0.1080, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0233, grad_fn=<MseLossBackward0>)\n",
            "0.023303091526031494\n",
            "loss:tensor(0.1135, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0223, grad_fn=<MseLossBackward0>)\n",
            "0.02225901000201702\n",
            "loss:tensor(0.1075, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0179, grad_fn=<MseLossBackward0>)\n",
            "0.017857292667031288\n",
            "loss:tensor(0.1254, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0104, grad_fn=<MseLossBackward0>)\n",
            "0.01037322636693716\n",
            "loss:tensor(0.0835, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0051, grad_fn=<MseLossBackward0>)\n",
            "0.005064649041742086\n",
            "loss:tensor(0.1079, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0038, grad_fn=<MseLossBackward0>)\n",
            "0.0038393058348447084\n",
            "loss:tensor(0.0932, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0030, grad_fn=<MseLossBackward0>)\n",
            "0.002991532441228628\n",
            "loss:tensor(0.1119, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0031, grad_fn=<MseLossBackward0>)\n",
            "0.003064181422814727\n",
            "loss:tensor(0.1144, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0035, grad_fn=<MseLossBackward0>)\n",
            "0.0034690536558628082\n",
            "loss:tensor(0.0696, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0035, grad_fn=<MseLossBackward0>)\n",
            "0.003520224243402481\n",
            "loss:tensor(0.0861, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0024, grad_fn=<MseLossBackward0>)\n",
            "0.0023621809668838978\n",
            "loss:tensor(0.0572, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0020, grad_fn=<MseLossBackward0>)\n",
            "0.002009027637541294\n",
            "loss:tensor(0.0891, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0020, grad_fn=<MseLossBackward0>)\n",
            "0.002035727258771658\n",
            "loss:tensor(0.0573, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0024, grad_fn=<MseLossBackward0>)\n",
            "0.002354246564209461\n",
            "loss:tensor(0.0897, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0027, grad_fn=<MseLossBackward0>)\n",
            "0.002678872086107731\n",
            "loss:tensor(0.0753, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0026, grad_fn=<MseLossBackward0>)\n",
            "0.0026357292663306\n",
            "loss:tensor(0.0940, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0021, grad_fn=<MseLossBackward0>)\n",
            "0.002135238377377391\n",
            "loss:tensor(0.1064, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
            "0.0018557618604972959\n",
            "loss:tensor(0.0583, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
            "0.0016891196137294173\n",
            "loss:tensor(0.0640, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
            "0.001392180798575282\n",
            "loss:tensor(0.0933, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
            "0.0014974266523495317\n",
            "loss:tensor(0.0703, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
            "0.0019359812140464783\n",
            "loss:tensor(0.0509, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
            "0.0017037972575053573\n",
            "loss:tensor(0.1084, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
            "0.0019108160631731153\n",
            "loss:tensor(0.0699, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0028, grad_fn=<MseLossBackward0>)\n",
            "0.0028492785058915615\n",
            "loss:tensor(0.0702, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0032, grad_fn=<MseLossBackward0>)\n",
            "0.003159969812259078\n",
            "loss:tensor(0.0834, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0040, grad_fn=<MseLossBackward0>)\n",
            "0.004019313957542181\n",
            "loss:tensor(0.1029, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0081, grad_fn=<MseLossBackward0>)\n",
            "0.00806388445198536\n",
            "loss:tensor(0.0382, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0112, grad_fn=<MseLossBackward0>)\n",
            "0.01115343812853098\n",
            "loss:tensor(0.0680, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0172, grad_fn=<MseLossBackward0>)\n",
            "0.017200056463479996\n",
            "loss:tensor(0.0805, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0253, grad_fn=<MseLossBackward0>)\n",
            "0.025314630940556526\n",
            "loss:tensor(0.0862, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0278, grad_fn=<MseLossBackward0>)\n",
            "0.027757013216614723\n",
            "loss:tensor(0.0631, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0226, grad_fn=<MseLossBackward0>)\n",
            "0.022582143545150757\n",
            "loss:tensor(0.0946, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0141, grad_fn=<MseLossBackward0>)\n",
            "0.014060222543776035\n",
            "loss:tensor(0.1119, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0066, grad_fn=<MseLossBackward0>)\n",
            "0.006595600862056017\n",
            "loss:tensor(0.1006, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0069, grad_fn=<MseLossBackward0>)\n",
            "0.006871924735605717\n",
            "loss:tensor(0.1159, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0147, grad_fn=<MseLossBackward0>)\n",
            "0.014722424559295177\n",
            "loss:tensor(0.1149, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0246, grad_fn=<MseLossBackward0>)\n",
            "0.024631652981042862\n",
            "loss:tensor(0.0791, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0309, grad_fn=<MseLossBackward0>)\n",
            "0.030914854258298874\n",
            "loss:tensor(0.1157, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0292, grad_fn=<MseLossBackward0>)\n",
            "0.029187779873609543\n",
            "loss:tensor(0.1059, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0217, grad_fn=<MseLossBackward0>)\n",
            "0.021728534251451492\n",
            "loss:tensor(0.0848, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0148, grad_fn=<MseLossBackward0>)\n",
            "0.014841600321233273\n",
            "loss:tensor(0.0568, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0084, grad_fn=<MseLossBackward0>)\n",
            "0.008372332900762558\n",
            "loss:tensor(0.1321, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0112, grad_fn=<MseLossBackward0>)\n",
            "0.011196333914995193\n",
            "loss:tensor(0.0695, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0117, grad_fn=<MseLossBackward0>)\n",
            "0.011688628233969212\n",
            "loss:tensor(0.0562, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0115, grad_fn=<MseLossBackward0>)\n",
            "0.011483844369649887\n",
            "loss:tensor(0.1325, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0064, grad_fn=<MseLossBackward0>)\n",
            "0.006353637669235468\n",
            "loss:tensor(0.0610, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0128, grad_fn=<MseLossBackward0>)\n",
            "0.012793028727173805\n",
            "loss:tensor(0.0428, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0338, grad_fn=<MseLossBackward0>)\n",
            "0.03375697880983353\n",
            "loss:tensor(0.0646, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0552, grad_fn=<MseLossBackward0>)\n",
            "0.05523069202899933\n",
            "loss:tensor(0.0806, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0678, grad_fn=<MseLossBackward0>)\n",
            "0.06783534586429596\n",
            "loss:tensor(0.1109, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0696, grad_fn=<MseLossBackward0>)\n",
            "0.0695735290646553\n",
            "loss:tensor(0.0926, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0500, grad_fn=<MseLossBackward0>)\n",
            "0.04995341971516609\n",
            "loss:tensor(0.1066, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0305, grad_fn=<MseLossBackward0>)\n",
            "0.030496928840875626\n",
            "loss:tensor(0.1325, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0213, grad_fn=<MseLossBackward0>)\n",
            "0.021294010803103447\n",
            "loss:tensor(0.0717, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0223, grad_fn=<MseLossBackward0>)\n",
            "0.022344710305333138\n",
            "loss:tensor(0.0650, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0506, grad_fn=<MseLossBackward0>)\n",
            "0.05055316165089607\n",
            "loss:tensor(0.0684, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0664, grad_fn=<MseLossBackward0>)\n",
            "0.066412553191185\n",
            "loss:tensor(0.0817, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0663, grad_fn=<MseLossBackward0>)\n",
            "0.06629186868667603\n",
            "loss:tensor(0.1147, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0515, grad_fn=<MseLossBackward0>)\n",
            "0.05150497704744339\n",
            "loss:tensor(0.1273, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0376, grad_fn=<MseLossBackward0>)\n",
            "0.037599775940179825\n",
            "loss:tensor(0.1225, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0196, grad_fn=<MseLossBackward0>)\n",
            "0.019562695175409317\n",
            "loss:tensor(0.1084, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0079, grad_fn=<MseLossBackward0>)\n",
            "0.007853588089346886\n",
            "loss:tensor(0.1029, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0035, grad_fn=<MseLossBackward0>)\n",
            "0.003485038410872221\n",
            "loss:tensor(0.1598, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0064, grad_fn=<MseLossBackward0>)\n",
            "0.006415780168026686\n",
            "loss:tensor(0.0950, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0149, grad_fn=<MseLossBackward0>)\n",
            "0.014902283437550068\n",
            "loss:tensor(0.1568, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0273, grad_fn=<MseLossBackward0>)\n",
            "0.02731686271727085\n",
            "loss:tensor(0.0962, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0416, grad_fn=<MseLossBackward0>)\n",
            "0.041628383100032806\n",
            "loss:tensor(0.0627, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0529, grad_fn=<MseLossBackward0>)\n",
            "0.05292440578341484\n",
            "loss:tensor(0.0941, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0636, grad_fn=<MseLossBackward0>)\n",
            "0.0635562315583229\n",
            "loss:tensor(0.1212, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0718, grad_fn=<MseLossBackward0>)\n",
            "0.07183771580457687\n",
            "loss:tensor(0.0728, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0741, grad_fn=<MseLossBackward0>)\n",
            "0.07411898672580719\n",
            "loss:tensor(0.1380, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0571, grad_fn=<MseLossBackward0>)\n",
            "0.05705126002430916\n",
            "loss:tensor(0.0770, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0416, grad_fn=<MseLossBackward0>)\n",
            "0.04158820956945419\n",
            "loss:tensor(0.0675, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0269, grad_fn=<MseLossBackward0>)\n",
            "0.026912136003375053\n",
            "loss:tensor(0.1609, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0185, grad_fn=<MseLossBackward0>)\n",
            "0.018481291830539703\n",
            "loss:tensor(0.1709, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0189, grad_fn=<MseLossBackward0>)\n",
            "0.018898189067840576\n",
            "loss:tensor(0.1800, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0269, grad_fn=<MseLossBackward0>)\n",
            "0.02686391957104206\n",
            "loss:tensor(0.1115, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0336, grad_fn=<MseLossBackward0>)\n",
            "0.033604905009269714\n",
            "loss:tensor(0.0800, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0395, grad_fn=<MseLossBackward0>)\n",
            "0.03952690213918686\n",
            "loss:tensor(0.0988, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0493, grad_fn=<MseLossBackward0>)\n",
            "0.04931539297103882\n",
            "loss:tensor(0.0904, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0545, grad_fn=<MseLossBackward0>)\n",
            "0.05451580509543419\n",
            "loss:tensor(0.1209, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0576, grad_fn=<MseLossBackward0>)\n",
            "0.05764498561620712\n",
            "loss:tensor(0.0816, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0562, grad_fn=<MseLossBackward0>)\n",
            "0.0561862587928772\n",
            "loss:tensor(0.1348, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0504, grad_fn=<MseLossBackward0>)\n",
            "0.05036599561572075\n",
            "loss:tensor(0.1814, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0342, grad_fn=<MseLossBackward0>)\n",
            "0.03416188806295395\n",
            "loss:tensor(0.1177, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0166, grad_fn=<MseLossBackward0>)\n",
            "0.016621241346001625\n",
            "loss:tensor(0.2199, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0030, grad_fn=<MseLossBackward0>)\n",
            "0.002953986870124936\n",
            "loss:tensor(0.1684, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0025, grad_fn=<MseLossBackward0>)\n",
            "0.0024730863515287638\n",
            "loss:tensor(0.0745, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0110, grad_fn=<MseLossBackward0>)\n",
            "0.0109667694196105\n",
            "loss:tensor(0.1172, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0196, grad_fn=<MseLossBackward0>)\n",
            "0.019563565030694008\n",
            "loss:tensor(0.1084, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0220, grad_fn=<MseLossBackward0>)\n",
            "0.021983878687024117\n",
            "loss:tensor(0.1690, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0203, grad_fn=<MseLossBackward0>)\n",
            "0.020271383225917816\n",
            "loss:tensor(0.1761, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0165, grad_fn=<MseLossBackward0>)\n",
            "0.01647029258310795\n",
            "loss:tensor(0.1756, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0126, grad_fn=<MseLossBackward0>)\n",
            "0.012558957561850548\n",
            "loss:tensor(0.2560, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0123, grad_fn=<MseLossBackward0>)\n",
            "0.012262338772416115\n",
            "loss:tensor(0.1285, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0183, grad_fn=<MseLossBackward0>)\n",
            "0.018286818638443947\n",
            "loss:tensor(0.1188, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0221, grad_fn=<MseLossBackward0>)\n",
            "0.022142592817544937\n",
            "loss:tensor(0.1263, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0173, grad_fn=<MseLossBackward0>)\n",
            "0.017286570742726326\n",
            "loss:tensor(0.1500, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0124, grad_fn=<MseLossBackward0>)\n",
            "0.01241845078766346\n",
            "loss:tensor(0.1147, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0091, grad_fn=<MseLossBackward0>)\n",
            "0.00908774696290493\n",
            "loss:tensor(0.0964, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0050, grad_fn=<MseLossBackward0>)\n",
            "0.0050227101892232895\n",
            "loss:tensor(0.0805, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
            "0.0017530655022710562\n",
            "loss:tensor(0.1337, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
            "0.0017154968809336424\n",
            "loss:tensor(0.0612, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0045, grad_fn=<MseLossBackward0>)\n",
            "0.004485866986215115\n",
            "loss:tensor(0.1168, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0114, grad_fn=<MseLossBackward0>)\n",
            "0.011426035314798355\n",
            "loss:tensor(0.1196, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0141, grad_fn=<MseLossBackward0>)\n",
            "0.014106946997344494\n",
            "loss:tensor(0.0939, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0135, grad_fn=<MseLossBackward0>)\n",
            "0.013525690883398056\n",
            "loss:tensor(0.1144, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0127, grad_fn=<MseLossBackward0>)\n",
            "0.012688754126429558\n",
            "loss:tensor(0.1333, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0103, grad_fn=<MseLossBackward0>)\n",
            "0.010335911065340042\n",
            "loss:tensor(0.0708, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0045, grad_fn=<MseLossBackward0>)\n",
            "0.004467271734029055\n",
            "loss:tensor(0.0774, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
            "0.0019496879540383816\n",
            "loss:tensor(0.0960, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0034, grad_fn=<MseLossBackward0>)\n",
            "0.0033974319230765104\n",
            "loss:tensor(0.0981, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0074, grad_fn=<MseLossBackward0>)\n",
            "0.0074207778088748455\n",
            "loss:tensor(0.0917, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0154, grad_fn=<MseLossBackward0>)\n",
            "0.01536511816084385\n",
            "loss:tensor(0.1267, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0206, grad_fn=<MseLossBackward0>)\n",
            "0.0205759946256876\n",
            "loss:tensor(0.1125, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0221, grad_fn=<MseLossBackward0>)\n",
            "0.022141307592391968\n",
            "loss:tensor(0.1825, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0144, grad_fn=<MseLossBackward0>)\n",
            "0.014435065910220146\n",
            "loss:tensor(0.1327, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0084, grad_fn=<MseLossBackward0>)\n",
            "0.008402409963309765\n",
            "loss:tensor(0.0880, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0121, grad_fn=<MseLossBackward0>)\n",
            "0.01212153211236\n",
            "loss:tensor(0.1325, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0300, grad_fn=<MseLossBackward0>)\n",
            "0.029956484213471413\n",
            "loss:tensor(0.0918, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0600, grad_fn=<MseLossBackward0>)\n",
            "0.059973131865262985\n",
            "loss:tensor(0.1082, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.1033, grad_fn=<MseLossBackward0>)\n",
            "0.10325297713279724\n",
            "loss:tensor(0.1275, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.1386, grad_fn=<MseLossBackward0>)\n",
            "0.13860556483268738\n",
            "loss:tensor(0.1295, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.1261, grad_fn=<MseLossBackward0>)\n",
            "0.12609617412090302\n",
            "loss:tensor(0.1572, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0857, grad_fn=<MseLossBackward0>)\n",
            "0.0856838971376419\n",
            "loss:tensor(0.0916, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0470, grad_fn=<MseLossBackward0>)\n",
            "0.047032225877046585\n",
            "loss:tensor(0.1112, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0164, grad_fn=<MseLossBackward0>)\n",
            "0.016392529010772705\n",
            "loss:tensor(0.0853, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0030, grad_fn=<MseLossBackward0>)\n",
            "0.002988187363371253\n",
            "loss:tensor(0.0565, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0049, grad_fn=<MseLossBackward0>)\n",
            "0.004913621116429567\n",
            "loss:tensor(0.0600, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0185, grad_fn=<MseLossBackward0>)\n",
            "0.018466683104634285\n",
            "loss:tensor(0.1078, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0249, grad_fn=<MseLossBackward0>)\n",
            "0.024885183200240135\n",
            "loss:tensor(0.1068, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0250, grad_fn=<MseLossBackward0>)\n",
            "0.024974342435598373\n",
            "loss:tensor(0.1689, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0145, grad_fn=<MseLossBackward0>)\n",
            "0.014500067569315434\n",
            "loss:tensor(0.1025, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0079, grad_fn=<MseLossBackward0>)\n",
            "0.007897546514868736\n",
            "loss:tensor(0.1239, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0056, grad_fn=<MseLossBackward0>)\n",
            "0.005551691632717848\n",
            "loss:tensor(0.0572, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0074, grad_fn=<MseLossBackward0>)\n",
            "0.007431334350258112\n",
            "loss:tensor(0.1174, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0207, grad_fn=<MseLossBackward0>)\n",
            "0.020746367052197456\n",
            "loss:tensor(0.0885, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0336, grad_fn=<MseLossBackward0>)\n",
            "0.03358611837029457\n",
            "loss:tensor(0.1782, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0282, grad_fn=<MseLossBackward0>)\n",
            "0.02819470874965191\n",
            "loss:tensor(0.0799, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0028, grad_fn=<MseLossBackward0>)\n",
            "0.0028176908381283283\n",
            "loss:tensor(0.1022, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0197, grad_fn=<MseLossBackward0>)\n",
            "0.0196822602301836\n",
            "loss:tensor(0.1118, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0519, grad_fn=<MseLossBackward0>)\n",
            "0.05193869769573212\n",
            "loss:tensor(0.1653, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.1591, grad_fn=<MseLossBackward0>)\n",
            "0.15907984972000122\n",
            "loss:tensor(0.0753, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.2448, grad_fn=<MseLossBackward0>)\n",
            "0.2448447346687317\n",
            "loss:tensor(0.0629, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.2942, grad_fn=<MseLossBackward0>)\n",
            "0.29423853754997253\n",
            "loss:tensor(0.1415, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.2548, grad_fn=<MseLossBackward0>)\n",
            "0.25475114583969116\n",
            "loss:tensor(0.1361, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.1756, grad_fn=<MseLossBackward0>)\n",
            "0.17561903595924377\n",
            "loss:tensor(0.1647, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.1029, grad_fn=<MseLossBackward0>)\n",
            "0.10288893431425095\n",
            "loss:tensor(0.1286, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0519, grad_fn=<MseLossBackward0>)\n",
            "0.05194588378071785\n",
            "loss:tensor(0.1448, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0233, grad_fn=<MseLossBackward0>)\n",
            "0.0233272984623909\n",
            "loss:tensor(0.1770, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0125, grad_fn=<MseLossBackward0>)\n",
            "0.012511291541159153\n",
            "loss:tensor(0.1163, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0066, grad_fn=<MseLossBackward0>)\n",
            "0.006566993892192841\n",
            "loss:tensor(0.1494, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0037, grad_fn=<MseLossBackward0>)\n",
            "0.0036616986617445946\n",
            "loss:tensor(0.1077, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0032, grad_fn=<MseLossBackward0>)\n",
            "0.00322642270475626\n",
            "loss:tensor(0.0697, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0067, grad_fn=<MseLossBackward0>)\n",
            "0.00669191824272275\n",
            "loss:tensor(0.1137, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0096, grad_fn=<MseLossBackward0>)\n",
            "0.009552146308124065\n",
            "loss:tensor(0.1046, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0072, grad_fn=<MseLossBackward0>)\n",
            "0.007241311948746443\n",
            "loss:tensor(0.1077, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0057, grad_fn=<MseLossBackward0>)\n",
            "0.005746695213019848\n",
            "loss:tensor(0.0671, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0106, grad_fn=<MseLossBackward0>)\n",
            "0.010596376843750477\n",
            "loss:tensor(0.1746, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.1566, grad_fn=<MseLossBackward0>)\n",
            "0.15663817524909973\n",
            "loss:tensor(0.1405, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.1823, grad_fn=<MseLossBackward0>)\n",
            "0.18228091299533844\n",
            "loss:tensor(0.1689, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.1667, grad_fn=<MseLossBackward0>)\n",
            "0.16671939194202423\n",
            "loss:tensor(0.2123, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.1213, grad_fn=<MseLossBackward0>)\n",
            "0.12128055840730667\n",
            "loss:tensor(0.0882, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0906, grad_fn=<MseLossBackward0>)\n",
            "0.09056854248046875\n",
            "loss:tensor(0.1069, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0649, grad_fn=<MseLossBackward0>)\n",
            "0.06491146981716156\n",
            "loss:tensor(0.1929, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0489, grad_fn=<MseLossBackward0>)\n",
            "0.04885941743850708\n",
            "loss:tensor(0.0965, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0372, grad_fn=<MseLossBackward0>)\n",
            "0.037244684994220734\n",
            "loss:tensor(0.1024, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0298, grad_fn=<MseLossBackward0>)\n",
            "0.02977534756064415\n",
            "loss:tensor(0.1772, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0148, grad_fn=<MseLossBackward0>)\n",
            "0.01479780487716198\n",
            "loss:tensor(0.1806, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0093, grad_fn=<MseLossBackward0>)\n",
            "0.009290766902267933\n",
            "loss:tensor(0.1054, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0039, grad_fn=<MseLossBackward0>)\n",
            "0.003943507093936205\n",
            "loss:tensor(0.1207, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
            "0.0014317132299765944\n",
            "loss:tensor(0.1653, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
            "0.0014961709966883063\n",
            "loss:tensor(0.1722, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
            "0.0012386388843879104\n",
            "loss:tensor(0.1195, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0031, grad_fn=<MseLossBackward0>)\n",
            "0.0030977327842265368\n",
            "loss:tensor(0.1813, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0135, grad_fn=<MseLossBackward0>)\n",
            "0.013525058515369892\n",
            "loss:tensor(0.1334, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0250, grad_fn=<MseLossBackward0>)\n",
            "0.024996886029839516\n",
            "loss:tensor(0.1725, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0337, grad_fn=<MseLossBackward0>)\n",
            "0.03367180749773979\n",
            "loss:tensor(0.1408, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0395, grad_fn=<MseLossBackward0>)\n",
            "0.039486873894929886\n",
            "loss:tensor(0.1560, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0334, grad_fn=<MseLossBackward0>)\n",
            "0.03343439847230911\n",
            "loss:tensor(0.1071, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0302, grad_fn=<MseLossBackward0>)\n",
            "0.0301753468811512\n",
            "loss:tensor(0.1401, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0206, grad_fn=<MseLossBackward0>)\n",
            "0.02056826464831829\n",
            "loss:tensor(0.1130, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0174, grad_fn=<MseLossBackward0>)\n",
            "0.017416035756468773\n",
            "loss:tensor(0.1076, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0134, grad_fn=<MseLossBackward0>)\n",
            "0.013412519358098507\n",
            "loss:tensor(0.1388, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0126, grad_fn=<MseLossBackward0>)\n",
            "0.012622945941984653\n",
            "loss:tensor(0.0770, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0099, grad_fn=<MseLossBackward0>)\n",
            "0.00992987398058176\n",
            "loss:tensor(0.0931, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0084, grad_fn=<MseLossBackward0>)\n",
            "0.008389344438910484\n",
            "loss:tensor(0.1487, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0160, grad_fn=<MseLossBackward0>)\n",
            "0.016022292897105217\n",
            "loss:tensor(0.1189, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0315, grad_fn=<MseLossBackward0>)\n",
            "0.03145351633429527\n",
            "loss:tensor(0.1190, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0506, grad_fn=<MseLossBackward0>)\n",
            "0.05061330273747444\n",
            "loss:tensor(0.1113, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0777, grad_fn=<MseLossBackward0>)\n",
            "0.07770245522260666\n",
            "loss:tensor(0.0916, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.1080, grad_fn=<MseLossBackward0>)\n",
            "0.10797291249036789\n",
            "loss:tensor(0.0856, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.1383, grad_fn=<MseLossBackward0>)\n",
            "0.1383049190044403\n",
            "loss:tensor(0.1363, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.1398, grad_fn=<MseLossBackward0>)\n",
            "0.13983991742134094\n",
            "loss:tensor(0.0626, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.1392, grad_fn=<MseLossBackward0>)\n",
            "0.13921503722667694\n",
            "loss:tensor(0.1212, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.1396, grad_fn=<MseLossBackward0>)\n",
            "0.13962118327617645\n",
            "loss:tensor(0.0565, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.1366, grad_fn=<MseLossBackward0>)\n",
            "0.13656404614448547\n",
            "loss:tensor(0.1068, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.1383, grad_fn=<MseLossBackward0>)\n",
            "0.13831430673599243\n",
            "loss:tensor(0.2091, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.1001, grad_fn=<MseLossBackward0>)\n",
            "0.10008791089057922\n",
            "loss:tensor(0.1933, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0678, grad_fn=<MseLossBackward0>)\n",
            "0.06777357310056686\n",
            "loss:tensor(0.1092, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0396, grad_fn=<MseLossBackward0>)\n",
            "0.03960739076137543\n",
            "loss:tensor(0.0965, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0214, grad_fn=<MseLossBackward0>)\n",
            "0.021401632577180862\n",
            "loss:tensor(0.2178, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0133, grad_fn=<MseLossBackward0>)\n",
            "0.013297636061906815\n",
            "loss:tensor(0.1798, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0101, grad_fn=<MseLossBackward0>)\n",
            "0.010145235806703568\n",
            "loss:tensor(0.1236, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0076, grad_fn=<MseLossBackward0>)\n",
            "0.0075795287266373634\n",
            "loss:tensor(0.0990, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0085, grad_fn=<MseLossBackward0>)\n",
            "0.008523178286850452\n",
            "loss:tensor(0.1201, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0136, grad_fn=<MseLossBackward0>)\n",
            "0.013550941832363605\n",
            "loss:tensor(0.1132, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0219, grad_fn=<MseLossBackward0>)\n",
            "0.021886862814426422\n",
            "loss:tensor(0.1292, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0313, grad_fn=<MseLossBackward0>)\n",
            "0.0313289612531662\n",
            "loss:tensor(0.0943, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0388, grad_fn=<MseLossBackward0>)\n",
            "0.03879933059215546\n",
            "loss:tensor(0.1016, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0395, grad_fn=<MseLossBackward0>)\n",
            "0.03951166942715645\n",
            "loss:tensor(0.0602, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0356, grad_fn=<MseLossBackward0>)\n",
            "0.03560074791312218\n",
            "loss:tensor(0.1730, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0234, grad_fn=<MseLossBackward0>)\n",
            "0.023441797122359276\n",
            "loss:tensor(0.1428, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0132, grad_fn=<MseLossBackward0>)\n",
            "0.013217117637395859\n",
            "loss:tensor(0.1231, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0075, grad_fn=<MseLossBackward0>)\n",
            "0.00745459645986557\n",
            "loss:tensor(0.1397, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0059, grad_fn=<MseLossBackward0>)\n",
            "0.005948202684521675\n",
            "loss:tensor(0.1513, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0096, grad_fn=<MseLossBackward0>)\n",
            "0.009646404534578323\n",
            "loss:tensor(0.1119, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0203, grad_fn=<MseLossBackward0>)\n",
            "0.02027672901749611\n",
            "loss:tensor(0.1146, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0574, grad_fn=<MseLossBackward0>)\n",
            "0.05736292898654938\n",
            "loss:tensor(0.1406, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.1249, grad_fn=<MseLossBackward0>)\n",
            "0.12494644522666931\n",
            "loss:tensor(0.0799, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.2023, grad_fn=<MseLossBackward0>)\n",
            "0.20232586562633514\n",
            "loss:tensor(0.1306, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
            "0.25000566244125366\n",
            "loss:tensor(0.1060, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.2655, grad_fn=<MseLossBackward0>)\n",
            "0.26546332240104675\n",
            "loss:tensor(0.0783, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.2795, grad_fn=<MseLossBackward0>)\n",
            "0.27947738766670227\n",
            "loss:tensor(0.0987, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.2493, grad_fn=<MseLossBackward0>)\n",
            "0.2492857277393341\n",
            "loss:tensor(0.0785, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.2240, grad_fn=<MseLossBackward0>)\n",
            "0.22402149438858032\n",
            "loss:tensor(0.0405, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.1994, grad_fn=<MseLossBackward0>)\n",
            "0.19940726459026337\n",
            "loss:tensor(0.0994, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.1694, grad_fn=<MseLossBackward0>)\n",
            "0.16940699517726898\n",
            "loss:tensor(0.1105, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.1312, grad_fn=<MseLossBackward0>)\n",
            "0.1311958283185959\n",
            "loss:tensor(0.1250, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0794, grad_fn=<MseLossBackward0>)\n",
            "0.07941961288452148\n",
            "loss:tensor(0.0602, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0424, grad_fn=<MseLossBackward0>)\n",
            "0.0424237884581089\n",
            "loss:tensor(0.0789, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0158, grad_fn=<MseLossBackward0>)\n",
            "0.015848901122808456\n",
            "loss:tensor(0.1072, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0069, grad_fn=<MseLossBackward0>)\n",
            "0.006916974671185017\n",
            "loss:tensor(0.1136, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0118, grad_fn=<MseLossBackward0>)\n",
            "0.01176859624683857\n",
            "loss:tensor(0.1157, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0454, grad_fn=<MseLossBackward0>)\n",
            "0.04544723033905029\n",
            "loss:tensor(0.1451, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.1469, grad_fn=<MseLossBackward0>)\n",
            "0.1468503326177597\n",
            "loss:tensor(0.1327, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.1223, grad_fn=<MseLossBackward0>)\n",
            "0.12230940163135529\n",
            "loss:tensor(0.1501, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0819, grad_fn=<MseLossBackward0>)\n",
            "0.08193245530128479\n",
            "loss:tensor(0.0827, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0510, grad_fn=<MseLossBackward0>)\n",
            "0.05096618831157684\n",
            "loss:tensor(0.0779, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0277, grad_fn=<MseLossBackward0>)\n",
            "0.027738312259316444\n",
            "loss:tensor(0.0836, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0168, grad_fn=<MseLossBackward0>)\n",
            "0.016802852973341942\n",
            "loss:tensor(0.0813, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0505, grad_fn=<MseLossBackward0>)\n",
            "0.050492480397224426\n",
            "loss:tensor(0.0597, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0822, grad_fn=<MseLossBackward0>)\n",
            "0.0822312980890274\n",
            "loss:tensor(0.1333, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.1052, grad_fn=<MseLossBackward0>)\n",
            "0.1052192971110344\n",
            "loss:tensor(0.0981, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.1350, grad_fn=<MseLossBackward0>)\n",
            "0.13498900830745697\n",
            "loss:tensor(0.1649, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.1526, grad_fn=<MseLossBackward0>)\n",
            "0.15258830785751343\n",
            "loss:tensor(0.1204, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.1600, grad_fn=<MseLossBackward0>)\n",
            "0.1600044071674347\n",
            "loss:tensor(0.1259, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.1667, grad_fn=<MseLossBackward0>)\n",
            "0.16673293709754944\n",
            "loss:tensor(0.1453, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.1437, grad_fn=<MseLossBackward0>)\n",
            "0.1436980813741684\n",
            "loss:tensor(0.1245, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.1117, grad_fn=<MseLossBackward0>)\n",
            "0.11174894869327545\n",
            "loss:tensor(0.1206, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0521, grad_fn=<MseLossBackward0>)\n",
            "0.052105315029621124\n",
            "loss:tensor(0.1029, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0159, grad_fn=<MseLossBackward0>)\n",
            "0.01587498188018799\n",
            "loss:tensor(0.0574, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0369, grad_fn=<MseLossBackward0>)\n",
            "0.03692459687590599\n",
            "loss:tensor(0.0925, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0599, grad_fn=<MseLossBackward0>)\n",
            "0.059880275279283524\n",
            "loss:tensor(0.1038, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0919, grad_fn=<MseLossBackward0>)\n",
            "0.0919441282749176\n",
            "loss:tensor(0.1129, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.1190, grad_fn=<MseLossBackward0>)\n",
            "0.1189725399017334\n",
            "loss:tensor(0.0932, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.1312, grad_fn=<MseLossBackward0>)\n",
            "0.13121776282787323\n",
            "loss:tensor(0.0505, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.1270, grad_fn=<MseLossBackward0>)\n",
            "0.12700572609901428\n",
            "loss:tensor(0.1594, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.1138, grad_fn=<MseLossBackward0>)\n",
            "0.11381364613771439\n",
            "loss:tensor(0.1117, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0907, grad_fn=<MseLossBackward0>)\n",
            "0.09065139293670654\n",
            "loss:tensor(0.1025, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0596, grad_fn=<MseLossBackward0>)\n",
            "0.05956518277525902\n",
            "loss:tensor(0.0646, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0395, grad_fn=<MseLossBackward0>)\n",
            "0.03950343653559685\n",
            "loss:tensor(0.0458, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0221, grad_fn=<MseLossBackward0>)\n",
            "0.02205202542245388\n",
            "loss:tensor(0.0782, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0084, grad_fn=<MseLossBackward0>)\n",
            "0.008449781686067581\n",
            "loss:tensor(0.0512, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0191, grad_fn=<MseLossBackward0>)\n",
            "0.0190774854272604\n",
            "loss:tensor(0.0593, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0276, grad_fn=<MseLossBackward0>)\n",
            "0.027642952278256416\n",
            "loss:tensor(0.0461, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0347, grad_fn=<MseLossBackward0>)\n",
            "0.0347297340631485\n",
            "loss:tensor(0.1031, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0448, grad_fn=<MseLossBackward0>)\n",
            "0.04478902742266655\n",
            "loss:tensor(0.1006, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0629, grad_fn=<MseLossBackward0>)\n",
            "0.06294496357440948\n",
            "loss:tensor(0.0777, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0723, grad_fn=<MseLossBackward0>)\n",
            "0.07226204872131348\n",
            "loss:tensor(0.0971, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0753, grad_fn=<MseLossBackward0>)\n",
            "0.07528945058584213\n",
            "loss:tensor(0.1050, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0746, grad_fn=<MseLossBackward0>)\n",
            "0.07457225769758224\n",
            "loss:tensor(0.0980, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0767, grad_fn=<MseLossBackward0>)\n",
            "0.07671370357275009\n",
            "loss:tensor(0.0774, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0822, grad_fn=<MseLossBackward0>)\n",
            "0.08215039223432541\n",
            "loss:tensor(0.0477, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0853, grad_fn=<MseLossBackward0>)\n",
            "0.0852716937661171\n",
            "loss:tensor(0.0608, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0853, grad_fn=<MseLossBackward0>)\n",
            "0.08527017384767532\n",
            "loss:tensor(0.0716, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0782, grad_fn=<MseLossBackward0>)\n",
            "0.07820223271846771\n",
            "loss:tensor(0.0830, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0701, grad_fn=<MseLossBackward0>)\n",
            "0.07011408358812332\n",
            "loss:tensor(0.1228, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0520, grad_fn=<MseLossBackward0>)\n",
            "0.052021320909261703\n",
            "loss:tensor(0.0882, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0352, grad_fn=<MseLossBackward0>)\n",
            "0.03519262745976448\n",
            "loss:tensor(0.0902, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0253, grad_fn=<MseLossBackward0>)\n",
            "0.025310078635811806\n",
            "loss:tensor(0.0862, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0163, grad_fn=<MseLossBackward0>)\n",
            "0.016295814886689186\n",
            "loss:tensor(0.0456, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0130, grad_fn=<MseLossBackward0>)\n",
            "0.01295185275375843\n",
            "loss:tensor(0.0702, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0130, grad_fn=<MseLossBackward0>)\n",
            "0.013012050651013851\n",
            "loss:tensor(0.0529, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0124, grad_fn=<MseLossBackward0>)\n",
            "0.012382169254124165\n",
            "loss:tensor(0.1400, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0114, grad_fn=<MseLossBackward0>)\n",
            "0.011372935026884079\n",
            "loss:tensor(0.0894, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0119, grad_fn=<MseLossBackward0>)\n",
            "0.011930000968277454\n",
            "loss:tensor(0.0819, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0133, grad_fn=<MseLossBackward0>)\n",
            "0.013280630111694336\n",
            "loss:tensor(0.1671, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0166, grad_fn=<MseLossBackward0>)\n",
            "0.016566207632422447\n",
            "loss:tensor(0.1391, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0166, grad_fn=<MseLossBackward0>)\n",
            "0.016619311645627022\n",
            "loss:tensor(0.0883, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0143, grad_fn=<MseLossBackward0>)\n",
            "0.01427967194467783\n",
            "loss:tensor(0.0706, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0108, grad_fn=<MseLossBackward0>)\n",
            "0.010764551348984241\n",
            "loss:tensor(0.1011, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0086, grad_fn=<MseLossBackward0>)\n",
            "0.00861653033643961\n",
            "loss:tensor(0.1283, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0073, grad_fn=<MseLossBackward0>)\n",
            "0.007282726466655731\n",
            "loss:tensor(0.0669, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0083, grad_fn=<MseLossBackward0>)\n",
            "0.00831916369497776\n",
            "loss:tensor(0.0555, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0098, grad_fn=<MseLossBackward0>)\n",
            "0.009802314452826977\n",
            "loss:tensor(0.0670, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0127, grad_fn=<MseLossBackward0>)\n",
            "0.012749151326715946\n",
            "loss:tensor(0.0934, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0159, grad_fn=<MseLossBackward0>)\n",
            "0.015888797119259834\n",
            "loss:tensor(0.1071, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0254, grad_fn=<MseLossBackward0>)\n",
            "0.025411903858184814\n",
            "loss:tensor(0.1173, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0229, grad_fn=<MseLossBackward0>)\n",
            "0.02294160984456539\n",
            "loss:tensor(0.0696, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0213, grad_fn=<MseLossBackward0>)\n",
            "0.021312586963176727\n",
            "loss:tensor(0.0600, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0183, grad_fn=<MseLossBackward0>)\n",
            "0.018281465396285057\n",
            "loss:tensor(0.0743, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0160, grad_fn=<MseLossBackward0>)\n",
            "0.015984421595931053\n",
            "loss:tensor(0.1218, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0121, grad_fn=<MseLossBackward0>)\n",
            "0.01205403171479702\n",
            "loss:tensor(0.0759, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0125, grad_fn=<MseLossBackward0>)\n",
            "0.01254935935139656\n",
            "loss:tensor(0.0529, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0078, grad_fn=<MseLossBackward0>)\n",
            "0.0077636088244616985\n",
            "loss:tensor(0.1336, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0055, grad_fn=<MseLossBackward0>)\n",
            "0.005467191804200411\n",
            "loss:tensor(0.0520, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0036, grad_fn=<MseLossBackward0>)\n",
            "0.0035970823373645544\n",
            "loss:tensor(0.0585, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0022, grad_fn=<MseLossBackward0>)\n",
            "0.002218397567048669\n",
            "loss:tensor(0.0811, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
            "0.0017472559120506048\n",
            "loss:tensor(0.0449, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
            "0.0011704264907166362\n",
            "loss:tensor(0.0944, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
            "0.0012285319389775395\n",
            "loss:tensor(0.0969, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
            "0.0010516057955101132\n",
            "loss:tensor(0.0897, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0023, grad_fn=<MseLossBackward0>)\n",
            "0.0022628153674304485\n",
            "loss:tensor(0.0566, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0053, grad_fn=<MseLossBackward0>)\n",
            "0.005261430516839027\n",
            "loss:tensor(0.0628, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0044, grad_fn=<MseLossBackward0>)\n",
            "0.004375663585960865\n",
            "loss:tensor(0.0734, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0028, grad_fn=<MseLossBackward0>)\n",
            "0.0027805939316749573\n",
            "loss:tensor(0.0507, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
            "0.001906269695609808\n",
            "loss:tensor(0.0469, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
            "0.000851040706038475\n",
            "loss:tensor(0.0996, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
            "0.0007055018213577569\n",
            "loss:tensor(0.0776, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0023, grad_fn=<MseLossBackward0>)\n",
            "0.002290570642799139\n",
            "loss:tensor(0.0889, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0048, grad_fn=<MseLossBackward0>)\n",
            "0.004832962527871132\n",
            "loss:tensor(0.0952, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0057, grad_fn=<MseLossBackward0>)\n",
            "0.005735489539802074\n",
            "loss:tensor(0.0910, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0050, grad_fn=<MseLossBackward0>)\n",
            "0.0050292606465518475\n",
            "loss:tensor(0.0792, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0035, grad_fn=<MseLossBackward0>)\n",
            "0.0034533930011093616\n",
            "loss:tensor(0.0569, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0041, grad_fn=<MseLossBackward0>)\n",
            "0.0040876674465835094\n",
            "loss:tensor(0.1058, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0056, grad_fn=<MseLossBackward0>)\n",
            "0.005557751748710871\n",
            "loss:tensor(0.0555, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0096, grad_fn=<MseLossBackward0>)\n",
            "0.009560133330523968\n",
            "loss:tensor(0.0903, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0141, grad_fn=<MseLossBackward0>)\n",
            "0.014093402773141861\n",
            "loss:tensor(0.0893, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0170, grad_fn=<MseLossBackward0>)\n",
            "0.017005689442157745\n",
            "loss:tensor(0.0455, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0202, grad_fn=<MseLossBackward0>)\n",
            "0.020154405385255814\n",
            "loss:tensor(0.0679, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0213, grad_fn=<MseLossBackward0>)\n",
            "0.021272560581564903\n",
            "loss:tensor(0.0941, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0202, grad_fn=<MseLossBackward0>)\n",
            "0.020207440480589867\n",
            "loss:tensor(0.0749, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0209, grad_fn=<MseLossBackward0>)\n",
            "0.020869040861725807\n",
            "loss:tensor(0.1013, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0233, grad_fn=<MseLossBackward0>)\n",
            "0.023255307227373123\n",
            "loss:tensor(0.0655, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0257, grad_fn=<MseLossBackward0>)\n",
            "0.025668052956461906\n",
            "loss:tensor(0.0402, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0370, grad_fn=<MseLossBackward0>)\n",
            "0.03698763996362686\n",
            "loss:tensor(0.0537, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0591, grad_fn=<MseLossBackward0>)\n",
            "0.05909937992691994\n",
            "loss:tensor(0.0890, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0781, grad_fn=<MseLossBackward0>)\n",
            "0.07811615616083145\n",
            "loss:tensor(0.0630, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0840, grad_fn=<MseLossBackward0>)\n",
            "0.08401086181402206\n",
            "loss:tensor(0.0833, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0879, grad_fn=<MseLossBackward0>)\n",
            "0.08787289261817932\n",
            "loss:tensor(0.0921, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0864, grad_fn=<MseLossBackward0>)\n",
            "0.08643531054258347\n",
            "loss:tensor(0.0846, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0712, grad_fn=<MseLossBackward0>)\n",
            "0.07119091600179672\n",
            "loss:tensor(0.1338, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0455, grad_fn=<MseLossBackward0>)\n",
            "0.045534614473581314\n",
            "loss:tensor(0.0439, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0279, grad_fn=<MseLossBackward0>)\n",
            "0.02787127159535885\n",
            "loss:tensor(0.0537, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0222, grad_fn=<MseLossBackward0>)\n",
            "0.02219417877495289\n",
            "loss:tensor(0.0477, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0188, grad_fn=<MseLossBackward0>)\n",
            "0.01878628320991993\n",
            "loss:tensor(0.0753, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0146, grad_fn=<MseLossBackward0>)\n",
            "0.014605879783630371\n",
            "loss:tensor(0.0551, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0114, grad_fn=<MseLossBackward0>)\n",
            "0.011360102333128452\n",
            "loss:tensor(0.0999, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0080, grad_fn=<MseLossBackward0>)\n",
            "0.00797083880752325\n",
            "loss:tensor(0.0839, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0028, grad_fn=<MseLossBackward0>)\n",
            "0.0027504886966198683\n",
            "loss:tensor(0.0456, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0051, grad_fn=<MseLossBackward0>)\n",
            "0.0051128254272043705\n",
            "loss:tensor(0.0848, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0086, grad_fn=<MseLossBackward0>)\n",
            "0.00857068132609129\n",
            "loss:tensor(0.0632, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0084, grad_fn=<MseLossBackward0>)\n",
            "0.008389228954911232\n",
            "loss:tensor(0.0625, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0085, grad_fn=<MseLossBackward0>)\n",
            "0.008519736118614674\n",
            "loss:tensor(0.0850, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0088, grad_fn=<MseLossBackward0>)\n",
            "0.008814845234155655\n",
            "loss:tensor(0.1550, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0086, grad_fn=<MseLossBackward0>)\n",
            "0.008598137646913528\n",
            "loss:tensor(0.0417, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0098, grad_fn=<MseLossBackward0>)\n",
            "0.009824861772358418\n",
            "loss:tensor(0.0915, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0092, grad_fn=<MseLossBackward0>)\n",
            "0.009186803363263607\n",
            "loss:tensor(0.0991, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0078, grad_fn=<MseLossBackward0>)\n",
            "0.007799099665135145\n",
            "loss:tensor(0.0673, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0065, grad_fn=<MseLossBackward0>)\n",
            "0.00648542819544673\n",
            "loss:tensor(0.0667, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0047, grad_fn=<MseLossBackward0>)\n",
            "0.004730128683149815\n",
            "loss:tensor(0.0823, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0026, grad_fn=<MseLossBackward0>)\n",
            "0.0026473042089492083\n",
            "loss:tensor(0.0849, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
            "0.0017755053704604506\n",
            "loss:tensor(0.0983, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
            "0.0012563248164951801\n",
            "loss:tensor(0.1093, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
            "0.0008598166168667376\n",
            "loss:tensor(0.0793, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
            "0.0018273212481290102\n",
            "loss:tensor(0.1152, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0050, grad_fn=<MseLossBackward0>)\n",
            "0.004989834036678076\n",
            "loss:tensor(0.0544, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0090, grad_fn=<MseLossBackward0>)\n",
            "0.008996164426207542\n",
            "loss:tensor(0.1064, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0125, grad_fn=<MseLossBackward0>)\n",
            "0.012520461343228817\n",
            "loss:tensor(0.0418, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0127, grad_fn=<MseLossBackward0>)\n",
            "0.012709925882518291\n",
            "loss:tensor(0.0753, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0089, grad_fn=<MseLossBackward0>)\n",
            "0.008881209418177605\n",
            "loss:tensor(0.0509, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0052, grad_fn=<MseLossBackward0>)\n",
            "0.005226604640483856\n",
            "loss:tensor(0.0752, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0025, grad_fn=<MseLossBackward0>)\n",
            "0.0025287142489105463\n",
            "loss:tensor(0.0684, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
            "0.0012476466363295913\n",
            "loss:tensor(0.0818, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
            "0.0011690609389916062\n",
            "loss:tensor(0.0905, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0024, grad_fn=<MseLossBackward0>)\n",
            "0.0023588144686073065\n",
            "loss:tensor(0.1185, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0039, grad_fn=<MseLossBackward0>)\n",
            "0.0038798311725258827\n",
            "loss:tensor(0.0574, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0050, grad_fn=<MseLossBackward0>)\n",
            "0.005003179889172316\n",
            "loss:tensor(0.0776, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0067, grad_fn=<MseLossBackward0>)\n",
            "0.006691403687000275\n",
            "loss:tensor(0.1388, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0089, grad_fn=<MseLossBackward0>)\n",
            "0.008896623738110065\n",
            "loss:tensor(0.0756, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0116, grad_fn=<MseLossBackward0>)\n",
            "0.011555018834769726\n",
            "loss:tensor(0.0580, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0146, grad_fn=<MseLossBackward0>)\n",
            "0.014627995900809765\n",
            "loss:tensor(0.0653, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0161, grad_fn=<MseLossBackward0>)\n",
            "0.016146399080753326\n",
            "loss:tensor(0.0482, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0199, grad_fn=<MseLossBackward0>)\n",
            "0.01989809237420559\n",
            "loss:tensor(0.0815, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0249, grad_fn=<MseLossBackward0>)\n",
            "0.024906087666749954\n",
            "loss:tensor(0.0426, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0296, grad_fn=<MseLossBackward0>)\n",
            "0.02960963174700737\n",
            "loss:tensor(0.0608, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0314, grad_fn=<MseLossBackward0>)\n",
            "0.031427737325429916\n",
            "loss:tensor(0.0806, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0312, grad_fn=<MseLossBackward0>)\n",
            "0.031164294108748436\n",
            "loss:tensor(0.0974, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0300, grad_fn=<MseLossBackward0>)\n",
            "0.029997443780303\n",
            "loss:tensor(0.0963, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0244, grad_fn=<MseLossBackward0>)\n",
            "0.024366963654756546\n",
            "loss:tensor(0.0645, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0172, grad_fn=<MseLossBackward0>)\n",
            "0.017196564003825188\n",
            "loss:tensor(0.0714, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0097, grad_fn=<MseLossBackward0>)\n",
            "0.009739161469042301\n",
            "loss:tensor(0.0868, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0045, grad_fn=<MseLossBackward0>)\n",
            "0.004543551709502935\n",
            "loss:tensor(0.0519, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
            "0.0019484877120703459\n",
            "loss:tensor(0.0546, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0025, grad_fn=<MseLossBackward0>)\n",
            "0.0024616732262074947\n",
            "loss:tensor(0.0539, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0055, grad_fn=<MseLossBackward0>)\n",
            "0.005488419905304909\n",
            "loss:tensor(0.1249, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0114, grad_fn=<MseLossBackward0>)\n",
            "0.011375983245670795\n",
            "loss:tensor(0.0543, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0234, grad_fn=<MseLossBackward0>)\n",
            "0.023364974185824394\n",
            "loss:tensor(0.0647, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0343, grad_fn=<MseLossBackward0>)\n",
            "0.03431824594736099\n",
            "loss:tensor(0.1408, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0251, grad_fn=<MseLossBackward0>)\n",
            "0.02513551153242588\n",
            "loss:tensor(0.1290, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0145, grad_fn=<MseLossBackward0>)\n",
            "0.01453693863004446\n",
            "loss:tensor(0.1053, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0064, grad_fn=<MseLossBackward0>)\n",
            "0.006428101100027561\n",
            "loss:tensor(0.0577, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
            "0.0017393487505614758\n",
            "loss:tensor(0.1102, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
            "0.0012787714367732406\n",
            "loss:tensor(0.0931, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0059, grad_fn=<MseLossBackward0>)\n",
            "0.005858161952346563\n",
            "loss:tensor(0.2038, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0142, grad_fn=<MseLossBackward0>)\n",
            "0.014217623509466648\n",
            "loss:tensor(0.0653, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0250, grad_fn=<MseLossBackward0>)\n",
            "0.025026308372616768\n",
            "loss:tensor(0.1758, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0322, grad_fn=<MseLossBackward0>)\n",
            "0.03218800202012062\n",
            "loss:tensor(0.2222, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0350, grad_fn=<MseLossBackward0>)\n",
            "0.03504341095685959\n",
            "loss:tensor(0.1274, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0350, grad_fn=<MseLossBackward0>)\n",
            "0.03501337766647339\n",
            "loss:tensor(0.0734, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0305, grad_fn=<MseLossBackward0>)\n",
            "0.030522508546710014\n",
            "loss:tensor(0.1383, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0225, grad_fn=<MseLossBackward0>)\n",
            "0.022487876936793327\n",
            "loss:tensor(0.1475, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0199, grad_fn=<MseLossBackward0>)\n",
            "0.019886702299118042\n",
            "loss:tensor(0.1078, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0227, grad_fn=<MseLossBackward0>)\n",
            "0.022735094651579857\n",
            "loss:tensor(0.0726, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0246, grad_fn=<MseLossBackward0>)\n",
            "0.024629810824990273\n",
            "loss:tensor(0.0949, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0244, grad_fn=<MseLossBackward0>)\n",
            "0.024442758411169052\n",
            "loss:tensor(0.1218, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0184, grad_fn=<MseLossBackward0>)\n",
            "0.018372178077697754\n",
            "loss:tensor(0.1180, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0118, grad_fn=<MseLossBackward0>)\n",
            "0.011753259226679802\n",
            "loss:tensor(0.1148, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0123, grad_fn=<MseLossBackward0>)\n",
            "0.012298700399696827\n",
            "loss:tensor(0.0568, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0127, grad_fn=<MseLossBackward0>)\n",
            "0.012734445743262768\n",
            "loss:tensor(0.0672, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0081, grad_fn=<MseLossBackward0>)\n",
            "0.008114339783787727\n",
            "loss:tensor(0.1110, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0077, grad_fn=<MseLossBackward0>)\n",
            "0.007730044424533844\n",
            "loss:tensor(0.1289, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0079, grad_fn=<MseLossBackward0>)\n",
            "0.007918372750282288\n",
            "loss:tensor(0.0608, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0077, grad_fn=<MseLossBackward0>)\n",
            "0.007738098967820406\n",
            "loss:tensor(0.1324, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0092, grad_fn=<MseLossBackward0>)\n",
            "0.009192092344164848\n",
            "loss:tensor(0.0649, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0129, grad_fn=<MseLossBackward0>)\n",
            "0.012876962311565876\n",
            "loss:tensor(0.0461, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0139, grad_fn=<MseLossBackward0>)\n",
            "0.013948333449661732\n",
            "loss:tensor(0.1072, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0111, grad_fn=<MseLossBackward0>)\n",
            "0.011121095158159733\n",
            "loss:tensor(0.1434, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0085, grad_fn=<MseLossBackward0>)\n",
            "0.008517277427017689\n",
            "loss:tensor(0.1569, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0070, grad_fn=<MseLossBackward0>)\n",
            "0.007032377645373344\n",
            "loss:tensor(0.0967, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0060, grad_fn=<MseLossBackward0>)\n",
            "0.006009318865835667\n",
            "loss:tensor(0.0544, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0038, grad_fn=<MseLossBackward0>)\n",
            "0.0038161305710673332\n",
            "loss:tensor(0.1086, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0024, grad_fn=<MseLossBackward0>)\n",
            "0.002441314049065113\n",
            "loss:tensor(0.1221, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
            "0.0016492282738909125\n",
            "loss:tensor(0.1547, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0027, grad_fn=<MseLossBackward0>)\n",
            "0.0026823098305612803\n",
            "loss:tensor(0.1135, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0058, grad_fn=<MseLossBackward0>)\n",
            "0.005776305217295885\n",
            "loss:tensor(0.1273, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0098, grad_fn=<MseLossBackward0>)\n",
            "0.00981382466852665\n",
            "loss:tensor(0.0454, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0152, grad_fn=<MseLossBackward0>)\n",
            "0.015198391862213612\n",
            "loss:tensor(0.0417, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0190, grad_fn=<MseLossBackward0>)\n",
            "0.019018011167645454\n",
            "loss:tensor(0.0791, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0245, grad_fn=<MseLossBackward0>)\n",
            "0.02449760213494301\n",
            "loss:tensor(0.1401, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0340, grad_fn=<MseLossBackward0>)\n",
            "0.03399769961833954\n",
            "loss:tensor(0.1363, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0572, grad_fn=<MseLossBackward0>)\n",
            "0.0571528859436512\n",
            "loss:tensor(0.1193, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0732, grad_fn=<MseLossBackward0>)\n",
            "0.07315140217542648\n",
            "loss:tensor(0.1146, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0654, grad_fn=<MseLossBackward0>)\n",
            "0.06539950519800186\n",
            "loss:tensor(0.0943, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0528, grad_fn=<MseLossBackward0>)\n",
            "0.05281606689095497\n",
            "loss:tensor(0.1747, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0323, grad_fn=<MseLossBackward0>)\n",
            "0.03234262764453888\n",
            "loss:tensor(0.1688, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0150, grad_fn=<MseLossBackward0>)\n",
            "0.015003987587988377\n",
            "loss:tensor(0.0436, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0084, grad_fn=<MseLossBackward0>)\n",
            "0.00839040894061327\n",
            "loss:tensor(0.1536, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0094, grad_fn=<MseLossBackward0>)\n",
            "0.009431155398488045\n",
            "loss:tensor(0.0794, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0115, grad_fn=<MseLossBackward0>)\n",
            "0.011534247547388077\n",
            "loss:tensor(0.1894, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0123, grad_fn=<MseLossBackward0>)\n",
            "0.012325980700552464\n",
            "loss:tensor(0.1269, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0133, grad_fn=<MseLossBackward0>)\n",
            "0.013295634649693966\n",
            "loss:tensor(0.0528, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0161, grad_fn=<MseLossBackward0>)\n",
            "0.016142474487423897\n",
            "loss:tensor(0.0548, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0206, grad_fn=<MseLossBackward0>)\n",
            "0.020580394193530083\n",
            "loss:tensor(0.0870, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0249, grad_fn=<MseLossBackward0>)\n",
            "0.024942142888903618\n",
            "loss:tensor(0.0769, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0277, grad_fn=<MseLossBackward0>)\n",
            "0.02770104818046093\n",
            "loss:tensor(0.1164, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0388, grad_fn=<MseLossBackward0>)\n",
            "0.038824740797281265\n",
            "loss:tensor(0.1425, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0449, grad_fn=<MseLossBackward0>)\n",
            "0.04487886652350426\n",
            "loss:tensor(0.1797, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0412, grad_fn=<MseLossBackward0>)\n",
            "0.041211891919374466\n",
            "loss:tensor(0.1137, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0414, grad_fn=<MseLossBackward0>)\n",
            "0.04137995094060898\n",
            "loss:tensor(0.0995, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0365, grad_fn=<MseLossBackward0>)\n",
            "0.036468248814344406\n",
            "loss:tensor(0.0692, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0243, grad_fn=<MseLossBackward0>)\n",
            "0.02427685633301735\n",
            "loss:tensor(0.0872, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0134, grad_fn=<MseLossBackward0>)\n",
            "0.013428429141640663\n",
            "loss:tensor(0.1029, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0322, grad_fn=<MseLossBackward0>)\n",
            "0.032220032066106796\n",
            "loss:tensor(0.1109, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0634, grad_fn=<MseLossBackward0>)\n",
            "0.06344780325889587\n",
            "loss:tensor(0.1397, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0744, grad_fn=<MseLossBackward0>)\n",
            "0.07438637316226959\n",
            "loss:tensor(0.1099, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0690, grad_fn=<MseLossBackward0>)\n",
            "0.06904848664999008\n",
            "loss:tensor(0.0544, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0648, grad_fn=<MseLossBackward0>)\n",
            "0.06482262164354324\n",
            "loss:tensor(0.1225, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0518, grad_fn=<MseLossBackward0>)\n",
            "0.051798127591609955\n",
            "loss:tensor(0.1002, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0495, grad_fn=<MseLossBackward0>)\n",
            "0.04949505627155304\n",
            "loss:tensor(0.1128, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.1105, grad_fn=<MseLossBackward0>)\n",
            "0.11051924526691437\n",
            "loss:tensor(0.1264, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.1114, grad_fn=<MseLossBackward0>)\n",
            "0.11143449693918228\n",
            "loss:tensor(0.1563, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0966, grad_fn=<MseLossBackward0>)\n",
            "0.09663075953722\n",
            "loss:tensor(0.1044, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0762, grad_fn=<MseLossBackward0>)\n",
            "0.0762399435043335\n",
            "loss:tensor(0.0854, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0515, grad_fn=<MseLossBackward0>)\n",
            "0.05151929706335068\n",
            "loss:tensor(0.0947, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0264, grad_fn=<MseLossBackward0>)\n",
            "0.026365365833044052\n",
            "loss:tensor(0.1309, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0179, grad_fn=<MseLossBackward0>)\n",
            "0.01786900870501995\n",
            "loss:tensor(0.1335, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0215, grad_fn=<MseLossBackward0>)\n",
            "0.021510763093829155\n",
            "loss:tensor(0.1025, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0292, grad_fn=<MseLossBackward0>)\n",
            "0.0291709303855896\n",
            "loss:tensor(0.1240, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0552, grad_fn=<MseLossBackward0>)\n",
            "0.05519000068306923\n",
            "loss:tensor(0.0903, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0880, grad_fn=<MseLossBackward0>)\n",
            "0.08797033131122589\n",
            "loss:tensor(0.0601, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.1275, grad_fn=<MseLossBackward0>)\n",
            "0.1275314837694168\n",
            "loss:tensor(0.0688, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.1682, grad_fn=<MseLossBackward0>)\n",
            "0.16819068789482117\n",
            "loss:tensor(0.1062, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.1920, grad_fn=<MseLossBackward0>)\n",
            "0.19198384881019592\n",
            "loss:tensor(0.0894, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.2008, grad_fn=<MseLossBackward0>)\n",
            "0.20075350999832153\n",
            "loss:tensor(0.0963, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.1909, grad_fn=<MseLossBackward0>)\n",
            "0.19093291461467743\n",
            "loss:tensor(0.0755, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.1782, grad_fn=<MseLossBackward0>)\n",
            "0.17822104692459106\n",
            "loss:tensor(0.0399, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.1700, grad_fn=<MseLossBackward0>)\n",
            "0.17000730335712433\n",
            "loss:tensor(0.0408, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.1656, grad_fn=<MseLossBackward0>)\n",
            "0.16564126312732697\n",
            "loss:tensor(0.1445, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.1468, grad_fn=<MseLossBackward0>)\n",
            "0.14681454002857208\n",
            "loss:tensor(0.0679, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.1282, grad_fn=<MseLossBackward0>)\n",
            "0.12817955017089844\n",
            "loss:tensor(0.0700, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.1145, grad_fn=<MseLossBackward0>)\n",
            "0.11447369307279587\n",
            "loss:tensor(0.0967, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0964, grad_fn=<MseLossBackward0>)\n",
            "0.0963544100522995\n",
            "loss:tensor(0.1053, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0829, grad_fn=<MseLossBackward0>)\n",
            "0.082941934466362\n",
            "loss:tensor(0.0890, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0747, grad_fn=<MseLossBackward0>)\n",
            "0.07465726882219315\n",
            "loss:tensor(0.1097, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0688, grad_fn=<MseLossBackward0>)\n",
            "0.06880014389753342\n",
            "loss:tensor(0.0649, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0684, grad_fn=<MseLossBackward0>)\n",
            "0.06842304766178131\n",
            "loss:tensor(0.0723, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0805, grad_fn=<MseLossBackward0>)\n",
            "0.08047623187303543\n",
            "loss:tensor(0.0746, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0938, grad_fn=<MseLossBackward0>)\n",
            "0.093837209045887\n",
            "loss:tensor(0.0486, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.1134, grad_fn=<MseLossBackward0>)\n",
            "0.11340808868408203\n",
            "loss:tensor(0.1040, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.1395, grad_fn=<MseLossBackward0>)\n",
            "0.13950371742248535\n",
            "loss:tensor(0.0605, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.1728, grad_fn=<MseLossBackward0>)\n",
            "0.17276065051555634\n",
            "loss:tensor(0.1646, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.1779, grad_fn=<MseLossBackward0>)\n",
            "0.1778634637594223\n",
            "loss:tensor(0.1461, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.1809, grad_fn=<MseLossBackward0>)\n",
            "0.18094667792320251\n",
            "loss:tensor(0.0876, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.1619, grad_fn=<MseLossBackward0>)\n",
            "0.16189749538898468\n",
            "loss:tensor(0.2139, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.1356, grad_fn=<MseLossBackward0>)\n",
            "0.13564032316207886\n",
            "loss:tensor(0.0875, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.1102, grad_fn=<MseLossBackward0>)\n",
            "0.11016153544187546\n",
            "loss:tensor(0.1783, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0819, grad_fn=<MseLossBackward0>)\n",
            "0.08190178871154785\n",
            "loss:tensor(0.1814, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0494, grad_fn=<MseLossBackward0>)\n",
            "0.04937267675995827\n",
            "loss:tensor(0.0782, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0194, grad_fn=<MseLossBackward0>)\n",
            "0.01943693496286869\n",
            "loss:tensor(0.0874, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0405, grad_fn=<MseLossBackward0>)\n",
            "0.040522560477256775\n",
            "loss:tensor(0.1394, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.1061, grad_fn=<MseLossBackward0>)\n",
            "0.10607375204563141\n",
            "loss:tensor(0.0825, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.1451, grad_fn=<MseLossBackward0>)\n",
            "0.1451297253370285\n",
            "loss:tensor(0.1565, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.1718, grad_fn=<MseLossBackward0>)\n",
            "0.17179720103740692\n",
            "loss:tensor(0.1062, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.1949, grad_fn=<MseLossBackward0>)\n",
            "0.19494573771953583\n",
            "loss:tensor(0.0900, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.2117, grad_fn=<MseLossBackward0>)\n",
            "0.21171143651008606\n",
            "loss:tensor(0.0710, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.2327, grad_fn=<MseLossBackward0>)\n",
            "0.23272012174129486\n",
            "loss:tensor(0.0545, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.2522, grad_fn=<MseLossBackward0>)\n",
            "0.2522403597831726\n",
            "loss:tensor(0.0877, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.2592, grad_fn=<MseLossBackward0>)\n",
            "0.2591608166694641\n",
            "loss:tensor(0.1056, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.2378, grad_fn=<MseLossBackward0>)\n",
            "0.23778694868087769\n",
            "loss:tensor(0.1238, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.1941, grad_fn=<MseLossBackward0>)\n",
            "0.194132998585701\n",
            "loss:tensor(0.0584, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.1553, grad_fn=<MseLossBackward0>)\n",
            "0.15533266961574554\n",
            "loss:tensor(0.0863, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.1230, grad_fn=<MseLossBackward0>)\n",
            "0.12302009761333466\n",
            "loss:tensor(0.0561, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0979, grad_fn=<MseLossBackward0>)\n",
            "0.09788446128368378\n",
            "loss:tensor(0.0695, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0718, grad_fn=<MseLossBackward0>)\n",
            "0.07180602103471756\n",
            "loss:tensor(0.1870, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0473, grad_fn=<MseLossBackward0>)\n",
            "0.047305259853601456\n",
            "loss:tensor(0.0802, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0332, grad_fn=<MseLossBackward0>)\n",
            "0.03315569460391998\n",
            "loss:tensor(0.0613, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0284, grad_fn=<MseLossBackward0>)\n",
            "0.028369737789034843\n",
            "loss:tensor(0.1088, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0267, grad_fn=<MseLossBackward0>)\n",
            "0.02665134146809578\n",
            "loss:tensor(0.1097, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0240, grad_fn=<MseLossBackward0>)\n",
            "0.023955948650836945\n",
            "loss:tensor(0.0738, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0226, grad_fn=<MseLossBackward0>)\n",
            "0.022580726072192192\n",
            "loss:tensor(0.0603, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0233, grad_fn=<MseLossBackward0>)\n",
            "0.023281408473849297\n",
            "loss:tensor(0.1122, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0296, grad_fn=<MseLossBackward0>)\n",
            "0.029639730229973793\n",
            "loss:tensor(0.0943, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0377, grad_fn=<MseLossBackward0>)\n",
            "0.03766928240656853\n",
            "loss:tensor(0.0629, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0464, grad_fn=<MseLossBackward0>)\n",
            "0.046360839158296585\n",
            "loss:tensor(0.0721, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0452, grad_fn=<MseLossBackward0>)\n",
            "0.045172035694122314\n",
            "loss:tensor(0.0942, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0405, grad_fn=<MseLossBackward0>)\n",
            "0.040512386709451675\n",
            "loss:tensor(0.0787, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0323, grad_fn=<MseLossBackward0>)\n",
            "0.03227418288588524\n",
            "loss:tensor(0.0301, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0272, grad_fn=<MseLossBackward0>)\n",
            "0.027195679023861885\n",
            "loss:tensor(0.1111, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0240, grad_fn=<MseLossBackward0>)\n",
            "0.024047821760177612\n",
            "loss:tensor(0.0920, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0340, grad_fn=<MseLossBackward0>)\n",
            "0.033990636467933655\n",
            "loss:tensor(0.0646, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0472, grad_fn=<MseLossBackward0>)\n",
            "0.047238484025001526\n",
            "loss:tensor(0.1158, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0665, grad_fn=<MseLossBackward0>)\n",
            "0.06648383289575577\n",
            "loss:tensor(0.0535, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0908, grad_fn=<MseLossBackward0>)\n",
            "0.09083134680986404\n",
            "loss:tensor(0.0635, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0959, grad_fn=<MseLossBackward0>)\n",
            "0.0958714634180069\n",
            "loss:tensor(0.0719, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0863, grad_fn=<MseLossBackward0>)\n",
            "0.08627491444349289\n",
            "loss:tensor(0.0509, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0672, grad_fn=<MseLossBackward0>)\n",
            "0.06716419756412506\n",
            "loss:tensor(0.0806, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0415, grad_fn=<MseLossBackward0>)\n",
            "0.04154938831925392\n",
            "loss:tensor(0.1024, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0354, grad_fn=<MseLossBackward0>)\n",
            "0.03542282059788704\n",
            "loss:tensor(0.0906, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0257, grad_fn=<MseLossBackward0>)\n",
            "0.02570297010242939\n",
            "loss:tensor(0.0389, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0202, grad_fn=<MseLossBackward0>)\n",
            "0.020211195573210716\n",
            "loss:tensor(0.0655, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0162, grad_fn=<MseLossBackward0>)\n",
            "0.016222165897488594\n",
            "loss:tensor(0.0590, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0124, grad_fn=<MseLossBackward0>)\n",
            "0.012350163422524929\n",
            "loss:tensor(0.0763, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0157, grad_fn=<MseLossBackward0>)\n",
            "0.015660583972930908\n",
            "loss:tensor(0.0383, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0166, grad_fn=<MseLossBackward0>)\n",
            "0.016556983813643456\n",
            "loss:tensor(0.0383, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0216, grad_fn=<MseLossBackward0>)\n",
            "0.0215873084962368\n",
            "loss:tensor(0.0628, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0255, grad_fn=<MseLossBackward0>)\n",
            "0.02553570084273815\n",
            "loss:tensor(0.0515, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0338, grad_fn=<MseLossBackward0>)\n",
            "0.033806703984737396\n",
            "loss:tensor(0.0686, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0359, grad_fn=<MseLossBackward0>)\n",
            "0.035851623862981796\n",
            "loss:tensor(0.0611, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0354, grad_fn=<MseLossBackward0>)\n",
            "0.03544192761182785\n",
            "loss:tensor(0.0732, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0253, grad_fn=<MseLossBackward0>)\n",
            "0.02532312087714672\n",
            "loss:tensor(0.0670, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0217, grad_fn=<MseLossBackward0>)\n",
            "0.021684644743800163\n",
            "loss:tensor(0.0634, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0193, grad_fn=<MseLossBackward0>)\n",
            "0.019261576235294342\n",
            "loss:tensor(0.0746, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0175, grad_fn=<MseLossBackward0>)\n",
            "0.017544535920023918\n",
            "loss:tensor(0.0448, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0166, grad_fn=<MseLossBackward0>)\n",
            "0.0165999885648489\n",
            "loss:tensor(0.0704, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0160, grad_fn=<MseLossBackward0>)\n",
            "0.016047609969973564\n",
            "loss:tensor(0.1247, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0131, grad_fn=<MseLossBackward0>)\n",
            "0.013099013827741146\n",
            "loss:tensor(0.1121, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0086, grad_fn=<MseLossBackward0>)\n",
            "0.00859307311475277\n",
            "loss:tensor(0.1038, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0074, grad_fn=<MseLossBackward0>)\n",
            "0.0074400752782821655\n",
            "loss:tensor(0.0876, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0094, grad_fn=<MseLossBackward0>)\n",
            "0.009357418864965439\n",
            "loss:tensor(0.0633, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0121, grad_fn=<MseLossBackward0>)\n",
            "0.012119066901504993\n",
            "loss:tensor(0.0421, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0124, grad_fn=<MseLossBackward0>)\n",
            "0.012418748810887337\n",
            "loss:tensor(0.0580, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0112, grad_fn=<MseLossBackward0>)\n",
            "0.011166083626449108\n",
            "loss:tensor(0.0807, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0091, grad_fn=<MseLossBackward0>)\n",
            "0.009140366688370705\n",
            "loss:tensor(0.0915, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0059, grad_fn=<MseLossBackward0>)\n",
            "0.005857379175722599\n",
            "loss:tensor(0.0977, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0042, grad_fn=<MseLossBackward0>)\n",
            "0.0042173429392278194\n",
            "loss:tensor(0.0464, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0049, grad_fn=<MseLossBackward0>)\n",
            "0.004912772681564093\n",
            "loss:tensor(0.0422, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0127, grad_fn=<MseLossBackward0>)\n",
            "0.01266209315508604\n",
            "loss:tensor(0.1436, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0146, grad_fn=<MseLossBackward0>)\n",
            "0.014628694392740726\n",
            "loss:tensor(0.1152, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0165, grad_fn=<MseLossBackward0>)\n",
            "0.01650058664381504\n",
            "loss:tensor(0.0636, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0167, grad_fn=<MseLossBackward0>)\n",
            "0.016665522009134293\n",
            "loss:tensor(0.0852, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0180, grad_fn=<MseLossBackward0>)\n",
            "0.017954183742403984\n",
            "loss:tensor(0.0575, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0263, grad_fn=<MseLossBackward0>)\n",
            "0.02626979537308216\n",
            "loss:tensor(0.0803, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0289, grad_fn=<MseLossBackward0>)\n",
            "0.028915610164403915\n",
            "loss:tensor(0.0879, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0310, grad_fn=<MseLossBackward0>)\n",
            "0.031044771894812584\n",
            "loss:tensor(0.0585, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0086, grad_fn=<MseLossBackward0>)\n",
            "0.008596462197601795\n",
            "loss:tensor(0.0760, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
            "0.0012114470591768622\n",
            "loss:tensor(0.0842, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
            "0.0009875815594568849\n",
            "loss:tensor(0.0877, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0046, grad_fn=<MseLossBackward0>)\n",
            "0.004589340649545193\n",
            "loss:tensor(0.1001, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0066, grad_fn=<MseLossBackward0>)\n",
            "0.006625440903007984\n",
            "loss:tensor(0.1069, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0096, grad_fn=<MseLossBackward0>)\n",
            "0.009648696519434452\n",
            "loss:tensor(0.0996, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0124, grad_fn=<MseLossBackward0>)\n",
            "0.012405665591359138\n",
            "loss:tensor(0.0520, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0161, grad_fn=<MseLossBackward0>)\n",
            "0.016086531803011894\n",
            "loss:tensor(0.0853, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0146, grad_fn=<MseLossBackward0>)\n",
            "0.014613116160035133\n",
            "loss:tensor(0.0894, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0131, grad_fn=<MseLossBackward0>)\n",
            "0.013074143789708614\n",
            "loss:tensor(0.0505, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0091, grad_fn=<MseLossBackward0>)\n",
            "0.009079884737730026\n",
            "loss:tensor(0.1330, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0095, grad_fn=<MseLossBackward0>)\n",
            "0.00949143897742033\n",
            "loss:tensor(0.0884, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0107, grad_fn=<MseLossBackward0>)\n",
            "0.01065896824002266\n",
            "loss:tensor(0.1046, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0146, grad_fn=<MseLossBackward0>)\n",
            "0.014578273519873619\n",
            "loss:tensor(0.0646, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0192, grad_fn=<MseLossBackward0>)\n",
            "0.01920613832771778\n",
            "loss:tensor(0.0883, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0107, grad_fn=<MseLossBackward0>)\n",
            "0.01068952027708292\n",
            "loss:tensor(0.0772, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0047, grad_fn=<MseLossBackward0>)\n",
            "0.004729670472443104\n",
            "loss:tensor(0.0822, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0026, grad_fn=<MseLossBackward0>)\n",
            "0.0026471891906112432\n",
            "loss:tensor(0.0556, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0061, grad_fn=<MseLossBackward0>)\n",
            "0.006062599364668131\n",
            "loss:tensor(0.1053, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0233, grad_fn=<MseLossBackward0>)\n",
            "0.023349016904830933\n",
            "loss:tensor(0.1015, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0539, grad_fn=<MseLossBackward0>)\n",
            "0.05391833931207657\n",
            "loss:tensor(0.0574, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0928, grad_fn=<MseLossBackward0>)\n",
            "0.09280967712402344\n",
            "loss:tensor(0.0563, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.1335, grad_fn=<MseLossBackward0>)\n",
            "0.13348180055618286\n",
            "loss:tensor(0.0490, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.1588, grad_fn=<MseLossBackward0>)\n",
            "0.15875929594039917\n",
            "loss:tensor(0.1054, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.1612, grad_fn=<MseLossBackward0>)\n",
            "0.1612335741519928\n",
            "loss:tensor(0.1425, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.1341, grad_fn=<MseLossBackward0>)\n",
            "0.1340700089931488\n",
            "loss:tensor(0.0562, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.1073, grad_fn=<MseLossBackward0>)\n",
            "0.10726885497570038\n",
            "loss:tensor(0.0674, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0847, grad_fn=<MseLossBackward0>)\n",
            "0.08468422293663025\n",
            "loss:tensor(0.0980, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0615, grad_fn=<MseLossBackward0>)\n",
            "0.061501480638980865\n",
            "loss:tensor(0.1125, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0465, grad_fn=<MseLossBackward0>)\n",
            "0.046543918550014496\n",
            "loss:tensor(0.0726, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0267, grad_fn=<MseLossBackward0>)\n",
            "0.026735123246908188\n",
            "loss:tensor(0.0646, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0161, grad_fn=<MseLossBackward0>)\n",
            "0.01606724224984646\n",
            "loss:tensor(0.0627, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0126, grad_fn=<MseLossBackward0>)\n",
            "0.012576180510222912\n",
            "loss:tensor(0.0795, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0109, grad_fn=<MseLossBackward0>)\n",
            "0.010910780169069767\n",
            "loss:tensor(0.1016, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0074, grad_fn=<MseLossBackward0>)\n",
            "0.007437433581799269\n",
            "loss:tensor(0.0663, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0049, grad_fn=<MseLossBackward0>)\n",
            "0.004850925412029028\n",
            "loss:tensor(0.0779, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
            "0.0016348769422620535\n",
            "loss:tensor(0.0642, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
            "0.0012576557928696275\n",
            "loss:tensor(0.0600, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
            "0.001208011875860393\n",
            "loss:tensor(0.0522, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
            "0.0013543287059292197\n",
            "loss:tensor(0.0469, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0022, grad_fn=<MseLossBackward0>)\n",
            "0.0021883647423237562\n",
            "loss:tensor(0.0974, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0023, grad_fn=<MseLossBackward0>)\n",
            "0.002311944728717208\n",
            "loss:tensor(0.1037, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
            "0.001476246165111661\n",
            "loss:tensor(0.1038, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0032, grad_fn=<MseLossBackward0>)\n",
            "0.003162345150485635\n",
            "loss:tensor(0.0877, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0070, grad_fn=<MseLossBackward0>)\n",
            "0.006958687212318182\n",
            "loss:tensor(0.0908, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0123, grad_fn=<MseLossBackward0>)\n",
            "0.012273686937987804\n",
            "loss:tensor(0.0367, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0170, grad_fn=<MseLossBackward0>)\n",
            "0.017023544758558273\n",
            "loss:tensor(0.0455, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0181, grad_fn=<MseLossBackward0>)\n",
            "0.01807771995663643\n",
            "loss:tensor(0.0836, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0140, grad_fn=<MseLossBackward0>)\n",
            "0.013982151634991169\n",
            "loss:tensor(0.0905, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0108, grad_fn=<MseLossBackward0>)\n",
            "0.010811989195644855\n",
            "loss:tensor(0.0521, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0105, grad_fn=<MseLossBackward0>)\n",
            "0.01051334198564291\n",
            "loss:tensor(0.0695, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0135, grad_fn=<MseLossBackward0>)\n",
            "0.013487715274095535\n",
            "loss:tensor(0.1090, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0177, grad_fn=<MseLossBackward0>)\n",
            "0.01773124188184738\n",
            "loss:tensor(0.1266, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0165, grad_fn=<MseLossBackward0>)\n",
            "0.016470415517687798\n",
            "loss:tensor(0.0815, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0170, grad_fn=<MseLossBackward0>)\n",
            "0.017004376277327538\n",
            "loss:tensor(0.0485, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0163, grad_fn=<MseLossBackward0>)\n",
            "0.01625719480216503\n",
            "loss:tensor(0.0648, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0162, grad_fn=<MseLossBackward0>)\n",
            "0.0161922387778759\n",
            "loss:tensor(0.0517, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0195, grad_fn=<MseLossBackward0>)\n",
            "0.019532708451151848\n",
            "loss:tensor(0.0628, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0263, grad_fn=<MseLossBackward0>)\n",
            "0.02626127004623413\n",
            "loss:tensor(0.1128, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0362, grad_fn=<MseLossBackward0>)\n",
            "0.036186762154102325\n",
            "loss:tensor(0.0915, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0441, grad_fn=<MseLossBackward0>)\n",
            "0.04406983405351639\n",
            "loss:tensor(0.0882, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0481, grad_fn=<MseLossBackward0>)\n",
            "0.04811469465494156\n",
            "loss:tensor(0.0650, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0426, grad_fn=<MseLossBackward0>)\n",
            "0.0425548292696476\n",
            "loss:tensor(0.0576, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0348, grad_fn=<MseLossBackward0>)\n",
            "0.03478075563907623\n",
            "loss:tensor(0.0667, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0281, grad_fn=<MseLossBackward0>)\n",
            "0.028081346303224564\n",
            "loss:tensor(0.0425, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0226, grad_fn=<MseLossBackward0>)\n",
            "0.02264648862183094\n",
            "loss:tensor(0.0766, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0179, grad_fn=<MseLossBackward0>)\n",
            "0.01786685176193714\n",
            "loss:tensor(0.0485, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0206, grad_fn=<MseLossBackward0>)\n",
            "0.02062252350151539\n",
            "loss:tensor(0.0706, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0218, grad_fn=<MseLossBackward0>)\n",
            "0.021811317652463913\n",
            "loss:tensor(0.0957, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0210, grad_fn=<MseLossBackward0>)\n",
            "0.020968269556760788\n",
            "loss:tensor(0.0477, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0190, grad_fn=<MseLossBackward0>)\n",
            "0.019037770107388496\n",
            "loss:tensor(0.1215, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0188, grad_fn=<MseLossBackward0>)\n",
            "0.018763044849038124\n",
            "loss:tensor(0.0455, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0220, grad_fn=<MseLossBackward0>)\n",
            "0.022017771378159523\n",
            "loss:tensor(0.0753, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0263, grad_fn=<MseLossBackward0>)\n",
            "0.026281237602233887\n",
            "loss:tensor(0.0599, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0331, grad_fn=<MseLossBackward0>)\n",
            "0.03311965614557266\n",
            "loss:tensor(0.0795, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0432, grad_fn=<MseLossBackward0>)\n",
            "0.04321771115064621\n",
            "loss:tensor(0.0555, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0525, grad_fn=<MseLossBackward0>)\n",
            "0.052453458309173584\n",
            "loss:tensor(0.0528, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0591, grad_fn=<MseLossBackward0>)\n",
            "0.059056881815195084\n",
            "loss:tensor(0.0883, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0547, grad_fn=<MseLossBackward0>)\n",
            "0.05474111810326576\n",
            "loss:tensor(0.0547, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0506, grad_fn=<MseLossBackward0>)\n",
            "0.050587669014930725\n",
            "loss:tensor(0.0935, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0359, grad_fn=<MseLossBackward0>)\n",
            "0.03586956113576889\n",
            "loss:tensor(0.0623, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0214, grad_fn=<MseLossBackward0>)\n",
            "0.021428871899843216\n",
            "loss:tensor(0.0684, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0125, grad_fn=<MseLossBackward0>)\n",
            "0.012510617263615131\n",
            "loss:tensor(0.0999, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0071, grad_fn=<MseLossBackward0>)\n",
            "0.007141917012631893\n",
            "loss:tensor(0.0556, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0061, grad_fn=<MseLossBackward0>)\n",
            "0.006125963758677244\n",
            "loss:tensor(0.0390, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0067, grad_fn=<MseLossBackward0>)\n",
            "0.006690171547234058\n",
            "loss:tensor(0.0733, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0075, grad_fn=<MseLossBackward0>)\n",
            "0.007455866318196058\n",
            "loss:tensor(0.0620, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0060, grad_fn=<MseLossBackward0>)\n",
            "0.005963551811873913\n",
            "loss:tensor(0.0392, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0040, grad_fn=<MseLossBackward0>)\n",
            "0.004004230257123709\n",
            "loss:tensor(0.0340, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0043, grad_fn=<MseLossBackward0>)\n",
            "0.004330137278884649\n",
            "loss:tensor(0.0619, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0055, grad_fn=<MseLossBackward0>)\n",
            "0.005521761253476143\n",
            "loss:tensor(0.0433, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0088, grad_fn=<MseLossBackward0>)\n",
            "0.008780528791248798\n",
            "loss:tensor(0.0753, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0138, grad_fn=<MseLossBackward0>)\n",
            "0.013824131339788437\n",
            "loss:tensor(0.0456, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0193, grad_fn=<MseLossBackward0>)\n",
            "0.019273467361927032\n",
            "loss:tensor(0.1121, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0183, grad_fn=<MseLossBackward0>)\n",
            "0.018318256363272667\n",
            "loss:tensor(0.0622, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0137, grad_fn=<MseLossBackward0>)\n",
            "0.013654726557433605\n",
            "loss:tensor(0.0555, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0095, grad_fn=<MseLossBackward0>)\n",
            "0.009475832805037498\n",
            "loss:tensor(0.0524, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0064, grad_fn=<MseLossBackward0>)\n",
            "0.006441519595682621\n",
            "loss:tensor(0.0546, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0062, grad_fn=<MseLossBackward0>)\n",
            "0.00617362093180418\n",
            "loss:tensor(0.0273, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0063, grad_fn=<MseLossBackward0>)\n",
            "0.006251311395317316\n",
            "loss:tensor(0.0464, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0079, grad_fn=<MseLossBackward0>)\n",
            "0.007860582321882248\n",
            "loss:tensor(0.0747, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0053, grad_fn=<MseLossBackward0>)\n",
            "0.005263136234134436\n",
            "loss:tensor(0.0974, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0034, grad_fn=<MseLossBackward0>)\n",
            "0.003420670283958316\n",
            "loss:tensor(0.0415, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0023, grad_fn=<MseLossBackward0>)\n",
            "0.0022790825460106134\n",
            "loss:tensor(0.0345, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
            "0.0017731616972014308\n",
            "loss:tensor(0.0526, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0025, grad_fn=<MseLossBackward0>)\n",
            "0.0024519257713109255\n",
            "loss:tensor(0.0793, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0044, grad_fn=<MseLossBackward0>)\n",
            "0.0044205705635249615\n",
            "loss:tensor(0.0689, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0087, grad_fn=<MseLossBackward0>)\n",
            "0.008650471456348896\n",
            "loss:tensor(0.1049, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0129, grad_fn=<MseLossBackward0>)\n",
            "0.012866807170212269\n",
            "loss:tensor(0.0601, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0163, grad_fn=<MseLossBackward0>)\n",
            "0.01634499803185463\n",
            "loss:tensor(0.0698, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0140, grad_fn=<MseLossBackward0>)\n",
            "0.014027406461536884\n",
            "loss:tensor(0.0552, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0100, grad_fn=<MseLossBackward0>)\n",
            "0.009982508607208729\n",
            "loss:tensor(0.0573, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0068, grad_fn=<MseLossBackward0>)\n",
            "0.006837374530732632\n",
            "loss:tensor(0.0800, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0040, grad_fn=<MseLossBackward0>)\n",
            "0.003981459885835648\n",
            "loss:tensor(0.0510, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0051, grad_fn=<MseLossBackward0>)\n",
            "0.005055433139204979\n",
            "loss:tensor(0.0560, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0077, grad_fn=<MseLossBackward0>)\n",
            "0.007738002575933933\n",
            "loss:tensor(0.0934, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0104, grad_fn=<MseLossBackward0>)\n",
            "0.010377463884651661\n",
            "loss:tensor(0.0685, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0154, grad_fn=<MseLossBackward0>)\n",
            "0.01543167419731617\n",
            "loss:tensor(0.0796, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0199, grad_fn=<MseLossBackward0>)\n",
            "0.019937220960855484\n",
            "loss:tensor(0.1299, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0207, grad_fn=<MseLossBackward0>)\n",
            "0.020662132650613785\n",
            "loss:tensor(0.0545, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0239, grad_fn=<MseLossBackward0>)\n",
            "0.023889046162366867\n",
            "loss:tensor(0.0906, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0306, grad_fn=<MseLossBackward0>)\n",
            "0.03059942089021206\n",
            "loss:tensor(0.0433, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0341, grad_fn=<MseLossBackward0>)\n",
            "0.034112777560949326\n",
            "loss:tensor(0.0494, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0304, grad_fn=<MseLossBackward0>)\n",
            "0.030435185879468918\n",
            "loss:tensor(0.0697, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0263, grad_fn=<MseLossBackward0>)\n",
            "0.026334067806601524\n",
            "loss:tensor(0.0613, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0197, grad_fn=<MseLossBackward0>)\n",
            "0.019722849130630493\n",
            "loss:tensor(0.0524, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0123, grad_fn=<MseLossBackward0>)\n",
            "0.012250932864844799\n",
            "loss:tensor(0.0668, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0075, grad_fn=<MseLossBackward0>)\n",
            "0.007508588954806328\n",
            "loss:tensor(0.0308, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0090, grad_fn=<MseLossBackward0>)\n",
            "0.009008675813674927\n",
            "loss:tensor(0.0491, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0083, grad_fn=<MseLossBackward0>)\n",
            "0.008273356594145298\n",
            "loss:tensor(0.0349, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0059, grad_fn=<MseLossBackward0>)\n",
            "0.005943139083683491\n",
            "loss:tensor(0.0652, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0030, grad_fn=<MseLossBackward0>)\n",
            "0.0029920232482254505\n",
            "loss:tensor(0.0437, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
            "0.0008499074610881507\n",
            "loss:tensor(0.0920, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
            "0.0005879166419617832\n",
            "loss:tensor(0.0787, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0020, grad_fn=<MseLossBackward0>)\n",
            "0.002011572476476431\n",
            "loss:tensor(0.0923, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0030, grad_fn=<MseLossBackward0>)\n",
            "0.002995942020788789\n",
            "loss:tensor(0.0777, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0035, grad_fn=<MseLossBackward0>)\n",
            "0.0035239015705883503\n",
            "loss:tensor(0.0604, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0046, grad_fn=<MseLossBackward0>)\n",
            "0.004597881343215704\n",
            "loss:tensor(0.0441, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0069, grad_fn=<MseLossBackward0>)\n",
            "0.006861524656414986\n",
            "loss:tensor(0.0754, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0101, grad_fn=<MseLossBackward0>)\n",
            "0.010051843710243702\n",
            "loss:tensor(0.0408, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0139, grad_fn=<MseLossBackward0>)\n",
            "0.013936078175902367\n",
            "loss:tensor(0.0581, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0175, grad_fn=<MseLossBackward0>)\n",
            "0.017475122585892677\n",
            "loss:tensor(0.0907, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0185, grad_fn=<MseLossBackward0>)\n",
            "0.018529372289776802\n",
            "loss:tensor(0.1005, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0188, grad_fn=<MseLossBackward0>)\n",
            "0.018837671726942062\n",
            "loss:tensor(0.0344, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0185, grad_fn=<MseLossBackward0>)\n",
            "0.018489427864551544\n",
            "loss:tensor(0.1483, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0165, grad_fn=<MseLossBackward0>)\n",
            "0.016548864543437958\n",
            "loss:tensor(0.0487, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0150, grad_fn=<MseLossBackward0>)\n",
            "0.014981075190007687\n",
            "loss:tensor(0.0407, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0155, grad_fn=<MseLossBackward0>)\n",
            "0.015492859296500683\n",
            "loss:tensor(0.0646, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0180, grad_fn=<MseLossBackward0>)\n",
            "0.01800035871565342\n",
            "loss:tensor(0.0395, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0205, grad_fn=<MseLossBackward0>)\n",
            "0.02047506719827652\n",
            "loss:tensor(0.0941, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0259, grad_fn=<MseLossBackward0>)\n",
            "0.02588430605828762\n",
            "loss:tensor(0.1371, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0289, grad_fn=<MseLossBackward0>)\n",
            "0.028905298560857773\n",
            "loss:tensor(0.0387, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0315, grad_fn=<MseLossBackward0>)\n",
            "0.03149480000138283\n",
            "loss:tensor(0.0965, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0263, grad_fn=<MseLossBackward0>)\n",
            "0.02633971907198429\n",
            "loss:tensor(0.1235, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0151, grad_fn=<MseLossBackward0>)\n",
            "0.015093647874891758\n",
            "loss:tensor(0.1166, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0060, grad_fn=<MseLossBackward0>)\n",
            "0.005955255590379238\n",
            "loss:tensor(0.0869, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0068, grad_fn=<MseLossBackward0>)\n",
            "0.00677934056147933\n",
            "loss:tensor(0.0697, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0168, grad_fn=<MseLossBackward0>)\n",
            "0.016806675121188164\n",
            "loss:tensor(0.0956, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0228, grad_fn=<MseLossBackward0>)\n",
            "0.022752247750759125\n",
            "loss:tensor(0.1437, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0218, grad_fn=<MseLossBackward0>)\n",
            "0.021782802417874336\n",
            "loss:tensor(0.1558, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0123, grad_fn=<MseLossBackward0>)\n",
            "0.012256491929292679\n",
            "loss:tensor(0.0606, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0047, grad_fn=<MseLossBackward0>)\n",
            "0.004666638560593128\n",
            "loss:tensor(0.1120, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
            "0.0016379153821617365\n",
            "loss:tensor(0.0762, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0051, grad_fn=<MseLossBackward0>)\n",
            "0.0051197181455791\n",
            "loss:tensor(0.0661, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0134, grad_fn=<MseLossBackward0>)\n",
            "0.013427087105810642\n",
            "loss:tensor(0.1497, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0245, grad_fn=<MseLossBackward0>)\n",
            "0.024525349959731102\n",
            "loss:tensor(0.1409, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0307, grad_fn=<MseLossBackward0>)\n",
            "0.03068065084517002\n",
            "loss:tensor(0.0873, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0290, grad_fn=<MseLossBackward0>)\n",
            "0.02899208478629589\n",
            "loss:tensor(0.1165, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0218, grad_fn=<MseLossBackward0>)\n",
            "0.0217555221170187\n",
            "loss:tensor(0.0891, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0145, grad_fn=<MseLossBackward0>)\n",
            "0.01454151514917612\n",
            "loss:tensor(0.0723, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0088, grad_fn=<MseLossBackward0>)\n",
            "0.008838466368615627\n",
            "loss:tensor(0.0680, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0056, grad_fn=<MseLossBackward0>)\n",
            "0.005647838581353426\n",
            "loss:tensor(0.0709, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0052, grad_fn=<MseLossBackward0>)\n",
            "0.005198416765779257\n",
            "loss:tensor(0.0756, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0059, grad_fn=<MseLossBackward0>)\n",
            "0.005911290645599365\n",
            "loss:tensor(0.0609, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0070, grad_fn=<MseLossBackward0>)\n",
            "0.006965803913772106\n",
            "loss:tensor(0.0862, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0045, grad_fn=<MseLossBackward0>)\n",
            "0.00451391376554966\n",
            "loss:tensor(0.1233, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0027, grad_fn=<MseLossBackward0>)\n",
            "0.002745233941823244\n",
            "loss:tensor(0.0312, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0022, grad_fn=<MseLossBackward0>)\n",
            "0.0021628732793033123\n",
            "loss:tensor(0.0621, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0034, grad_fn=<MseLossBackward0>)\n",
            "0.003363457741215825\n",
            "loss:tensor(0.0619, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0050, grad_fn=<MseLossBackward0>)\n",
            "0.004995923023670912\n",
            "loss:tensor(0.0641, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0121, grad_fn=<MseLossBackward0>)\n",
            "0.012112853117287159\n",
            "loss:tensor(0.0727, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0178, grad_fn=<MseLossBackward0>)\n",
            "0.017790354788303375\n",
            "loss:tensor(0.0606, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0234, grad_fn=<MseLossBackward0>)\n",
            "0.02336343564093113\n",
            "loss:tensor(0.0510, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0284, grad_fn=<MseLossBackward0>)\n",
            "0.028439538553357124\n",
            "loss:tensor(0.0308, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0319, grad_fn=<MseLossBackward0>)\n",
            "0.03192875534296036\n",
            "loss:tensor(0.0495, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0335, grad_fn=<MseLossBackward0>)\n",
            "0.03347129747271538\n",
            "loss:tensor(0.0649, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0321, grad_fn=<MseLossBackward0>)\n",
            "0.032089296728372574\n",
            "loss:tensor(0.0512, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0273, grad_fn=<MseLossBackward0>)\n",
            "0.027277622371912003\n",
            "loss:tensor(0.1290, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0264, grad_fn=<MseLossBackward0>)\n",
            "0.02638816460967064\n",
            "loss:tensor(0.0437, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0252, grad_fn=<MseLossBackward0>)\n",
            "0.025181813165545464\n",
            "loss:tensor(0.0484, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0284, grad_fn=<MseLossBackward0>)\n",
            "0.028395315632224083\n",
            "loss:tensor(0.0672, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0293, grad_fn=<MseLossBackward0>)\n",
            "0.029256703332066536\n",
            "loss:tensor(0.0936, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0275, grad_fn=<MseLossBackward0>)\n",
            "0.027486853301525116\n",
            "loss:tensor(0.1204, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0251, grad_fn=<MseLossBackward0>)\n",
            "0.02505180425941944\n",
            "loss:tensor(0.0797, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0272, grad_fn=<MseLossBackward0>)\n",
            "0.027171611785888672\n",
            "loss:tensor(0.0961, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0300, grad_fn=<MseLossBackward0>)\n",
            "0.029983213171362877\n",
            "loss:tensor(0.1103, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0304, grad_fn=<MseLossBackward0>)\n",
            "0.03037884645164013\n",
            "loss:tensor(0.0653, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0275, grad_fn=<MseLossBackward0>)\n",
            "0.02746718004345894\n",
            "loss:tensor(0.0913, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0242, grad_fn=<MseLossBackward0>)\n",
            "0.024235419929027557\n",
            "loss:tensor(0.0998, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0180, grad_fn=<MseLossBackward0>)\n",
            "0.018001170828938484\n",
            "loss:tensor(0.0526, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0112, grad_fn=<MseLossBackward0>)\n",
            "0.011225387454032898\n",
            "loss:tensor(0.1398, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0151, grad_fn=<MseLossBackward0>)\n",
            "0.01505787018686533\n",
            "loss:tensor(0.0530, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0270, grad_fn=<MseLossBackward0>)\n",
            "0.02696734294295311\n",
            "loss:tensor(0.1202, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0426, grad_fn=<MseLossBackward0>)\n",
            "0.04263925552368164\n",
            "loss:tensor(0.0811, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0525, grad_fn=<MseLossBackward0>)\n",
            "0.0524631068110466\n",
            "loss:tensor(0.0723, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0566, grad_fn=<MseLossBackward0>)\n",
            "0.0565970353782177\n",
            "loss:tensor(0.0991, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0557, grad_fn=<MseLossBackward0>)\n",
            "0.05567745119333267\n",
            "loss:tensor(0.0839, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0545, grad_fn=<MseLossBackward0>)\n",
            "0.054451461881399155\n",
            "loss:tensor(0.0329, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0456, grad_fn=<MseLossBackward0>)\n",
            "0.04556693509221077\n",
            "loss:tensor(0.0614, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0295, grad_fn=<MseLossBackward0>)\n",
            "0.029498359188437462\n",
            "loss:tensor(0.0536, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0182, grad_fn=<MseLossBackward0>)\n",
            "0.01817307062447071\n",
            "loss:tensor(0.0587, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0140, grad_fn=<MseLossBackward0>)\n",
            "0.013978417962789536\n",
            "loss:tensor(0.0949, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0181, grad_fn=<MseLossBackward0>)\n",
            "0.018081553280353546\n",
            "loss:tensor(0.0851, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0202, grad_fn=<MseLossBackward0>)\n",
            "0.020212816074490547\n",
            "loss:tensor(0.1492, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0201, grad_fn=<MseLossBackward0>)\n",
            "0.020105767995119095\n",
            "loss:tensor(0.0925, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0224, grad_fn=<MseLossBackward0>)\n",
            "0.022365303710103035\n",
            "loss:tensor(0.1294, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0217, grad_fn=<MseLossBackward0>)\n",
            "0.021724339574575424\n",
            "loss:tensor(0.1123, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0232, grad_fn=<MseLossBackward0>)\n",
            "0.023172112181782722\n",
            "loss:tensor(0.1420, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0306, grad_fn=<MseLossBackward0>)\n",
            "0.030574003234505653\n",
            "loss:tensor(0.1069, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0342, grad_fn=<MseLossBackward0>)\n",
            "0.03420088440179825\n",
            "loss:tensor(0.0974, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0366, grad_fn=<MseLossBackward0>)\n",
            "0.036640968173742294\n",
            "loss:tensor(0.1067, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0320, grad_fn=<MseLossBackward0>)\n",
            "0.03196799010038376\n",
            "loss:tensor(0.1795, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0228, grad_fn=<MseLossBackward0>)\n",
            "0.022802108898758888\n",
            "loss:tensor(0.0655, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0158, grad_fn=<MseLossBackward0>)\n",
            "0.015810957178473473\n",
            "loss:tensor(0.0812, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0110, grad_fn=<MseLossBackward0>)\n",
            "0.010958850383758545\n",
            "loss:tensor(0.0730, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0058, grad_fn=<MseLossBackward0>)\n",
            "0.00582016259431839\n",
            "loss:tensor(0.0708, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0049, grad_fn=<MseLossBackward0>)\n",
            "0.004922283813357353\n",
            "loss:tensor(0.1265, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0037, grad_fn=<MseLossBackward0>)\n",
            "0.003688160562887788\n",
            "loss:tensor(0.0503, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0031, grad_fn=<MseLossBackward0>)\n",
            "0.003149400930851698\n",
            "loss:tensor(0.1018, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0034, grad_fn=<MseLossBackward0>)\n",
            "0.003440567757934332\n",
            "loss:tensor(0.0393, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0036, grad_fn=<MseLossBackward0>)\n",
            "0.0035824812948703766\n",
            "loss:tensor(0.0396, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0046, grad_fn=<MseLossBackward0>)\n",
            "0.004628445487469435\n",
            "loss:tensor(0.0494, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0071, grad_fn=<MseLossBackward0>)\n",
            "0.0071031758561730385\n",
            "loss:tensor(0.0568, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0074, grad_fn=<MseLossBackward0>)\n",
            "0.007360130548477173\n",
            "loss:tensor(0.0466, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0060, grad_fn=<MseLossBackward0>)\n",
            "0.006016379687935114\n",
            "loss:tensor(0.0372, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0062, grad_fn=<MseLossBackward0>)\n",
            "0.006230119150131941\n",
            "loss:tensor(0.0273, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0082, grad_fn=<MseLossBackward0>)\n",
            "0.008157179690897465\n",
            "loss:tensor(0.1042, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0093, grad_fn=<MseLossBackward0>)\n",
            "0.009306698106229305\n",
            "loss:tensor(0.0984, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0130, grad_fn=<MseLossBackward0>)\n",
            "0.012952574528753757\n",
            "loss:tensor(0.0956, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0127, grad_fn=<MseLossBackward0>)\n",
            "0.012651251628994942\n",
            "loss:tensor(0.0346, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0114, grad_fn=<MseLossBackward0>)\n",
            "0.011393990367650986\n",
            "loss:tensor(0.0607, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0083, grad_fn=<MseLossBackward0>)\n",
            "0.008280673995614052\n",
            "loss:tensor(0.0275, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0052, grad_fn=<MseLossBackward0>)\n",
            "0.005177624057978392\n",
            "loss:tensor(0.0552, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0032, grad_fn=<MseLossBackward0>)\n",
            "0.00318537768907845\n",
            "loss:tensor(0.0622, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0029, grad_fn=<MseLossBackward0>)\n",
            "0.002871172269806266\n",
            "loss:tensor(0.0494, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0029, grad_fn=<MseLossBackward0>)\n",
            "0.0029335180297493935\n",
            "loss:tensor(0.0320, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0032, grad_fn=<MseLossBackward0>)\n",
            "0.0032060558442026377\n",
            "loss:tensor(0.0577, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0041, grad_fn=<MseLossBackward0>)\n",
            "0.004116821102797985\n",
            "loss:tensor(0.0696, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0062, grad_fn=<MseLossBackward0>)\n",
            "0.006159528158605099\n",
            "loss:tensor(0.0449, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0075, grad_fn=<MseLossBackward0>)\n",
            "0.0074613350443542\n",
            "loss:tensor(0.0697, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0072, grad_fn=<MseLossBackward0>)\n",
            "0.007219407241791487\n",
            "loss:tensor(0.0712, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0060, grad_fn=<MseLossBackward0>)\n",
            "0.006047805305570364\n",
            "loss:tensor(0.0311, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0074, grad_fn=<MseLossBackward0>)\n",
            "0.007375896442681551\n",
            "loss:tensor(0.0273, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0105, grad_fn=<MseLossBackward0>)\n",
            "0.010493232868611813\n",
            "loss:tensor(0.1012, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0146, grad_fn=<MseLossBackward0>)\n",
            "0.014632190577685833\n",
            "loss:tensor(0.2280, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0235, grad_fn=<MseLossBackward0>)\n",
            "0.023450342938303947\n",
            "loss:tensor(0.1285, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0321, grad_fn=<MseLossBackward0>)\n",
            "0.03213636577129364\n",
            "loss:tensor(0.0672, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0337, grad_fn=<MseLossBackward0>)\n",
            "0.03368901461362839\n",
            "loss:tensor(0.0723, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0312, grad_fn=<MseLossBackward0>)\n",
            "0.031227076426148415\n",
            "loss:tensor(0.0391, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0295, grad_fn=<MseLossBackward0>)\n",
            "0.02945510484278202\n",
            "loss:tensor(0.0415, grad_fn=<MseLossBackward0>)\n",
            "tensor(0.0269, grad_fn=<MseLossBackward0>)\n",
            "0.026905197650194168\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.0269, grad_fn=<MseLossBackward0>)\n",
            "***********************\n",
            "Test acc with weight noise:tensor(0.0327, grad_fn=<MseLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install snntorch"
      ],
      "metadata": {
        "id": "LCCJGDrAk2lZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3da5610c-9ffb-4fb4-bb1c-ab6ed2efc0be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: snntorch in /usr/local/lib/python3.7/dist-packages (0.5.1)\n",
            "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from snntorch) (1.12.0+cu113)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from snntorch) (1.3.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from snntorch) (1.21.6)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from snntorch) (3.2.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.1.0->snntorch) (4.1.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->snntorch) (1.4.4)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->snntorch) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->snntorch) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->snntorch) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->snntorch) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->snntorch) (2022.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import snntorch as snn\n",
        "from snntorch import surrogate\n",
        "beta = 0.5  # neuron decay rate\n",
        "spike_grad = surrogate.fast_sigmoid()"
      ],
      "metadata": {
        "id": "gGmxqAhNHbEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, torch.nn as nn\n",
        "import snntorch as snn\n",
        "from snntorch import surrogate\n",
        "\n",
        "num_steps = 25 # number of time steps\n",
        "batch_size = 1\n",
        "beta = 0.5  # neuron decay rate\n",
        "spike_grad = surrogate.fast_sigmoid()\n",
        "\n",
        "net = nn.Sequential(\n",
        "      nn.Conv2d(1, 8, 5),\n",
        "      nn.MaxPool2d(2),\n",
        "      snn.Leaky(beta=beta, init_hidden=True, spike_grad=spike_grad),\n",
        "      nn.Conv2d(8, 16, 5),\n",
        "      nn.MaxPool2d(2),\n",
        "      snn.Leaky(beta=beta, init_hidden=True, spike_grad=spike_grad),\n",
        "      nn.Flatten(),\n",
        "      nn.Linear(16 * 4 * 4, 10),\n",
        "      snn.Leaky(beta=beta, init_hidden=True, spike_grad=spike_grad, output=True)\n",
        "      )\n",
        "\n",
        "# random input data\n",
        "data_in = torch.rand(num_steps, batch_size, 1, 28, 28)\n",
        "\n",
        "spike_recording = []\n",
        "\n",
        "for step in range(num_steps):\n",
        "    spike, state = net(data_in[step])\n",
        "    spike_recording.append(spike)"
      ],
      "metadata": {
        "id": "YlcokCg9GMIt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyparsing import actions\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torch import optim\n",
        "from random import randrange, uniform\n",
        "import math\n",
        "import pandas as pd\n",
        "import torch.utils as utils\n",
        "import operator\n",
        "import seaborn as sns\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import init\n",
        "from torch.nn import functional as F\n",
        "import math\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "\n",
        "class CTRNN(nn.Module):\n",
        "    \"\"\"Continuous-time RNN.\n",
        "\n",
        "    Args:\n",
        "        input_size: Number of input neurons\n",
        "        hidden_size: Number of hidden neurons\n",
        "\n",
        "    Inputs:\n",
        "        input: (seq_len, batch, input_size), network input\n",
        "        hidden: (batch, hidden_size), initial hidden activity\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, dt=None, **kwargs):\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.tau = 100\n",
        "        if dt is None:\n",
        "            alpha = 1\n",
        "        else:\n",
        "            alpha = dt / self.tau\n",
        "        self.alpha = alpha\n",
        "        self.oneminusalpha = 1 - alpha\n",
        "\n",
        "        self.input2h = nn.Linear(input_size, hidden_size)\n",
        "        self.h2h = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "    def init_hidden(self, input_shape):\n",
        "        batch_size = input_shape[1]\n",
        "        return torch.zeros(batch_size, self.hidden_size)\n",
        "\n",
        "    def recurrence(self, input, hidden):\n",
        "        \"\"\"Recurrence helper.\"\"\"\n",
        "        pre_activation = self.input2h(input) + self.h2h(hidden)\n",
        "        h_new = hidden * self.oneminusalpha + pre_activation * self.alpha\n",
        "        return torch.max(torch.tensor(0.0), torch.tanh(h_new))\n",
        "\n",
        "    def forward(self, input, hidden=None):\n",
        "        \"\"\"Propogate input through the network.\"\"\"\n",
        "        if hidden is None:\n",
        "            hidden = self.init_hidden(input.shape).to(input.device)\n",
        "\n",
        "        output = []\n",
        "        steps = range(input.size(0))\n",
        "        for i in steps:\n",
        "            hidden = self.recurrence(input[i], hidden)\n",
        "            output.append(hidden)\n",
        "\n",
        "        output = torch.stack(output, dim=0)\n",
        "        return output, hidden\n",
        "\n",
        "\n",
        "'''\n",
        "\n",
        "class CTRNN(nn.Module):\n",
        "    \"\"\"Continuous-time RNN.\n",
        "\n",
        "    Args:\n",
        "        input_size: Number of input neurons\n",
        "        hidden_size: Number of hidden neurons\n",
        "\n",
        "    Inputs:\n",
        "        input: (seq_len, batch, input_size), network input\n",
        "        hidden: (batch, hidden_size), initial hidden activity\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, dt=None, **kwargs):\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.tau = 100\n",
        "        if dt is None:\n",
        "            alpha = 1\n",
        "        else:\n",
        "            alpha = dt / self.tau\n",
        "        self.alpha = alpha\n",
        "        self.oneminusalpha = 1 - alpha\n",
        "\n",
        "        self.input2h = nn.Linear(input_size, hidden_size)\n",
        "        self.h2h = nn.Linear(hidden_size, hidden_size)\n",
        "        self.lif1 = snn.Leaky(beta=0.9)\n",
        "\n",
        "    def init_hidden(self, input_shape):\n",
        "        batch_size = input_shape[1]\n",
        "        return torch.zeros(batch_size, self.hidden_size)\n",
        "\n",
        "    def recurrence(self, input, hidden):\n",
        "        \"\"\"Recurrence helper.\"\"\"\n",
        "        pre_activation = self.input2h(input) + self.h2h(hidden)\n",
        "        h_new = torch.tanh(hidden * self.oneminusalpha +\n",
        "                           pre_activation * self.alpha)\n",
        "        return torch.max(torch.tensor(0.0), h_new)\n",
        "\n",
        "    def forward(self, input, hidden=None):\n",
        "\n",
        "        mem2 = self.lif1.init_leaky()\n",
        "\n",
        "        # Record the final layer\n",
        "        spk2_rec = []\n",
        "        mem2_rec = []\n",
        "\n",
        "        \"\"\"Propogate input through the network.\"\"\"\n",
        "        if hidden is None:\n",
        "            hidden = self.init_hidden(input.shape).to(input.device)\n",
        "\n",
        "        output = []\n",
        "        steps = range(input.size(0))\n",
        "        for i in steps:\n",
        "            random_noise = np.random.normal(0,1)\n",
        "            pre_activation = self.input2h(input[i])\n",
        "            spk2, mem2 = self.lif1(pre_activation, mem2)\n",
        "            spk2_rec.append(spk2)\n",
        "            mem2_rec.append(mem2+random_noise)\n",
        "        return torch.stack(spk2_rec, dim=0), torch.stack(mem2_rec, dim=0)\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "class RNNNet(nn.Module):\n",
        "    \"\"\"Recurrent network model.\n",
        "\n",
        "    Args:\n",
        "        input_size: int, input size\n",
        "        hidden_size: int, hidden size\n",
        "        output_size: int, output size\n",
        "        rnn: str, type of RNN, lstm, rnn, ctrnn, or eirnn\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, hidden_size, output_size, **kwargs):\n",
        "        super().__init__()\n",
        "\n",
        "        # Continuous time RNN\n",
        "        self.rnn = CTRNN(input_size, hidden_size, **kwargs)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        rnn_activity, acts = self.rnn(x)\n",
        "        out = self.fc(rnn_activity)\n",
        "        return out, rnn_activity, acts\n",
        "\n",
        "class ModelAnalyzer:\n",
        "    def __init__(self, model, loader):\n",
        "        self.model = model\n",
        "        self.loader = loader\n",
        "\n",
        "    def plot_pred_vs_ground_truth(self):\n",
        "\n",
        "        pred_array = []\n",
        "        gt_array = []\n",
        "        time_array = []\n",
        "\n",
        "        return_arr = []\n",
        "\n",
        "        model_input, labels = self.loader.generate_training_data()\n",
        "        outputs, rnn_output = self.model(model_input)\n",
        "\n",
        "\n",
        "        for i in range(60):\n",
        "            pred_array.append(outputs[i, 0, 0].item())\n",
        "            gt_array.append(labels[i, 0, 0].item())\n",
        "            time_array.append(i)\n",
        "\n",
        "        plt.plot(time_array, pred_array, label='pred')\n",
        "        plt.plot(time_array, gt_array, label='gt')\n",
        "        plt.show()\n",
        "\n",
        "        plt.plot(time_array, pred_array, label='pred')\n",
        "        plt.plot(time_array, gt_array, label='gt')\n",
        "        ax = plt.gca()\n",
        "        ax.set_ylim([-1.0, 1.0])\n",
        "        plt.show()\n",
        "\n",
        "    def plot_tuning_curve_hd(self, inputs, activations):\n",
        "        fig, a = plt.subplots(10, 10)\n",
        "\n",
        "        x_idx = 0\n",
        "        y_idx = 0\n",
        "\n",
        "        return_arr = []\n",
        "        max_array= []\n",
        "\n",
        "        start = True\n",
        "\n",
        "        for i in range(100):\n",
        "\n",
        "            hidden_activation = []\n",
        "            input_value = []\n",
        "\n",
        "\n",
        "            for k in range(2000):\n",
        "                for j in range(60):\n",
        "                    hidden_activation.append(abs(activations[k, j, 0, i]))\n",
        "                    input_value.append(inputs[k, j, 0, 2])\n",
        "\n",
        "            dataset = pd.DataFrame()\n",
        "            dataset['HD'] = input_value\n",
        "            dataset['Activation'] = hidden_activation\n",
        "            dataset['bins'] = pd.cut(dataset['HD'], 20).astype(str)\n",
        "            dataset['bins_mean'] = dataset.groupby('bins')['Activation'].transform('mean')\n",
        "            dataset.drop_duplicates('bins', inplace=True)\n",
        "            dataset.reset_index(inplace=True)\n",
        "            col = 'bins'\n",
        "            df = dataset.join(\n",
        "                dataset[col]\n",
        "                    .str.replace(\"]\", \"\", regex=False)\n",
        "                    .str.replace(\"(\", \"\", regex=False)\n",
        "                    .str.replace(\" \", \"\", regex=False)\n",
        "                    .str.replace(\",\", \"-\", regex=False)\n",
        "                    .str.replace(\".0\", \"\", regex=False)\n",
        "                    .str.replace(\".1\", \"\", regex=False)\n",
        "                    .str.replace(\".2\", \"\", regex=False)\n",
        "                    .str.replace(\".3\", \"\", regex=False)\n",
        "                    .str.replace(\".4\", \"\", regex=False)\n",
        "                    .str.replace(\".5\", \"\", regex=False)\n",
        "                    .str.replace(\".6\", \"\", regex=False)\n",
        "                    .str.replace(\".7\", \"\", regex=False)\n",
        "                    .str.replace(\".8\", \"\", regex=False)\n",
        "                    .str.replace(\".9\", \"\", regex=False)\n",
        "                    .str.extract(pat=r\"^[$]*(\\d+)[-\\s$]*(\\d+)$\")\n",
        "                    .astype(\"float\")\n",
        "                    .rename({0: f\"{col}_lower\", 1: f\"{col}_upper\"}, axis=\"columns\")\n",
        "            )\n",
        "            dff = df.dropna()\n",
        "            dff = dff.sort_values(by=['bins_lower'])\n",
        "\n",
        "            return_arr.append(df['bins_mean'].to_numpy())\n",
        "            max_array.append([dff['HD'].to_numpy(), dff['bins_mean'].to_numpy()])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            a[x_idx][y_idx].plot(dff['bins_lower'].to_numpy(), dff['bins_mean'].to_numpy())\n",
        "            #a[x_idx][y_idx].set_xlim(0, 360)\n",
        "            #a[x_idx][y_idx].set_ylim(0, 1.0)\n",
        "            x_idx += 1\n",
        "            if x_idx == 10 and start == False:\n",
        "                y_idx += 1\n",
        "                x_idx = 0\n",
        "            start = False\n",
        "\n",
        "        plt.setp(plt.gcf().get_axes(), xticks=[], yticks=[])\n",
        "        plt.show()\n",
        "        return np.array(return_arr), np.array(max_array)\n",
        "\n",
        "\n",
        "    def plot_tuning_curve_angular_velocity(self, inputs, activations):\n",
        "            fig, a = plt.subplots(10, 10)\n",
        "\n",
        "            x_idx = 0\n",
        "            y_idx = 0\n",
        "            return_arr = []\n",
        "            max_array = []\n",
        "\n",
        "            start = True\n",
        "\n",
        "            for i in range(100):\n",
        "\n",
        "                hidden_activation = []\n",
        "                input_value = []\n",
        "\n",
        "                for k in range(100):\n",
        "                    for j in range(60):\n",
        "                        hidden_activation.append(activations[k, j, 0, i])\n",
        "                        input_value.append(inputs[k, j, 0, 3])\n",
        "\n",
        "                dataset = pd.DataFrame()\n",
        "                dataset['HD'] = np.round(input_value)\n",
        "                dataset['Activation'] = hidden_activation\n",
        "                dataset['bins'] = pd.cut(dataset['HD'], 20).astype(str)\n",
        "                dataset['bins_mean'] = dataset.groupby('bins')['Activation'].transform('mean')\n",
        "                dataset.drop_duplicates('bins', inplace=True)\n",
        "                dataset.reset_index(inplace=True)\n",
        "                col = 'bins'\n",
        "                df = dataset.join(\n",
        "                    dataset[col]\n",
        "                        .str.replace(\"]\", \"\", regex=False)\n",
        "                        .str.replace(\"(\", \"\", regex=False)\n",
        "                        .str.replace(\" \", \"\", regex=False)\n",
        "                        .str.replace(\",\", \"-\", regex=False)\n",
        "                        .str.replace(\".0\", \"\", regex=False)\n",
        "                        .str.replace(\".1\", \"\", regex=False)\n",
        "                        .str.replace(\".2\", \"\", regex=False)\n",
        "                        .str.replace(\".3\", \"\", regex=False)\n",
        "                        .str.replace(\".4\", \"\", regex=False)\n",
        "                        .str.replace(\".5\", \"\", regex=False)\n",
        "                        .str.replace(\".6\", \"\", regex=False)\n",
        "                        .str.replace(\".7\", \"\", regex=False)\n",
        "                        .str.replace(\".8\", \"\", regex=False)\n",
        "                        .str.replace(\".9\", \"\", regex=False)\n",
        "                        .str.extract(pat=r\"^[$]*(\\d+)[-\\s$]*(\\d+)$\")\n",
        "                        .astype(\"float\")\n",
        "                        .rename({0: f\"{col}_lower\", 1: f\"{col}_upper\"}, axis=\"columns\")\n",
        "                )\n",
        "                dff = df.dropna()\n",
        "                dff = dff.sort_values(by=['bins_lower'])\n",
        "\n",
        "                return_arr.append(df['bins_mean'].to_numpy())\n",
        "                max_array.append([dff['bins_lower'].to_numpy(), dff['bins_mean'].to_numpy()])\n",
        "\n",
        "\n",
        "                a[x_idx][y_idx].plot(dff['bins_lower'].to_numpy(), dff['bins_mean'].to_numpy())\n",
        "                #a[x_idx][y_idx].set_xlim(0, 360)\n",
        "                #a[x_idx][y_idx].set_ylim(0, 1.0)\n",
        "                x_idx += 1\n",
        "                if x_idx == 10 and start == False:\n",
        "                    y_idx += 1\n",
        "                    x_idx = 0\n",
        "                start = False\n",
        "\n",
        "            plt.setp(plt.gcf().get_axes(), xticks=[], yticks=[])\n",
        "            plt.show()\n",
        "            return np.array(return_arr), np.array(max_array)\n",
        "\n",
        "    def plot_joint(self, HD, activations, AV):\n",
        "      fig, a = plt.subplots(10, 10)\n",
        "\n",
        "      x_idx = 0\n",
        "      y_idx = 0\n",
        "\n",
        "      for i in range(100):\n",
        "\n",
        "                act = []\n",
        "                hd = []\n",
        "                av = []\n",
        "\n",
        "                for k in range(2000):\n",
        "                    for j in range(60):\n",
        "                        act.append(activations[k, j, 0, i]*100.0)\n",
        "                        hd.append(HD[k, j, 0, 2])\n",
        "                        av.append(HD[k, j, 0, 3])\n",
        "\n",
        "                df = pd.DataFrame({'Weekday' : np.round(np.array(hd)), 'Hour':np.round(np.array(av)), 'Value':np.array(act)})\n",
        "                df['Weekday'] = pd.cut(df['Weekday'], 25)\n",
        "\n",
        "                piv = pd.pivot_table(df, index='Hour', columns='Weekday', values='Value')\n",
        "                piv = piv.fillna(0.0)\n",
        "\n",
        "\n",
        "                #fig, ax = plt.subplots()\n",
        "                #ax.set_aspect('equal')\n",
        "                #plt.axis([0, 12, 0, 7])\n",
        "                heatmap = a[x_idx][y_idx].pcolormesh(piv, cmap=\"seismic\", shading='gouraud')\n",
        "\n",
        "                #a[x_idx][y_idx].set_yticks(np.arange(piv.shape[0]) + 0.5, minor=False)\n",
        "                a[x_idx][y_idx].set_xticks(np.arange(piv.shape[1]) + 0.5, minor=False)\n",
        "                a[x_idx][y_idx].set_xticklabels(piv.columns, minor=False)\n",
        "                #a[x_idx][y_idx].set_yticklabels(piv.index, minor=False)\n",
        "                #a[x_idx][y_idx].plot(dff['bins_lower'].to_numpy(), dff['bins_mean'].to_numpy())\n",
        "                #a[x_idx][y_idx].set_xlim(0, 360)\n",
        "                #a[x_idx][y_idx].set_ylim(0, 1.0)\n",
        "                x_idx += 1\n",
        "                if x_idx == 10 and start == False:\n",
        "                    y_idx += 1\n",
        "                    x_idx = 0\n",
        "                start = False\n",
        "\n",
        "      plt.setp(plt.gcf().get_axes(), xticks=[], yticks=[])\n",
        "      plt.show()\n",
        "\n",
        "\n",
        "\n",
        "    def get_delta(self, idx, idy, maxes):\n",
        "      ##get 2d array for two idxs\n",
        "      ##find max and corresponding value\n",
        "      ##delta for corresponding values\n",
        "      pref1_x = maxes[idx, 0, :].reshape((-1, 1))\n",
        "      pref1_y = maxes[idx, 1, :].reshape((-1, 1))\n",
        "\n",
        "      pref2_x = maxes[idy, 0, :].reshape((-1, 1))\n",
        "      pref2_y = maxes[idy, 1, :].reshape((-1, 1))\n",
        "\n",
        "      indices1 = np.where(pref1_y == pref1_y.max())\n",
        "      indices2 = np.where(pref2_y == pref2_y.max())\n",
        "\n",
        "      if len(np.atleast_1d(pref1_x[indices1])) == 1 and len(np.atleast_1d(pref2_x[indices2])) == 1:\n",
        "        return (pref1_x[indices1][0] - pref2_x[indices2][0])\n",
        "      else:\n",
        "        return None\n",
        "\n",
        "\n",
        "\n",
        "    def plot_pref(self, model, maxes):\n",
        "      delta_po = None\n",
        "      weights = model.rnn.h2h.weight.detach().numpy()\n",
        "      weight_avg = []\n",
        "      delta_pref_or = []\n",
        "      for i in range(100):\n",
        "        for j in range(100):\n",
        "          weight = weights[i][j]\n",
        "          delta_po = self.get_delta(i, j, maxes)\n",
        "          if delta_po != None:\n",
        "            weight_avg.append(weight)\n",
        "            delta_pref_or.append(delta_po)\n",
        "\n",
        "      dataset = pd.DataFrame()\n",
        "      dataset['pref'] = delta_pref_or\n",
        "      dataset['weight'] = weight_avg\n",
        "      dataset['bins'] = pd.cut(dataset['pref'], 200).astype(str)\n",
        "      dataset['bins_mean'] = dataset.groupby('bins')['weight'].transform('mean')\n",
        "      #dataset = dataset.groupby('weight').mean()\n",
        "      #dataset.drop_duplicates('bins', inplace=True)\n",
        "      #dataset.reset_index(inplace=True)\n",
        "      y_mean = [np.mean(dataset['bins_mean'].to_numpy())]*len(dataset['pref'].to_numpy())\n",
        "\n",
        "      #plt.scatter(dataset['pref'].to_numpy(), dataset['bins_mean'].to_numpy())\n",
        "      #plt.plot(dataset['pref'].to_numpy(), y_mean)\n",
        "      #plt.show()\n",
        "      x = dataset['pref'].to_numpy()\n",
        "      y = dataset['bins_mean'].to_numpy()\n",
        "\n",
        "\n",
        "      fig, ax = plt.subplots()\n",
        "      sns.regplot(x=x, y=y, ax=ax, lowess=True)\n",
        "      ax.set_xlim(-180, 180)\n",
        "      plt.show()\n",
        "\n",
        "\n",
        "    def plot_pca_bumps(self, X): ##dont recalc PCA for each time point - use same and then loop through or vectorize\n",
        "        X_new = X[0, :, 0, :]\n",
        "        print(X_new.shape)\n",
        "        pca = PCA(n_components=2)\n",
        "        Xt = pca.fit_transform(X_new)\n",
        "        print(Xt.shape)\n",
        "        plot = plt.scatter(Xt[:,0], Xt[:,1])\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "    def plot_pca(self, X):\n",
        "        print(X.shape)\n",
        "        X_new = X[:, 0, 0, :]\n",
        "        pca = PCA(n_components=2)\n",
        "        Xt = pca.fit_transform(X_new)\n",
        "        plot = plt.scatter(Xt[:,0], Xt[:,1])\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "    def plot_conn(self, model):\n",
        "      weight_array = np.zeros((100, 100))\n",
        "\n",
        "      weights = model.rnn.h2h.weight.detach().numpy().T\n",
        "\n",
        "      for i in range(100):\n",
        "        for j in range(100):\n",
        "          weight_array[i, j] = weights[i][j]\n",
        "\n",
        "      weight_array = 2.*(weight_array - np.min(weight_array))/np.ptp(weight_array)-1\n",
        "\n",
        "      plt.pcolormesh(np.array(np.array(weight_array).reshape((100, 100))), cmap='bwr')\n",
        "      plt.show()\n",
        "\n",
        "\n",
        "class ModelTrainer:\n",
        "    import torch\n",
        "    import numpy as np\n",
        "\n",
        "    def __init__(self, tau: object, dt: object, x_0: object, W: object, seed: object) -> object:\n",
        "        self.seed = seed\n",
        "        self.torch.manual_seed(seed)\n",
        "        self.np.random.seed(seed)\n",
        "        self.x_0 = x_0\n",
        "        self.dt = dt\n",
        "        self.tau = tau\n",
        "        self.W = W\n",
        "        self.hidden_activations = []\n",
        "        self.input_activations = []\n",
        "        self.train_loader = None\n",
        "\n",
        "    def get_train_loader(self):\n",
        "\n",
        "        if self.train_loader == None:\n",
        "            print('WARNING: loader is None')\n",
        "\n",
        "        return self.train_loader\n",
        "\n",
        "    def compute_angular_velocity(self, x, prev_angular_velocity):\n",
        "        return (0.03 * x) + (0.8 * prev_angular_velocity)  ##convert 0.03 rad/s to deg/s using 180/pi\n",
        "\n",
        "    def load_training_data(self):\n",
        "        train_x = np.load('/home/chris/Desktop/data_x.npy').astype(float)\n",
        "        train_y = np.load('/home/chris/Desktop/data_x.npy').astype(float)\n",
        "        dataset = utils.data.TensorDataset(train_x, train_y)\n",
        "        dataloader = utils.data.DataLoader(dataset)\n",
        "        print(train_x)\n",
        "        print(train_y)\n",
        "\n",
        "        return dataloader\n",
        "\n",
        "    def generate_training_data(self):\n",
        "\n",
        "        final_array1 = np.zeros((60, 8, 3))\n",
        "        final_array2 = np.zeros((60, 8, 4))\n",
        "\n",
        "        for i in range(8):\n",
        "          time_steps = 60\n",
        "          prev_angular_velocity = np.random.normal(0,1) * 0.03\n",
        "          angular_velocity = np.random.normal(0,1) * 0.03\n",
        "          theta = np.radians(uniform(0, 360))\n",
        "          theta_initial = theta\n",
        "\n",
        "          theta_array = []\n",
        "          time_array = []\n",
        "\n",
        "          input_values = []\n",
        "          input_array = []\n",
        "\n",
        "          output_values = []\n",
        "          output_array = []\n",
        "\n",
        "\n",
        "          for step in range(0, time_steps):\n",
        "\n",
        "\n",
        "              x = np.random.normal(0,1)\n",
        "\n",
        "\n",
        "              if step < 30:\n",
        "                  prev_angular_velocity = angular_velocity = 0.0\n",
        "                  input_values.append(0.0)\n",
        "\n",
        "                  input_values.append(np.sin(theta_initial))\n",
        "                  input_values.append(np.cos(theta_initial))\n",
        "              else:\n",
        "                  angular_velocity = self.compute_angular_velocity(x, prev_angular_velocity)\n",
        "                  prev_angular_velocity = angular_velocity\n",
        "                  input_values.append(angular_velocity)\n",
        "\n",
        "                  input_values.append(0.0)\n",
        "                  input_values.append(0.0)\n",
        "\n",
        "                  theta = theta + angular_velocity\n",
        "\n",
        "\n",
        "              output_values.append(np.sin(theta))\n",
        "              output_values.append(np.cos(theta))\n",
        "              output_values.append(np.degrees(theta))\n",
        "              output_values.append(angular_velocity*(180.0/math.pi))\n",
        "\n",
        "              #theta_initial = theta\n",
        "\n",
        "              theta_array.append(np.degrees(theta))\n",
        "              time_array.append(step)\n",
        "\n",
        "              input_array.append(input_values)\n",
        "              output_array.append(output_values)\n",
        "\n",
        "              input_values = []\n",
        "              output_values = []\n",
        "\n",
        "          #print(input_array)\n",
        "          #print(output_array)\n",
        "          input_arrayy = np.array(input_array).reshape((60, 1, 3))\n",
        "          output_arrayy = np.array(output_array).reshape((60, 1, 4))\n",
        "\n",
        "\n",
        "          #plt.plot(time_array, theta_array)\n",
        "          plt.show()\n",
        "\n",
        "          #data_pd = pd.DataFrame(input_array)\n",
        "          #data_pd.columns = ['Sin Theta0', 'Cos Theta0', 'w', 'Sin Theta', 'Cos Theta']\n",
        "\n",
        "          #print(data_pd.head())\n",
        "\n",
        "          #plt.plot(time_array, data_pd[['Sin Theta']].to_numpy())\n",
        "          #plt.plot(time_array, data_pd[['Cos Theta']].to_numpy())\n",
        "          #plt.show()\n",
        "          final_array1[:, i, :] = input_arrayy.reshape((60, 3))\n",
        "          final_array2[:, i, :] = output_arrayy.reshape((60, 4))\n",
        "\n",
        "\n",
        "        train_x = torch.Tensor(np.array(final_array1).reshape((60, 8, 3)))\n",
        "        train_y = torch.Tensor(np.array(final_array2).reshape((60, 8, 4)))\n",
        "\n",
        "\n",
        "        #dataset = utils.data.TensorDataset(train_x, train_y)\n",
        "        #dataloader = utils.data.DataLoader(dataset)\n",
        "\n",
        "        return train_x, train_y\n",
        "\n",
        "    def generate_training_data_with_noise(self):\n",
        "\n",
        "        final_array1 = np.zeros((60, 8, 3))\n",
        "        final_array2 = np.zeros((60, 8, 4))\n",
        "\n",
        "        for i in range(8):\n",
        "          time_steps = 60\n",
        "          prev_angular_velocity = np.random.normal(0,1) * 0.03\n",
        "          angular_velocity = np.random.normal(0,1) * 0.03\n",
        "          theta = np.radians(uniform(0, 360))\n",
        "          theta_initial = theta\n",
        "\n",
        "          theta_array = []\n",
        "          time_array = []\n",
        "\n",
        "          input_values = []\n",
        "          input_array = []\n",
        "\n",
        "          output_values = []\n",
        "          output_array = []\n",
        "\n",
        "\n",
        "          for step in range(0, time_steps):\n",
        "\n",
        "\n",
        "              x = np.random.normal(0,1)\n",
        "              random_noise = np.random.normal(0,1)*0.1\n",
        "\n",
        "\n",
        "              if step < 30:\n",
        "                  prev_angular_velocity = angular_velocity = 0.0\n",
        "                  input_values.append(0.0)\n",
        "\n",
        "                  input_values.append(np.sin(theta_initial))\n",
        "                  input_values.append(np.cos(theta_initial))\n",
        "              else:\n",
        "                  angular_velocity = self.compute_angular_velocity(x, prev_angular_velocity)\n",
        "                  prev_angular_velocity = angular_velocity\n",
        "                  input_values.append(angular_velocity + random_noise)\n",
        "\n",
        "                  input_values.append(0.0)\n",
        "                  input_values.append(0.0)\n",
        "\n",
        "                  theta = theta + angular_velocity\n",
        "\n",
        "\n",
        "              output_values.append(np.sin(theta))\n",
        "              output_values.append(np.cos(theta))\n",
        "              output_values.append(np.degrees(theta))\n",
        "              output_values.append(angular_velocity*(180.0/math.pi))\n",
        "\n",
        "              #theta_initial = theta\n",
        "\n",
        "              theta_array.append(np.degrees(theta))\n",
        "              time_array.append(step)\n",
        "\n",
        "              input_array.append(input_values)\n",
        "              output_array.append(output_values)\n",
        "\n",
        "              input_values = []\n",
        "              output_values = []\n",
        "\n",
        "          #print(input_array)\n",
        "          #print(output_array)\n",
        "          input_arrayy = np.array(input_array).reshape((60, 1, 3))\n",
        "          output_arrayy = np.array(output_array).reshape((60, 1, 4))\n",
        "\n",
        "\n",
        "          #plt.plot(time_array, theta_array)\n",
        "          plt.show()\n",
        "\n",
        "          #data_pd = pd.DataFrame(input_array)\n",
        "          #data_pd.columns = ['Sin Theta0', 'Cos Theta0', 'w', 'Sin Theta', 'Cos Theta']\n",
        "\n",
        "          #print(data_pd.head())\n",
        "\n",
        "          #plt.plot(time_array, data_pd[['Sin Theta']].to_numpy())\n",
        "          #plt.plot(time_array, data_pd[['Cos Theta']].to_numpy())\n",
        "          #plt.show()\n",
        "          final_array1[:, i, :] = input_arrayy.reshape((60, 3))\n",
        "          final_array2[:, i, :] = output_arrayy.reshape((60, 4))\n",
        "\n",
        "\n",
        "        train_x = torch.Tensor(np.array(final_array1).reshape((60, 8, 3)))\n",
        "        train_y = torch.Tensor(np.array(final_array2).reshape((60, 8, 4)))\n",
        "\n",
        "\n",
        "        #dataset = utils.data.TensorDataset(train_x, train_y)\n",
        "        #dataloader = utils.data.DataLoader(dataset)\n",
        "\n",
        "        return train_x, train_y\n",
        "\n",
        "\n",
        "    def generate_training_data_one_location(self, idx):\n",
        "\n",
        "        final_array1 = np.zeros((60, 9, 3))\n",
        "        final_array2 = np.zeros((60, 9, 4))\n",
        "\n",
        "        for i in range(9):\n",
        "          time_steps = 60\n",
        "          prev_angular_velocity = np.random.normal(0,1) * 0.03\n",
        "          angular_velocity = np.random.normal(0,1) * 0.03\n",
        "\n",
        "          theta = np.radians(uniform(0, 360))\n",
        "          theta_initial = theta\n",
        "\n",
        "          theta_array = []\n",
        "          time_array = []\n",
        "\n",
        "          input_values = []\n",
        "          input_array = []\n",
        "\n",
        "          output_values = []\n",
        "          output_array = []\n",
        "\n",
        "          vels = [1.3, .12, 1.2, .56, 1.9, 1.88, 2.1, 1,1]\n",
        "          vidx = 0\n",
        "\n",
        "\n",
        "          for step in range(0, time_steps):\n",
        "\n",
        "\n",
        "              x = np.random.normal(0,1)\n",
        "\n",
        "              if vidx == 8:\n",
        "                vidx = 0\n",
        "              else:\n",
        "                vidx = vidx + 1\n",
        "\n",
        "\n",
        "              if step < 30 or step > 45:\n",
        "                  prev_angular_velocity = angular_velocity = 0.0\n",
        "                  input_values.append(0.0)\n",
        "\n",
        "                  input_values.append(np.sin(np.radians(0.0)))\n",
        "                  input_values.append(np.cos(np.radians(0.0)))\n",
        "                  theta = np.radians(0.0)\n",
        "              else:\n",
        "                  angular_velocity = 0.0\n",
        "                  prev_angular_velocity = 0.0\n",
        "                  input_values.append(int(vels[vidx]))\n",
        "\n",
        "                  input_values.append(np.sin(np.radians(0.0)))\n",
        "                  input_values.append(np.cos(np.radians(0.0)))\n",
        "\n",
        "                  theta = 0.0\n",
        "\n",
        "              output_values.append(np.sin(theta))\n",
        "              output_values.append(np.cos(theta))\n",
        "              output_values.append(np.degrees(theta))\n",
        "              output_values.append(angular_velocity*(180.0/math.pi))\n",
        "\n",
        "              #theta_initial = theta\n",
        "\n",
        "              theta_array.append(np.degrees(theta))\n",
        "              time_array.append(step)\n",
        "\n",
        "              input_array.append(input_values)\n",
        "              output_array.append(output_values)\n",
        "\n",
        "              input_values = []\n",
        "              output_values = []\n",
        "\n",
        "          #print(input_array)\n",
        "          #print(output_array)\n",
        "          input_arrayy = np.array(input_array).reshape((60, 1, 3))\n",
        "          output_arrayy = np.array(output_array).reshape((60, 1, 4))\n",
        "\n",
        "\n",
        "          #plt.plot(time_array, theta_array)\n",
        "          plt.show()\n",
        "\n",
        "          #data_pd = pd.DataFrame(input_array)\n",
        "          #data_pd.columns = ['Sin Theta0', 'Cos Theta0', 'w', 'Sin Theta', 'Cos Theta']\n",
        "\n",
        "          #print(data_pd.head())\n",
        "\n",
        "          #plt.plot(time_array, data_pd[['Sin Theta']].to_numpy())\n",
        "          #plt.plot(time_array, data_pd[['Cos Theta']].to_numpy())\n",
        "          #plt.show()\n",
        "          final_array1[:, i, :] = input_arrayy.reshape((60, 3))\n",
        "          final_array2[:, i, :] = output_arrayy.reshape((60, 4))\n",
        "\n",
        "\n",
        "        train_x = torch.Tensor(np.array(final_array1).reshape((60, 9, 3)))\n",
        "        train_y = torch.Tensor(np.array(final_array2).reshape((60, 9, 4)))\n",
        "\n",
        "\n",
        "        #dataset = utils.data.TensorDataset(train_x, train_y)\n",
        "        #dataloader = utils.data.DataLoader(dataset)\n",
        "\n",
        "        return train_x, train_y\n",
        "\n",
        "\n",
        "    def test_data_size(self, dataset):\n",
        "        batch_size = 10\n",
        "        seq_len = 20  # sequence length\n",
        "        input_size = 3  # input dimension\n",
        "\n",
        "        # Make network\n",
        "        rnn = RNN(input_size=input_size, hidden_size=100, output_size=2, tau=250, dt=25)\n",
        "\n",
        "        # Run the sequence through the network\n",
        "        out, rnn_output = rnn(dataset)\n",
        "\n",
        "        print('Input of shape (SeqLen, BatchSize, InputDim)=', dataset.shape)\n",
        "        print('Output of shape (SeqLen, BatchSize, Neuron)=', out.shape)\n",
        "\n",
        "    def get_model(self):\n",
        "        return RNNNet(input_size=3, hidden_size=100, output_size=2, tau=250, dt=25)\n",
        "\n",
        "    def get_inputs_and_activations(self, model):\n",
        "\n",
        "        av = []\n",
        "\n",
        "        for i in range(2000): #5000 for snn joint tuning plots\n",
        "\n",
        "            model_input, labels = self.generate_training_data()\n",
        "\n",
        "            self.input_activations.append(labels.detach().numpy())\n",
        "\n",
        "            outputs, rnn_output, acts = model(model_input)\n",
        "\n",
        "            self.hidden_activations.append(rnn_output.detach().numpy())\n",
        "            av.append(model_input.detach().numpy()[:,:,0])\n",
        "\n",
        "\n",
        "\n",
        "        return np.array(self.input_activations), np.array(self.hidden_activations), np.array(av)\n",
        "\n",
        "    def get_inputs_and_activations_one_location(self, model):\n",
        "\n",
        "        av = []\n",
        "\n",
        "        for i in range(5000): #5000 for snn joint tuning plots\n",
        "\n",
        "            model_input, labels = self.generate_training_data_one_location(i)\n",
        "\n",
        "            self.input_activations.append(labels.detach().numpy())\n",
        "\n",
        "            outputs, rnn_output, acts = model(model_input)\n",
        "\n",
        "            self.hidden_activations.append(rnn_output.detach().numpy())\n",
        "            av.append(model_input.detach().numpy()[:,:,0])\n",
        "\n",
        "\n",
        "\n",
        "        return np.array(self.input_activations), np.array(self.hidden_activations), np.array(av)\n",
        "\n",
        "    def plot_grad_flow(self, named_parameters):\n",
        "        ave_grads = []\n",
        "        layers = []\n",
        "        for n, p in named_parameters:\n",
        "            if(p.requires_grad) and (\"bias\" not in n):\n",
        "                layers.append(n)\n",
        "                ave_grads.append(p.grad.abs().mean())\n",
        "        plt.plot(ave_grads, alpha=0.3, color=\"b\")\n",
        "        plt.hlines(0, 0, len(ave_grads)+1, linewidth=1, color=\"k\" )\n",
        "        plt.xticks(range(0,len(ave_grads), 1), layers, rotation=\"vertical\")\n",
        "        plt.xlim(xmin=0, xmax=len(ave_grads))\n",
        "        plt.xlabel(\"Layers\")\n",
        "        plt.ylabel(\"average gradient\")\n",
        "        plt.title(\"Gradient flow\")\n",
        "        plt.grid(True)\n",
        "\n",
        "    def train_model(self, model):\n",
        "        optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "        criterion = nn.MSELoss()\n",
        "\n",
        "        loss_array = []\n",
        "        iterations_array = []\n",
        "        itr_counter = 0\n",
        "        norm_array = []\n",
        "        total_norm = 0\n",
        "        model_analyzer = ModelAnalyzer(model, model_trainer)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        for epoch in range(2500):\n",
        "            model_inputt, labelss = self.generate_training_data()\n",
        "\n",
        "            model.train()\n",
        "\n",
        "            labels = labelss[: , :, 0:2]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs, rnn_output, acts = model(model_inputt)\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            loss.backward()\n",
        "            #self.plot_grad_flow(model.named_parameters())\n",
        "\n",
        "\n",
        "\n",
        "            norm_array.append(total_norm)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            print('loss:' + str(loss))\n",
        "\n",
        "            if epoch % 100 == 0:\n",
        "              inputs, activations, av = self.get_inputs_and_activations_one_location(model.eval())\n",
        "\n",
        "              HD_activations, maxes = model_analyzer.plot_tuning_curve_hd(inputs, activations)\n",
        "\n",
        "              model_analyzer.plot_pca_bumps(activations)\n",
        "\n",
        "\n",
        "            loss_array.append(loss)\n",
        "            iterations_array.append(epoch)\n",
        "            itr_counter += 1\n",
        "\n",
        "        plt.plot(iterations_array, norm_array)\n",
        "        plt.show()\n",
        "\n",
        "        return model\n",
        "\n",
        "    def get_test_acc(self, model):\n",
        "      criterion = nn.MSELoss()\n",
        "      loss_array = []\n",
        "      model_inputt, labelss = self.generate_training_data()\n",
        "      model.eval()\n",
        "      labels = labelss[: , :, 0:2]\n",
        "      outputs, rnn_output = model(model_inputt)\n",
        "      loss = criterion(outputs, labels)\n",
        "      print('Test acc:' + str(loss))\n",
        "\n",
        "    def test_with_weight_noise(self, model):\n",
        "\n",
        "      with torch.no_grad():\n",
        "        for param in model.parameters():\n",
        "          param.add_(torch.randn(param.size()) * 0.1)\n",
        "\n",
        "      criterion = nn.MSELoss()\n",
        "      loss_array = []\n",
        "      model_inputt, labelss = self.generate_training_data()\n",
        "      model.eval()\n",
        "      labels = labelss[: , :, 0:2]\n",
        "      outputs, rnn_output = model(model_inputt)\n",
        "      loss = criterion(outputs, labels)\n",
        "      print('Test acc with weight noise:' + str(loss))\n",
        "\n",
        "    def test_with_input_noise(self, model):\n",
        "      criterion = nn.MSELoss()\n",
        "      loss_array = []\n",
        "      model_inputt, labelss = self.generate_training_data_with_noise()\n",
        "      model.eval()\n",
        "      labels = labelss[: , :, 0:2]\n",
        "      outputs, rnn_output = model(model_inputt)\n",
        "      loss = criterion(outputs, labels)\n",
        "      print('Test acc with input noise:' + str(loss))\n",
        "\n",
        "\n",
        "    def run(self):\n",
        "        model = self.get_model()\n",
        "        model = self.train_model(model)\n",
        "        return model\n",
        "\n",
        "model_trainer = ModelTrainer(tau=250.0, dt=25.0, x_0=1.0, W=2.0, seed=3) #3\n",
        "model = model_trainer.run()\n",
        "model = model_trainer.get_test_acc(model)\n",
        "#loader = model_trainer.get_train_loader()\n",
        "\n",
        "model_analyzer = ModelAnalyzer(model, model_trainer)\n",
        "\n",
        "\n",
        "inputs, activations, av = model_trainer.get_inputs_and_activations_one_location(model.eval())\n",
        "\n",
        "#model_analyzer.plot_pred_vs_ground_truth()\n",
        "#HD_activations, maxes = model_analyzer.plot_tuning_curve_hd(inputs, activations)\n",
        "#AV_activations, maxes = model_analyzer.plot_tuning_curve_angular_velocity(inputs, activations)\n",
        "#model_analyzer.plot_joint(inputs, av, activations)\n",
        "#model_analyzer.plot_conn(model)\n",
        "#model_analyzer.plot_pref(model, maxes)\n",
        "model_analyzer.plot_pca_bumps(activations)"
      ],
      "metadata": {
        "id": "-7KOyCecGMLc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a7ea6ed3-99bf-4e42-f840-9cfac1becbcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss:tensor(0.4652, grad_fn=<MseLossBackward0>)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 100 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWAAAADrCAYAAABXYUzjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAEWUlEQVR4nO3dsW0UYRhF0TeIEpaY6b+W3SIcQw9DAYDklWxfYc6JJ3jRDf5gvuO6rgHw8b7UAwD+VwIMEBFggIgAA0QEGCAiwACRr898fLvdrvM832nK7x6Px8/rur7ZYYcddny2HduTAT7Pc/f7/W1WvcJxHC922GGHHZ9xx+YJAiAjwAARAQaICDBARIABIgIMEBFggIgAA0QEGCAiwAARAQaICDBARIABIgIMEBFggIgAA0SO67pe//Fx/Nj2158Lv4Pvf/qTvB122GHHv75jezLAALwdTxAAEQEGiLiKbIcddtgR7NhcRbbDDjvsSHZsniAAMgIMEBFggIgAA0QEGCAiwAARAQaICDBARIABIgIMEBFggIgAA0QEGCAiwAARAQaICDBAxFVkO+yww45gx+YqMkDGEwRARIABIq4i22GHHXYEOzZXke2www47kh2bJwiAjAADRAQYICLAABEBBogIMEBEgAEiAgwQEWCAiAADRAQYICLAABEBBogIMEBEgAEiAgwQEWCAiLP0dthhhx3Bjs1ZeoCMJwiAiKvIdthhhx3Bjs1VZDvssMOOZMfmCQIgI8AAEQEGiAgwQESAASICDBARYICIAANEBBggIsAAEQEGiAgwQESAASICDBARYICIAANEBBgg4iqyHXbYYUewY3MVGSDjCQIg4iqyHXbYYUewY3MV2Q477LAj2bF5ggDICDBARIABIgIMEBFggIgAA0QEGCAiwAARAQaICDBARIABIgIMEBFggIgAA0QEGCAiwAARAQaIuIpshx122BHs2FxFBsh4ggCIuIpshx122BHs2FxFtsMOO+xIdmyeIAAyAgwQEWCAiAADRAQYICLAABEBBogIMEBEgAEiAgwQEWCAiAADRAQYICLAABEBBogIMEBEgAEiriLbYYcddgQ7NleRATKeIAAiAgwQcZbeDjvssCPYsTlLb4cddtiR7Ng8QQBkBBggIsAAEQEGiAgwQESAASICDBARYICIAANEBBggIsAAEQEGiAgwQESAASICDBARYICIq8h22GGHHcGOzVVkgIwnCICIAANEXEW2ww477Ah2bK4i22GHHXYkOzZPEAAZAQaICDBARIABIgIMEBFggIgAA0QEGCAiwAARAQaICDBARIABIgIMEBFggIgAA0QEGCDiKrIddthhR7BjcxUZIOMJAiAiwAARV5HtsMMOO4Idm6vIdthhhx3Jjs0TBEBGgAEiAgwQEWCAiAADRAQYICLAABEBBogIMEBEgAEiAgwQEWCAiAADRAQYICLAABEBBoi4imyHHXbYEezYXEUGyHiCAIgIMEDEVWQ77LDDjmDH5iqyHXbYYUeyY/MEAZARYICIAANEBBggIsAAEQEGiAgwQESAASICDBARYICIAANEBBggIsAAEQEGiAgwQESAASKuItthhx12BDs2V5EBMp4gACICDBARYICIAANEBBggIsAAEQEGiAgwQESAASK/ALUgwqUOf59rAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60, 100)\n",
            "(60, 2)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss:tensor(0.4040, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.3335, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.2672, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1792, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.2735, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.4288, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1508, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1434, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.2435, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.2245, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1159, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1734, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1993, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.2073, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1325, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1147, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1431, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.0981, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1458, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1277, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1072, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1422, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1173, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1177, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.0891, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.0838, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1216, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1253, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1146, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1037, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1684, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1187, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1187, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.0944, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.0824, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.0955, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1267, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.0941, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1216, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.0726, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1148, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.0903, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1120, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.0499, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.0828, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1104, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1432, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1105, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1378, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1490, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1142, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.0896, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.0899, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.0888, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1011, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1214, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.0908, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1619, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1148, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.0807, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1251, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1138, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.0914, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1606, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1412, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1029, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1132, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.0606, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.0665, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1015, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1261, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1238, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1190, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1345, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1269, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.2182, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1854, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.0873, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1720, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1529, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1195, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.0868, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1391, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1993, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1586, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.0979, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1582, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1127, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1611, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1377, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1230, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1696, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1123, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.0889, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1348, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1696, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1114, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.0998, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1801, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.0938, grad_fn=<MseLossBackward0>)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 100 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWAAAADrCAYAAABXYUzjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAEWUlEQVR4nO3dsW0UYRhF0TeIEpaY6b+W3SIcQw9DAYDklWxfYc6JJ3jRDf5gvuO6rgHw8b7UAwD+VwIMEBFggIgAA0QEGCAiwACRr898fLvdrvM832nK7x6Px8/rur7ZYYcddny2HduTAT7Pc/f7/W1WvcJxHC922GGHHZ9xx+YJAiAjwAARAQaICDBARIABIgIMEBFggIgAA0QEGCAiwAARAQaICDBARIABIgIMEBFggIgAA0SO67pe//Fx/Nj2158Lv4Pvf/qTvB122GHHv75jezLAALwdTxAAEQEGiLiKbIcddtgR7NhcRbbDDjvsSHZsniAAMgIMEBFggIgAA0QEGCAiwAARAQaICDBARIABIgIMEBFggIgAA0QEGCAiwAARAQaICDBAxFVkO+yww45gx+YqMkDGEwRARIABIq4i22GHHXYEOzZXke2www47kh2bJwiAjAADRAQYICLAABEBBogIMEBEgAEiAgwQEWCAiAADRAQYICLAABEBBogIMEBEgAEiAgwQEWCAiLP0dthhhx3Bjs1ZeoCMJwiAiKvIdthhhx3Bjs1VZDvssMOOZMfmCQIgI8AAEQEGiAgwQESAASICDBARYICIAANEBBggIsAAEQEGiAgwQESAASICDBARYICIAANEBBgg4iqyHXbYYUewY3MVGSDjCQIg4iqyHXbYYUewY3MV2Q477LAj2bF5ggDICDBARIABIgIMEBFggIgAA0QEGCAiwAARAQaICDBARIABIgIMEBFggIgAA0QEGCAiwAARAQaIuIpshx122BHs2FxFBsh4ggCIuIpshx122BHs2FxFtsMOO+xIdmyeIAAyAgwQEWCAiAADRAQYICLAABEBBogIMEBEgAEiAgwQEWCAiAADRAQYICLAABEBBogIMEBEgAEiriLbYYcddgQ7NleRATKeIAAiAgwQcZbeDjvssCPYsTlLb4cddtiR7Ng8QQBkBBggIsAAEQEGiAgwQESAASICDBARYICIAANEBBggIsAAEQEGiAgwQESAASICDBARYICIq8h22GGHHcGOzVVkgIwnCICIAANEXEW2ww477Ah2bK4i22GHHXYkOzZPEAAZAQaICDBARIABIgIMEBFggIgAA0QEGCAiwAARAQaICDBARIABIgIMEBFggIgAA0QEGCDiKrIddthhR7BjcxUZIOMJAiAiwAARV5HtsMMOO4Idm6vIdthhhx3Jjs0TBEBGgAEiAgwQEWCAiAADRAQYICLAABEBBogIMEBEgAEiAgwQEWCAiAADRAQYICLAABEBBoi4imyHHXbYEezYXEUGyHiCAIgIMEDEVWQ77LDDjmDH5iqyHXbYYUeyY/MEAZARYICIAANEBBggIsAAEQEGiAgwQESAASICDBARYICIAANEBBggIsAAEQEGiAgwQESAASKuItthhx12BDs2V5EBMp4gACICDBARYICIAANEBBggIsAAEQEGiAgwQESAASK/ALUgwqUOf59rAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60, 100)\n",
            "(60, 2)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAD4CAYAAADhNOGaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAanklEQVR4nO3df5Ac5X3n8feH1QovJJcVFnHQSkiiTpFjioB8U0rOuGLAhpXtO6Q4viAuvogclBIHchW7TnVScWVcOC4Uq67syoU7rCIKOM4BNpF1m7J9G2xBcWUiW6uTQMB5YRHYaOSEDWK5ohhgtXzvj+lZendn9tf0zo/tz6tqaruf7p75qmfU3+7nefppRQRmZpZfZzU7ADMzay4nAjOznHMiMDPLOScCM7OccyIwM8u5Jc0OYD6WL18ea9asaXYYZmZt5ciRI/8UEedPLm/LRLBmzRoGBgaaHYaZWVuR9JNq5a4aMjPLOScCM7OccyIwM8s5JwIzs5zLJBFI2ifpJUlP1lh+haRXJR1LXp9LLdskaVDSkKSdWcRjZmazl1WvoXuAPwe+Ns06/zsi/lW6QFIHcCdwNXASOCypLyKeziiucQeOFtnTP8ipkRIrurvY0bueLRt6sv4YM7O2k8kVQUQ8Cpyex6YbgaGIOBERbwH3A5uziCntwNEiu/YfpzhSIoDiSIld+49z4Ggx648yM2s7jWwj+JeSHpf0XUkXJ2U9wIupdU4mZVNI2i5pQNLA8PDwnD54T/8gpdGxCWWl0TH29A/O6X3MzBajRiWC/wOsjohLgf8KHJjrG0TE3ogoRETh/POn3Bg3rVMjpTmVm5nlSUMSQUT8v4h4LZn+DtApaTlQBFalVl2ZlGVqRXfXnMrNzPKkIYlA0i9JUjK9Mfncl4HDwDpJayUtBbYCfVl//o7e9XR1dkwo6+rsYEfv+qw/ysys7WTSa0jSfcAVwHJJJ4HbgE6AiLgL+CTwaUlngBKwNcrPyDwj6RagH+gA9kXEU1nElFbpHeReQ2ZmU6kdn1lcKBTCg86Zmc2NpCMRUZhc7juLzcxyzonAzCznnAjMzHLOicDMLOecCMzMcs6JwMws55wIzMxyzonAzCznnAjMzHLOicDMLOecCMzMcs6JwMws55wIzMxyzonAzCznnAjMzHLOicDMLOecCMzMcs6JwMws55wIzMxyzonAzCznnAjMzHIuk0QgaZ+klyQ9WWP570h6QtJxSY9JujS17IWk/JikgSziMTOz2cvqiuAeYNM0y58HPhQRlwBfAPZOWn5lRFwWEYWM4jEzs1laksWbRMSjktZMs/yx1OwhYGUWn2tmZvVrRhvBjcB3U/MB/J2kI5K219pI0nZJA5IGhoeHFzxIM7O8yOSKYLYkXUk5EXwwVfzBiChK+kXgIUk/johHJ28bEXtJqpQKhUI0JGAzsxxo2BWBpF8F7gY2R8TLlfKIKCZ/XwK+BWxsVExmZtagRCDpQmA/8O8i4plU+bmSfr4yDVwDVO15ZGZmCyOTqiFJ9wFXAMslnQRuAzoBIuIu4HPAu4H/JgngTNJD6D3At5KyJcD/iIj/lUVMZmY2O1n1Grp+huU3ATdVKT8BXDp1CzMzaxTfWWxmlnNOBGZmOedEYGaWc04EZmY550RgZpZzTgRmZjnnRGBmlnNOBGZmOedEYGaWc04EZmY519BhqFvNgaNF9vQPcmqkxIruLnb0rmfLhp5mh2Vm1lC5TQQHjhbZtf84pdExAIojJXbtPw7gZGBmuZLbqqE9/YPjSaCiNDrGnv7BJkVkZtYcuU0Ep0ZKcyo3M1uscpsIVnR3zanczGyxym0i2NG7nq7OjgllXZ0d7Ohd36SIzMyaI7eNxZUGYfcaMrO8y20igHIy8IHfzPIut1VDZmZW5kRgZpZzTgRmZjmXSSKQtE/SS5KerLFckv5M0pCkJyS9P7Vsm6Rnk9e2LOIxM7PZy+qK4B5g0zTLPwqsS17bgf8OIOk84Dbg14CNwG2SlmUUk5mZzUImiSAiHgVOT7PKZuBrUXYI6JZ0AdALPBQRpyPiFeAhpk8oZmaWsUa1EfQAL6bmTyZltcqnkLRd0oCkgeHh4QUL1Mwsb9qmsTgi9kZEISIK559/frPDMTNbNBqVCIrAqtT8yqSsVrmZmTVIoxJBH/C7Se+hXwdejYifAf3ANZKWJY3E1yRlZmbWIJkMMSHpPuAKYLmkk5R7AnUCRMRdwHeAjwFDwOvA7yXLTkv6AnA4eavbI2K6RmczM8tYJokgIq6fYXkAN9dYtg/Yl0UcZmY2d23TWGxmZgvDicDMLOecCMzMcs6JwMws55wIzMxyzonAzCznnAjMzHLOicDMLOecCMzMcs6JwMws55wIzMxyzonAzCznnAjMzHLOicDMLOecCMzMcs6JwMws55wIzMxyzonAzCznMnlU5WJy4GiRPf2DnBopsaK7ix2969myoafZYZmZLRgngpQDR4vs2n+c0ugYAMWRErv2HwdwMjCzRSuTqiFJmyQNShqStLPK8i9LOpa8npE0klo2llrWl0U887Wnf3A8CVSURsfY0z/YpIjMzBZe3VcEkjqAO4GrgZPAYUl9EfF0ZZ2I+Exq/T8CNqTeohQRl9UbRxZOjZTmVG5mthhkcUWwERiKiBMR8RZwP7B5mvWvB+7L4HMzt6K7a07lZmaLQRaJoAd4MTV/MimbQtJqYC1wMFX8LkkDkg5J2lLrQyRtT9YbGB4eziDsqXb0rqers2NCWVdnBzt61y/I55mZtYJGNxZvBR6MiHRF/OqIKEq6CDgo6XhEPDd5w4jYC+wFKBQKsRDBVRqE3WvIzPIki0RQBFal5lcmZdVsBW5OF0REMfl7QtIjlNsPpiSCRtmyoccHfjPLlSyqhg4D6yStlbSU8sF+Su8fSe8FlgF/nypbJunsZHo5cDnw9ORtzcxs4dR9RRARZyTdAvQDHcC+iHhK0u3AQERUksJW4P6ISFfr/ArwVUlvU05Ku9O9jczMbOFp4nG5PRQKhRgYGGh2GGZmbUXSkYgoTC73WENmZjnnRGBmlnNOBGZmOedEYGaWc04EZmY550RgZpZzTgRmZjnnRGBmlnNOBGZmOedEYGaWc04EZmY550RgZpZzTgRmZjnnRGBmlnNOBGZmOedEYGaWc04EZmY550RgZpZzdT+zOA8OHC2yp3+QUyMlVnR3saN3PVs29DQ7LDOzTDgRzODA0SK79h+nNDoGQHGkxK79xwGcDMxsUXDV0Az29A+OJ4GK0ugYe/oHmxSRmVm2MkkEkjZJGpQ0JGlnleU3SBqWdCx53ZRatk3Ss8lrWxbxZOnUSGlO5WZm7abuqiFJHcCdwNXASeCwpL6IeHrSqg9ExC2Ttj0PuA0oAAEcSbZ9pd64srKiu4tilYP+iu6uJkRjZpa9LK4INgJDEXEiIt4C7gc2z3LbXuChiDidHPwfAjZlEFNmdvSup6uzY0JZV2cHO3rXNykiM7NsZZEIeoAXU/Mnk7LJfkvSE5IelLRqjtsiabukAUkDw8PDGYQ9O1s29HDHJy6hp7sLAT3dXdzxiUvcUGxmi0ajeg39LXBfRLwp6feBe4Gr5vIGEbEX2AtQKBQi+xBr27Khxwd+M1u0srgiKAKrUvMrk7JxEfFyRLyZzN4N/IvZbmtmZgsri0RwGFgnaa2kpcBWoC+9gqQLUrPXAv83me4HrpG0TNIy4JqkzMzMGqTuqqGIOCPpFsoH8A5gX0Q8Jel2YCAi+oD/IOla4AxwGrgh2fa0pC9QTiYAt0fE6XpjMjOz2VNEQ6vbM1EoFGJgYKDZYZiZtRVJRyKiMLncdxabmeWcE4GZWc45EZiZ5ZwTgZlZzjkRmJnlnBOBmVnOORGYmeWcE4GZWc45EZiZ5ZwTgZlZzvnh9XU4cLTInv5BTo2UWNHdxY7e9R6u2szajhPBPB04WmTX/uPjD7YvjpTYtf84gJOBmbUVVw3N057+wfEkUFEaHWNP/2CTIjIzmx8ngnk6VeWB9tOVm5m1KieCeVrR3TWncjOzVuVEME87etfT1dkxoayrs4MdveubFJGZ2fy4sXieKg3C7jVkZu3OiaAOWzb0+MBvZm3PVUNmZjnnRGBmlnNOBGZmOZdJIpC0SdKgpCFJO6ss/6ykpyU9Ien7klanlo1JOpa8+rKIx8zMZq/uxmJJHcCdwNXASeCwpL6IeDq12lGgEBGvS/o08CXgumRZKSIuqzcOMzObnyyuCDYCQxFxIiLeAu4HNqdXiIiHI+L1ZPYQsDKDzzUzswxkkQh6gBdT8yeTslpuBL6bmn+XpAFJhyRtqbWRpO3JegPDw8P1RWxmZuMaeh+BpE8BBeBDqeLVEVGUdBFwUNLxiHhu8rYRsRfYC1AoFKIhAZuZ5UAWVwRFYFVqfmVSNoGkjwC3AtdGxJuV8ogoJn9PAI8AGzKIyczMZimLK4LDwDpJaykngK3Av02vIGkD8FVgU0S8lCpfBrweEW9KWg5cTrkhuW35YTVm1m7qTgQRcUbSLUA/0AHsi4inJN0ODEREH7AH+Dngm5IAfhoR1wK/AnxV0tuUr052T+pt1Fb8sBoza0eKaL/q9kKhEAMDA80OY4rLdx+kWOV5BD3dXfxg51VNiMjM7B2SjkREYXK57yzOkB9WY2btyIkgQ35YjZm1IyeCDPlhNWbWjvw8ggz5YTVm1o6cCDLmh9WYWbtx1ZCZWc45EZiZ5ZyrhszMZlBtxABYPO2BTgRmZtOoNmLAjgcfh4DRt2O8rNYoAu0w7IwTwQJrhx+BmdW2p39wPAlUjI5NHZGhNDrGnv7BCf+/sxp2ZqGPI04EC8hjD5m1j1oH27mMDDB53WpJpFrCmCmuhT6OuLF4AU33IzCz1lE52BZHSgTlg+1nHjjGmp3f5qzyQJmzMnkUgSyGnWnEccSJYAF57CGz9lDtYFup/BmrMjBnx1mi86yJCaLaKAJZDDvTiOOIE8EC8thDZu1hrgfVs4DrNq6ip7sLUR5h+I5PXDKlqiaLYWcacRxxIlhAHnvIrD3M9aA6+nbw8I+H+cHOq/jydZcB8JkHjnH57oMcOPrOAxq3bOjhjk9cMmPCmE4jjiNuLF5AHnvIrD3s6F0/oUF2Nk6NlGbVkFvvsDONOI74wTRmi1yjujC3e1fpSvzFkRLinTaCWnqSq4h2ehhVrQfT+IrAbBFrVBfmxdBVOn3mnk5q3ed08tobZ8ZvHoN3qmY+88Cxqu/Vbh1CfEXQQO1+xmTtIf07O0uq2usl6zPWWo9prXxWu//Wa/3fbbfH0/qKoMkWwxmTtb7Jv7NqSQCyP2Od7v0Ww2+9Vj1/tbaFduwQkkmvIUmbJA1KGpK0s8rysyU9kCz/oaQ1qWW7kvJBSb1ZxNOKfHOZNUK131k13ed0cvnug6zd+e0pPV3mY6ZeN4v1t55Fr6BWUPcVgaQO4E7gauAkcFhSX0Q8nVrtRuCViPjnkrYCfwpcJ+l9wFbgYmAF8D1JvxwRs2+6bxO+ucwaYTa/p84O8dobZ3jl9VEgmzP22fS6Way/9cXwMKosrgg2AkMRcSIi3gLuBzZPWmczcG8y/SDwYUlKyu+PiDcj4nlgKHm/Rcc3l1kj1Po9dUjjZ6znLl0yoeET6j9jT58ZzzU2a74sEkEP8GJq/mRSVnWdiDgDvAq8e5bbLgq+uSxfDhwtZlr1Mlu1fmf/5bcv5fndH+cHO6/i1dJo1W3rPWPfsqGHH+y8iq9cd5l/622mbRqLJW0HtgNceOGFTY5m7tI3hRRHSnRIE87C2v3ScjGpt3dXMzsGzObmoxXdXVV7umR1xu4bKdtPFomgCKxKza9Myqqtc1LSEuAXgJdnuS0AEbEX2Avl7qMZxN1wlf8I7j3UurI4iGcx9HA9ZqqzbkRPl8VQb54nWVQNHQbWSVoraSnlxt++Sev0AduS6U8CB6N8A0MfsDXpVbQWWAf8KIOYWpZ7D7W2LL6fVu8YsFh6ulh26r4iiIgzkm4B+oEOYF9EPCXpdmAgIvqAvwD+StIQcJpysiBZ7xvA08AZ4ObF2GMordUPEnmXxfdTT9VLo2469Bm7pWXSRhAR3wG+M6nsc6npN4B/U2PbLwJfzCKOdrDQ9bNWnyy+n/lWvfimQ2sWD0PdYNV6dQi48r3nNyegFtKsnjZpWfTumm/Vi6sNrVnaptfQYrFlQw8DPznNXx/66fjohgH8zZEihdXn5fbMr1XOhrPq8TKfqhdXG1qzOBE0wcM/Hp4yxG0je5W0omb3tElrVv25qw2tWVw11AQ+85vK+8Q3HVrzOBE0Qa0zvLOkptSLz9VC1OV7CA5367TmcdVQE9QaoGssouV7iSxUXX49PW0W0x2s7tZpzeArgiaonPl1SFOWlUbH+HzfUw2LZa5n9wvVs2U+Z8OVpFQcKRG8k5Ta4arKrJX4iqBJtmzoqfmYu5HSKAeOFhf8zHA+Z/cLWZc/17PhVmpgNmtnviJoounqv7PqOz7dGf98zu5bqS7fDcxm2XAiaKLp6r9rPf+1lmoH/JmqTuZzIG2lni2tlJTM2pmrhppoy4YePvuNY7xdYyzV/3zgOH+y5ZLx+XTDaPc5nUTAq6VRus/p5LU3zow/bKRywD97yVnTVp3Mp996Kw0xvFieF2vWbE4ETVYrCQB8/dBP+fqhn1ZdVnnM4OTpitLoWM3HBlbO+Od7IG2Vni2tlJTM2pkTQZP11DgrX0iVM/7FcCBtlaRk1s6cCJpsR+96/rhG76F6LTunkzdG3572jN8HUjNzY3GTbdnQw7lLO2ZecY66Oju47V9f7DtVzWxGviJoAV/8zUvqviro7BDnLl3Cq6XRKVU8PvCb2XScCFrAlg09c04E53SexdIlHVUP/GZmc+FE0Ga6uzr5/LUX+6BvZplxImgRL+z+OGt2fntKeYfE9b+2asL9BGZmWXIiaCEv7P54s0MwsxxyryEzs5yrKxFIOk/SQ5KeTf4uq7LOZZL+XtJTkp6QdF1q2T2Snpd0LHldVk88ZmY2d/VeEewEvh8R64DvJ/OTvQ78bkRcDGwCviKpO7V8R0RclrwW5s4qMzOrqd5EsBm4N5m+F9gyeYWIeCYink2mTwEvAefX+blmZpaRehPBeyLiZ8n0PwDvmW5lSRuBpcBzqeIvJlVGX5Z09jTbbpc0IGlgeHi4zrDNzKxCEdMMfwlI+h7wS1UW3QrcGxHdqXVfiYgp7QTJsguAR4BtEXEoVfYPlJPDXuC5iLh9xqClYeAnM62XoeXAPzXw8+bK8dWn1eOD1o/R8dWnUfGtjogpNTIzdh+NiI/UWibpHyVdEBE/Sw7qL9VY758B3wZurSSB5L0rVxNvSvpL4D/OFE+yXUOrliQNREShkZ85F46vPq0eH7R+jI6vPs2Or96qoT5gWzK9Dfifk1eQtBT4FvC1iHhw0rILkr+i3L7wZJ3xmJnZHNWbCHYDV0t6FvhIMo+kgqS7k3V+G/gN4IYq3UT/WtJx4DjlS6M/qTMeMzObo7ruLI6Il4EPVykfAG5Kpr8OfL3G9lfV8/kNtLfZAczA8dWn1eOD1o/R8dWnqfHN2FhsZmaLm4eYMDPLOScCM7OccyJIzGbcpGS9sVSjd1+qfK2kH0oakvRA0luqofE1Y1wnSZskDSb/7ilDjEg6O9kfQ8n+WZNatispH5TUm0U884jvs5KeTvbX9yWtTi2r+l03OL4bJA2n4rgptWxb8nt4VtK2yds2KL4vp2J7RtJIalkj9t8+SS9JqtrjUGV/lsT/hKT3p5Y1Yv/NFN/vJHEdl/SYpEtTy15Iyo9JGliI+MZFhF/ldpIvATuT6Z3An9ZY77Ua5d8AtibTdwGfbnR8wC8D65LpFcDPgO5k/h7gkxnH1EH5LvGLKN8U+Djwvknr/CFwVzK9FXggmX5fsv7ZwNrkfTqaEN+VwDnJ9Kcr8U33XTc4vhuAP6+y7XnAieTvsmR6WaPjm7T+HwH7GrX/ks/4DeD9wJM1ln8M+C4g4NeBHzZq/80yvg9UPhf4aCW+ZP4FYPlC78OI8BVByozjJtWS3AdxFVC5T2JO289SK47rtBEYiogTEfEWcH8SZ1o67geBDyf7azNwf0S8GRHPA0PJ+zU0voh4OCJeT2YPASszjqGu+KbRCzwUEacj4hXgIcqDOjYzvuuB+zKOYVoR8ShweppVNlO+hymifDNrd3L/UiP234zxRcRjyedD439/45wI3jHbcZPepfKYR4ckVQ7G7wZGIuJMMn8SyPpZkg0b12kOeoAXU/PV/t3j6yT751XK+2s22zYivrQbKZ89VlT7rpsR328l39uDklbNcdtGxEdSpbYWOJgqXuj9Nxu1/g2N2H9zNfn3F8DfSToiaftCfnCunlCm6cdNGhcRIalWv9rVEVGUdBFwUOUb4l5tofgqd2z/FeVxnd5OincxcVyn/wTMOK5TXkj6FFAAPpQqnvJdR8Rz1d9hwfwtcF9EvCnp9ylfXbXi/TdbgQcjYixV1gr7ry1IupJyIvhgqviDyf77ReAhST9OrjAyl6tEEBmMmxQRxeTvCUmPABuAv6F8ybkkOetdCRSbEZ8yHtdpBkVgVWq+2r+7ss5JSUuAXwBenuW2jYgPSR+hnGw/FBFvVsprfNdZHshmjC/KN21W3E25raiy7RWTtn0kw9hmFV/KVuDmdEED9t9s1Po3NGL/zYqkX6X83X40/X2n9t9Lkr5FuapuQRLBgjdCtMsL2MPExtgvVVlnGXB2Mr0ceJak8Qz4JhMbi/+wCfEtpfyAoD+usuyC5K+ArwC7M4hpCeVGtrW805h48aR1bmZiY/E3kumLmdhYfILsG4tnE1/l4LRutt91g+O7IDX9m8ChZPo84PkkzmXJ9HmNji9Z772UGzbVyP2X+qw11G6M/TgTG4t/1Kj9N8v4LqTcPvaBSeXnAj+fmn4M2LQQ8UWEE0Fqx787OYg+C3yv8qOgXF1wdzL9AcrjIj2e/L0xtf1FwI+SL/Wblf8EDY7vU8AocCz1uixZdjCJ+UnKQ378XEZxfQx4JjmY3pqU3Q5cm0y/K9kfQ8n+uSi17a3JdoOUz4YW4nudKb7vAf+Y2l99M33XDY7vDuCpJI6Hgfemtv33yX4dAn6vGfEl859n0olFA/fffZR7x41Srue/EfgD4A+S5QLuTOI/DhQavP9miu9u4JXU728gKb8o2XePJ9//rQsRX+XlISbMzHLOvYbMzHLOicDMLOecCMzMcs6JwMws55wIzMxyzonAzCznnAjMzHLu/wPCTqrzrzdCAQAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss:tensor(0.1096, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.0945, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1164, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1601, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1275, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1455, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.0623, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1369, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.0566, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1046, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.0994, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1287, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.0775, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1232, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1637, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.0715, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.0652, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.0768, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.0768, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1009, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1224, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.0797, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.0725, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.0976, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.0828, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.0700, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.0918, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.0755, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.0720, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1180, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.0913, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.0908, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.0827, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1259, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1269, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1604, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.2681, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.0698, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.0605, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1084, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1604, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1469, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.0733, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.0811, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.0686, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.0530, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1228, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.0974, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.0779, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1468, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.0635, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.0961, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.0956, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.0759, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.0836, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.0846, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1148, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1060, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1053, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.0440, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1093, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.0546, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.0822, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1159, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.0904, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.0673, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.0678, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.0711, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.0722, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.0728, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.0979, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1644, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1661, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1548, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.0694, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1994, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1047, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1090, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1785, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1331, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1647, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1308, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1067, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1014, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1105, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1254, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1233, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.2228, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1341, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1044, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.0827, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1208, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.0811, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.0868, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.1226, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.0741, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.0891, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.0711, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.0734, grad_fn=<MseLossBackward0>)\n",
            "loss:tensor(0.0420, grad_fn=<MseLossBackward0>)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-8dbde3fe1286>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    962\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[0mmodel_trainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtau\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m250.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 964\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    965\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_test_acc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m \u001b[0;31m#loader = model_trainer.get_train_loader()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-8dbde3fe1286>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    958\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 960\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    961\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-8dbde3fe1286>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 906\u001b[0;31m               \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mav\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_inputs_and_activations_one_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    907\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    908\u001b[0m               \u001b[0mHD_activations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_analyzer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_tuning_curve_hd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-8dbde3fe1286>\u001b[0m in \u001b[0;36mget_inputs_and_activations_one_location\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    836\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#5000 for snn joint tuning plots\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 838\u001b[0;31m             \u001b[0mmodel_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_training_data_one_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    839\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_activations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-8dbde3fe1286>\u001b[0m in \u001b[0;36mgenerate_training_data_one_location\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    736\u001b[0m                   \u001b[0minput_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mradians\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m                   \u001b[0minput_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mradians\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 738\u001b[0;31m                   \u001b[0mtheta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mradians\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    739\u001b[0m               \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m                   \u001b[0mangular_velocity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install snntorch"
      ],
      "metadata": {
        "id": "1C4w4vZ4GMOd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e38b2a7-a666-464e-a445-c1070f2fcba6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting snntorch\n",
            "  Downloading snntorch-0.5.1-py2.py3-none-any.whl (92 kB)\n",
            "\u001b[K     |████████████████████████████████| 92 kB 275 kB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from snntorch) (3.2.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from snntorch) (1.21.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from snntorch) (1.3.5)\n",
            "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from snntorch) (1.12.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.1.0->snntorch) (4.1.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->snntorch) (1.4.3)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->snntorch) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->snntorch) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->snntorch) (0.11.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->snntorch) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->snntorch) (2022.1)\n",
            "Installing collected packages: snntorch\n",
            "Successfully installed snntorch-0.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import snntorch as snn\n",
        "from snntorch import surrogate\n",
        "beta = 0.5  # neuron decay rate\n",
        "spike_grad = surrogate.fast_sigmoid()"
      ],
      "metadata": {
        "id": "HpzTm__nO5Ld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jFlU_jM9O8Nm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_grad_flow(named_parameters):\n",
        "    '''Plots the gradients flowing through different layers in the net during training.\n",
        "    Can be used for checking for possible gradient vanishing / exploding problems.\n",
        "\n",
        "    Usage: Plug this function in Trainer class after loss.backwards() as\n",
        "    \"plot_grad_flow(self.model.named_parameters())\" to visualize the gradient flow'''\n",
        "    ave_grads = []\n",
        "    max_grads= []\n",
        "    layers = []\n",
        "    for n, p in named_parameters:\n",
        "        if(p.requires_grad) and (\"bias\" not in n):\n",
        "            layers.append(n)\n",
        "            ave_grads.append(p.grad.abs().mean())\n",
        "            max_grads.append(p.grad.abs().max())\n",
        "    plt.bar(np.arange(len(max_grads)), max_grads, alpha=0.1, lw=1, color=\"c\")\n",
        "    plt.bar(np.arange(len(max_grads)), ave_grads, alpha=0.1, lw=1, color=\"b\")\n",
        "    plt.hlines(0, 0, len(ave_grads)+1, lw=2, color=\"k\" )\n",
        "    plt.xticks(range(0,len(ave_grads), 1), layers, rotation=\"vertical\")\n",
        "    plt.xlim(left=0, right=len(ave_grads))\n",
        "    plt.ylim(bottom = -0.001, top=0.02) # zoom in on the lower gradient regions\n",
        "    plt.xlabel(\"Layers\")\n",
        "    plt.ylabel(\"average gradient\")\n",
        "    plt.title(\"Gradient flow\")"
      ],
      "metadata": {
        "id": "CVn2ROt4Kl3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = self.criterion(outputs, labels)\n",
        "loss.backward()\n",
        "plot_grad_flow(model.named_parameters())"
      ],
      "metadata": {
        "id": "0Ui_Dn9j7O15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GXYL_Jwi7O4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pt_RKAF17O7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gxDeqQkl8_gq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XuJPcdPu8_je"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}